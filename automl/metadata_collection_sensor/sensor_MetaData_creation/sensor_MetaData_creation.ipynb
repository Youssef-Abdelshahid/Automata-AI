{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84039d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 \n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "OPENML_CACHE_DIR = Path(\"openml_cache\").resolve()\n",
    "OPENML_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"OPENML_HOME\"] = str(OPENML_CACHE_DIR)\n",
    "os.environ[\"OPENML_CACHE_DIRECTORY\"] = str(OPENML_CACHE_DIR)\n",
    "\n",
    "\n",
    "os.environ[\"USERPROFILE\"] = str(OPENML_CACHE_DIR)\n",
    "drive, tail = os.path.splitdrive(str(OPENML_CACHE_DIR))\n",
    "if drive and tail:\n",
    "    os.environ[\"HOMEDRIVE\"] = drive\n",
    "    os.environ[\"HOMEPATH\"]  = tail\n",
    "\n",
    "\n",
    "_path_home = OPENML_CACHE_DIR\n",
    "pathlib.Path.home = staticmethod(lambda: _path_home)\n",
    "\n",
    "_orig_expanduser = os.path.expanduser\n",
    "def _expanduser_g(path):\n",
    "    if isinstance(path, str) and path.startswith(\"~\"):\n",
    "        return path.replace(\"~\", str(_path_home), 1)\n",
    "    return _orig_expanduser(path)\n",
    "os.path.expanduser = _expanduser_g\n",
    "\n",
    "for m in list(sys.modules):\n",
    "    if m == \"openml\" or m.startswith(\"openml.\"):\n",
    "        del sys.modules[m]\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"openml\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"openml.datasets.functions\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Path.home()            ->\", Path.home())\n",
    "print(\"os.path.expanduser('~') ->\", os.path.expanduser(\"~\"))\n",
    "print(\"ENV OPENML_HOME        ->\", os.environ.get(\"OPENML_HOME\"))\n",
    "print(\"ENV OPENML_CACHE_DIR   ->\", os.environ.get(\"OPENML_CACHE_DIRECTORY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5296b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 \n",
    "\n",
    "import os, gc, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import psutil\n",
    "import openml\n",
    "import sklearn\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder as _SklearnOHE, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception:\n",
    "    XGBClassifier = None\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "import sklearn.preprocessing as _skp\n",
    "_skp.OneHotEncoder = _SklearnOHE\n",
    "OneHotEncoder = _SklearnOHE\n",
    "\n",
    "try:\n",
    "    if hasattr(openml.config, \"set_cache_directory\"):\n",
    "        openml.config.set_cache_directory(str(OPENML_CACHE_DIR))\n",
    "except Exception:\n",
    "    pass\n",
    "openml.config.cache_directory = str(OPENML_CACHE_DIR)\n",
    "\n",
    "print(\"OpenML cache set to:\", openml.config.cache_directory)\n",
    "print(\"Verify home now    :\", Path.home())\n",
    "print(\"Verify expanduser  :\", os.path.expanduser(\"~\"))\n",
    "\n",
    "print(\"Current OpenML server:\", openml.config.server)\n",
    "\n",
    "# =================== GLOBAL CONFIG ===================\n",
    "\n",
    "MAX_TASKS      = None           \n",
    "MAX_DATASETS   = None\n",
    "ONLY_ACTIVE    = True\n",
    "SHUFFLE_TASKS  = True\n",
    "RANDOM_SEED    = 42\n",
    "\n",
    "SKIP_DATASET_IDS = {41811, 40864, 46043, 46046, 46026, 45923, 41883, 44324, 44310, 44296, 44284, 44312, 46056, 46040, 44250, 44313, 44307, 44251, 44273, 46036, 44244, 44332}\n",
    "\n",
    "SKIP_TASK_IDS    = {362155, 362156, 361188, 167207}\n",
    "\n",
    "# Candidate models \n",
    "ENABLED_MODELS = [\n",
    "    \"logreg\",\n",
    "    \"rf\",\n",
    "    \"xgboost\",\n",
    "    \"cnn1d\",\n",
    "    \"tiny_rnn\",\n",
    "    \"mlp\",\n",
    "    \"tinyconv\",\n",
    "]\n",
    "\n",
    "TRAINING_ORDER = [\n",
    "    \"cnn1d\",\n",
    "    \"tiny_rnn\",\n",
    "    \"mlp\",\n",
    "    \"tinyconv\",\n",
    "    \"logreg\",\n",
    "    \"rf\",\n",
    "    \"xgboost\",\n",
    "]\n",
    "\n",
    "# ---------- GPU settings ----------\n",
    "USE_GPU         = True          \n",
    "GPU_DEVICE      = \"cuda:0\"\n",
    "FALLBACK_TO_CPU = True          \n",
    "\n",
    "# ---------- Landmarks ----------\n",
    "LANDMARK_ENABLED            = True\n",
    "LANDMARK_SUBSAMPLE_FRACTION = 0.15  \n",
    "LANDMARK_MAX_ROWS           = 1000   \n",
    "LANDMARK_MIN_ROWS           = 50     \n",
    "LANDMARK_TIMEOUT_S          = 60.0   \n",
    "\n",
    "# ---------- Training limits ----------\n",
    "TRAIN_TIMEOUT_S        = 1800.0   \n",
    "MAX_TRAIN_ROWS         = 100_000  \n",
    "MIN_TRAIN_ROWS         = 50     \n",
    "MAX_FEATURES           = 400 \n",
    "MIN_FEATURES           = 2\n",
    "DL_MAX_EPOCHS          = 20\n",
    "DL_BATCH_SIZE          = 128\n",
    "DL_EARLY_STOP_PATIENCE = 3\n",
    "\n",
    "# ---------- Score weights (trade-off metric) ----------\n",
    "SCORE_WEIGHTS = {\n",
    "    \"accuracy\":           0.50,\n",
    "    \"trained_model_size_kb\":      0.25,\n",
    "    \"inference_speed_ms\": 0.10,\n",
    "    \"ram_usage_kb\":       0.15,\n",
    "}\n",
    "\n",
    "OUTPUT_CSV          = \"openml_metadata_created.csv\"\n",
    "\n",
    "CHECKPOINT_DIR      = Path(\"checkpoints\")\n",
    "CHECKPOINT_DATASETS = CHECKPOINT_DIR / \"finished_datasets.txt\"\n",
    "RUN_LOG             = CHECKPOINT_DIR / \"run_log.txt\"\n",
    "\n",
    "\n",
    "CACHE_DIR           = Path(openml.config.cache_directory)\n",
    "CACHE_MAX_GB        = 5.0\n",
    "\n",
    "MAX_THREADS = 4  # None = all available\n",
    "\n",
    "# ---------- OpenML fetch timeouts ----------\n",
    "REQUEST_CONNECT_TIMEOUT = 300.0\n",
    "REQUEST_READ_TIMEOUT    = 300.0\n",
    "FETCH_RETRIES           = 3\n",
    "RETRY_BACKOFF_S         = 3.0\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "print(\"torch present? :\", torch is not None)\n",
    "print(\"XGB available? :\", XGBClassifier is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 \n",
    "\n",
    "def _apply_thread_policy(max_threads: int):\n",
    "    vars_ = [\"OMP_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"MKL_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"]\n",
    "    if max_threads is None or int(max_threads) == -1:\n",
    "        for v in vars_:\n",
    "            os.environ.pop(v, None)\n",
    "        eff = \"unlimited (library default)\"\n",
    "    else:\n",
    "        mt = str(int(max_threads))\n",
    "        for v in vars_:\n",
    "            os.environ[v] = mt\n",
    "        eff = mt\n",
    "    print(f\"[threads] BLAS/OpenMP thread cap: {eff}\")\n",
    "\n",
    "_apply_thread_policy(MAX_THREADS)\n",
    "\n",
    "if torch is not None:\n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        device = torch.device(GPU_DEVICE)\n",
    "        print(f\"[device] Using CUDA device: {device}\")\n",
    "    else:\n",
    "        if USE_GPU and not torch.cuda.is_available():\n",
    "            print(\"[device] USE_GPU=True but CUDA not available → using CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = None\n",
    "    print(\"[device] torch not installed → deep models disabled\")\n",
    "\n",
    "# ---------- logger ----------\n",
    "RUN_LOG_PATH = Path(RUN_LOG)\n",
    "\n",
    "def log(msg: str):\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    print(line)\n",
    "    try:\n",
    "        RUN_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with RUN_LOG_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(line + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------- Checkpoint helpers ----------\n",
    "CHECKPOINT_DATASETS_PATH = Path(CHECKPOINT_DATASETS)\n",
    "\n",
    "def load_finished_datasets():\n",
    "    if not CHECKPOINT_DATASETS_PATH.exists():\n",
    "        return set()\n",
    "    with CHECKPOINT_DATASETS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        ids = [line.strip() for line in f if line.strip()]\n",
    "    return {int(x) for x in ids}\n",
    "\n",
    "def mark_dataset_finished(dataset_id: int):\n",
    "    CHECKPOINT_DATASETS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with CHECKPOINT_DATASETS_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(int(dataset_id)) + \"\\n\")\n",
    "    log(f\"[checkpoint] did={dataset_id} marked as finished\")\n",
    "\n",
    "# ---------- Cache management ----------\n",
    "def _cache_root() -> Path:\n",
    "    return Path(str(openml.config.cache_directory))\n",
    "\n",
    "def _www_root() -> Path:\n",
    "    base = _cache_root()\n",
    "    candidates = [\n",
    "        base / \"org\" / \"openml\" / \"www\",\n",
    "        base / \".openml\" / \"org\" / \"openml\" / \"www\",\n",
    "        base / \"www\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    p = base / \".openml\" / \"org\" / \"openml\" / \"www\"\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _cache_size_bytes() -> int:\n",
    "    root = _cache_root()\n",
    "    total = 0\n",
    "    for p in root.rglob(\"*\"):\n",
    "        try:\n",
    "            if p.is_file():\n",
    "                total += p.stat().st_size\n",
    "        except Exception:\n",
    "            continue\n",
    "    return total / (1024 ** 3)\n",
    "\n",
    "def _purge_www_tree() -> tuple[int, int]:\n",
    "    root = _www_root()\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "    files.sort(key=lambda p: p.stat().st_mtime if p.exists() else 0)\n",
    "    removed_count, removed_bytes = 0, 0\n",
    "    for f in files:\n",
    "        try:\n",
    "            removed_bytes += f.stat().st_size\n",
    "            f.unlink()\n",
    "            removed_count += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    dirs = sorted([p for p in root.rglob(\"*\") if p.is_dir()], reverse=True)\n",
    "    for d in dirs:\n",
    "        try:\n",
    "            d.rmdir()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return\n",
    "\n",
    "def prune_cache_if_needed(cache_dir: Path = CACHE_DIR, max_gb: float = CACHE_MAX_GB):\n",
    "    size_gb = _cache_size_bytes()\n",
    "    if size_gb <= max_gb:\n",
    "        return\n",
    "    log(f\"[cache] size={size_gb:.2f} GB > limit={max_gb:.2f} GB → pruning…\")\n",
    "    _purge_www_tree()\n",
    "    size_gb = _cache_size_bytes()\n",
    "\n",
    "    log(f\"[cache] new size={size_gb:.2f} GB\")\n",
    "\n",
    "# ---------- Memory cleanup ----------\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "    if torch is not None and torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "                torch.cuda.ipc_collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---------- CSV helper ----------\n",
    "def _append_csv(path: str, rows: list[dict]):\n",
    "    if not rows:\n",
    "        return\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_header = not path.exists() or path.stat().st_size == 0\n",
    "    pd.DataFrame(rows).to_csv(path, mode=\"a\", index=False, header=write_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from openml.exceptions import OpenMLServerException\n",
    "\n",
    "if not getattr(requests, \"_timeout_patched\", False):\n",
    "    _orig_session_init = requests.Session.__init__\n",
    "\n",
    "    def _patched_session_init(self, *args, **kwargs):\n",
    "        _orig_session_init(self, *args, **kwargs)\n",
    "\n",
    "        retry_strategy = Retry(\n",
    "            total=FETCH_RETRIES,\n",
    "            backoff_factor=2.0,  # in seconds\n",
    "            status_forcelist=[421, 429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"POST\", \"OPTIONS\"],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "        self.mount(\"https://\", adapter)\n",
    "        self.mount(\"http://\", adapter)\n",
    "\n",
    "        self.headers.update({\"Connection\": \"close\"})\n",
    "\n",
    "    requests.Session.__init__ = _patched_session_init\n",
    "\n",
    "    _orig_request = requests.sessions.Session.request\n",
    "\n",
    "    def _request(self, method, url, **kw):\n",
    "        kw.setdefault(\"timeout\", (REQUEST_CONNECT_TIMEOUT, REQUEST_READ_TIMEOUT))\n",
    "\n",
    "        headers = kw.get(\"headers\") or {}\n",
    "        headers.setdefault(\"Connection\", \"close\")\n",
    "        kw[\"headers\"] = headers\n",
    "\n",
    "        return _orig_request(self, method, url, **kw)\n",
    "\n",
    "    requests.sessions.Session.request = _request\n",
    "\n",
    "    _orig_api_request = requests.api.request\n",
    "\n",
    "    def _api_request(method, url, **kw):\n",
    "        kw.setdefault(\"timeout\", (REQUEST_CONNECT_TIMEOUT, REQUEST_READ_TIMEOUT))\n",
    "        headers = kw.get(\"headers\") or {}\n",
    "        headers.setdefault(\"Connection\", \"close\")\n",
    "        kw[\"headers\"] = headers\n",
    "        return _orig_api_request(method, url, **kw)\n",
    "\n",
    "    requests.api.request = _api_request\n",
    "\n",
    "    requests._timeout_patched = True\n",
    "\n",
    "log(\"[init] Requests timeout patch installed\")\n",
    "\n",
    "from pandas.api.types import (\n",
    "    CategoricalDtype,\n",
    "    is_bool_dtype,\n",
    "    is_numeric_dtype,\n",
    "    is_datetime64_any_dtype,\n",
    ")\n",
    "\n",
    "# ---------- Dtype repair ----------\n",
    "\n",
    "def _repair_categorical_dtypes(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "\n",
    "    for col in X.columns:\n",
    "        dt = X[col].dtype\n",
    "\n",
    "        if isinstance(dt, CategoricalDtype):\n",
    "            X[col] = X[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        if is_datetime64_any_dtype(dt):\n",
    "            X[col] = X[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        if dt == object:\n",
    "            X[col] = X[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        if not (\n",
    "            is_numeric_dtype(dt)\n",
    "            or is_bool_dtype(dt)\n",
    "            or is_datetime64_any_dtype(dt)\n",
    "        ):\n",
    "            X[col] = X[col].astype(str)\n",
    "            continue\n",
    "\n",
    "    return X\n",
    "\n",
    "# ---------- OpenML fetch ----------\n",
    "class TaskNotSupported(Exception):\n",
    "    pass\n",
    "\n",
    "def _get_task_with_retry(task_id: int):\n",
    "    for attempt in range(1, FETCH_RETRIES + 1):\n",
    "        try:\n",
    "            log(f\"[stage] get_task(tid={task_id}) attempt {attempt}/{FETCH_RETRIES}\")\n",
    "            t0 = time.time()\n",
    "            task = openml.tasks.get_task(task_id)\n",
    "            log(f\"[stage] get_task tid={task_id} ✓ in {time.time()-t0:.2f}s\")\n",
    "            return task\n",
    "        except OpenMLServerException as e:\n",
    "            msg = str(e)\n",
    "            if getattr(e, \"code\", None) in (151, 153) or \"Unknown task\" in msg or \"Deprecated task\" in msg:\n",
    "                log(f\"[skip][task_not_supported] tid={task_id} code={getattr(e, 'code', None)} msg={msg}\")\n",
    "                raise TaskNotSupported(msg)\n",
    "            log(f\"[warn] get_task tid={task_id} server error: {e}\")\n",
    "            if attempt == FETCH_RETRIES:\n",
    "                raise\n",
    "            time.sleep(RETRY_BACKOFF_S * attempt)\n",
    "        except Exception as e:\n",
    "            log(f\"[warn] get_task tid={task_id} failed: {e}\")\n",
    "            if attempt == FETCH_RETRIES:\n",
    "                raise\n",
    "            time.sleep(RETRY_BACKOFF_S * attempt)\n",
    "\n",
    "def _get_dataset_with_retry(task):\n",
    "    ds_id = int(task.dataset_id)\n",
    "    for attempt in range(1, FETCH_RETRIES + 1):\n",
    "        try:\n",
    "            log(f\"[stage] get_dataset(did={ds_id}) attempt {attempt}/{FETCH_RETRIES}\")\n",
    "            t0 = time.time()\n",
    "            ds = task.get_dataset() \n",
    "            log(f\"[stage] get_dataset did={ds_id} ✓ in {time.time()-t0:.2f}s\")\n",
    "            return ds\n",
    "        except Exception as e:\n",
    "            log(f\"[warn] get_dataset did={ds_id} failed: {e}\")\n",
    "            if attempt == FETCH_RETRIES:\n",
    "                raise\n",
    "            time.sleep(RETRY_BACKOFF_S)\n",
    "\n",
    "def _get_data_with_retry(dataset, target_name: str):\n",
    "    for attempt in range(1, FETCH_RETRIES + 1):\n",
    "        try:\n",
    "            log(f\"[stage] get_data(target='{target_name}') attempt {attempt}/{FETCH_RETRIES}\")\n",
    "            t0 = time.time()\n",
    "            X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "                target=target_name, dataset_format=\"dataframe\"\n",
    "            )\n",
    "            log(f\"[stage] get_data ✓ in {time.time()-t0:.2f}s \"\n",
    "                f\"(n={getattr(X, 'shape', ['?','?'])[0]}, d={getattr(X,'shape',['?','?'])[1]})\")\n",
    "            return X, y, categorical_indicator, attribute_names\n",
    "        except Exception as e:\n",
    "            log(f\"[warn] get_data failed: {e}\")\n",
    "            if attempt == FETCH_RETRIES:\n",
    "                raise\n",
    "            time.sleep(RETRY_BACKOFF_S)\n",
    "\n",
    "def fetch_task_and_data(task_id: int):\n",
    "    task = _get_task_with_retry(task_id)\n",
    "\n",
    "    dataset = _get_dataset_with_retry(task)\n",
    "    dataset_name = dataset.name\n",
    "\n",
    "    target_name = task.target_name\n",
    "    X, y, categorical_indicator, attribute_names = _get_data_with_retry(dataset, target_name)\n",
    "\n",
    "    X = _repair_categorical_dtypes(X)\n",
    "\n",
    "    return task, dataset, X, y, categorical_indicator, attribute_names, dataset_name\n",
    "\n",
    "# ---------- Preprocessor builder ----------\n",
    "\n",
    "def build_preprocessor(X_train: pd.DataFrame):\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X_train.columns if c not in numeric_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        cat_ohe = OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            sparse_output=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        cat_ohe = OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            sparse=False\n",
    "        )\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", cat_ohe),\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if numeric_cols:\n",
    "        transformers.append((\"num\", numeric_transformer, numeric_cols))\n",
    "    if cat_cols:\n",
    "        transformers.append((\"cat\", categorical_transformer, cat_cols))\n",
    "\n",
    "    if not transformers:\n",
    "        raise ValueError(\"No columns in X_train to build a preprocessor.\")\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    numeric_mask = np.array([col in numeric_cols for col in X_train.columns], dtype=bool)\n",
    "\n",
    "    return preprocessor, numeric_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00002763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------- Dataset mmeta-features ----------\n",
    "\n",
    "def compute_dataset_features(X: pd.DataFrame, y) -> dict:\n",
    "    meta = {}\n",
    "\n",
    "    try:\n",
    "        n_samples, n_features = X.shape\n",
    "    except Exception:\n",
    "        n_samples, n_features = None, None\n",
    "    meta[\"n_samples\"]  = int(n_samples) if n_samples is not None else None\n",
    "    meta[\"n_features\"] = int(n_features) if n_features is not None else None\n",
    "\n",
    "    try:\n",
    "        dtypes = X.dtypes\n",
    "        numeric_mask = [is_numeric_dtype(dt) for dt in dtypes]\n",
    "        n_numeric = int(np.sum(numeric_mask))\n",
    "        n_categorical = int(len(dtypes) - n_numeric)\n",
    "\n",
    "        n_binary = 0\n",
    "        for col in X.columns:\n",
    "            vals = pd.Series(X[col]).dropna().unique()\n",
    "            if len(vals) == 2:\n",
    "                n_binary += 1\n",
    "\n",
    "        meta[\"n_numeric_features\"]     = n_numeric\n",
    "        meta[\"n_categorical_features\"] = n_categorical\n",
    "        meta[\"n_binary_features\"]      = int(n_binary)\n",
    "    except Exception as e:\n",
    "        log(f\"[meta] numeric/categorical error: {e}\")\n",
    "        numeric_mask = None  \n",
    "        meta[\"n_numeric_features\"]     = None\n",
    "        meta[\"n_categorical_features\"] = None\n",
    "        meta[\"n_binary_features\"]      = None\n",
    "\n",
    "    try:\n",
    "        y_arr = np.asarray(y)\n",
    "        le = LabelEncoder()\n",
    "        y_enc = le.fit_transform(y_arr)\n",
    "        classes, counts = np.unique(y_enc, return_counts=True)\n",
    "        n_classes = len(classes)\n",
    "        probs = counts / counts.sum()\n",
    "        class_balance_std = float(probs.std()) if n_classes > 0 else None\n",
    "        class_entropy = float(-(probs * np.log2(probs + 1e-12)).sum()) if n_classes > 0 else None\n",
    "        meta[\"n_classes\"]         = int(n_classes)\n",
    "        meta[\"class_balance_std\"] = class_balance_std\n",
    "        meta[\"class_entropy\"]     = class_entropy\n",
    "    except Exception as e:\n",
    "        log(f\"[meta] class_stats error: {e}\")\n",
    "        meta[\"n_classes\"]         = None\n",
    "        meta[\"class_balance_std\"] = None\n",
    "        meta[\"class_entropy\"]     = None\n",
    "\n",
    "    try:\n",
    "        num_cols = X.select_dtypes(include=[np.number])\n",
    "\n",
    "        mean_var = 0.0\n",
    "        med_var  = 0.0\n",
    "        mean_corr = 0.0\n",
    "        max_corr  = 0.0\n",
    "\n",
    "        if num_cols.shape[1] > 0 and num_cols.shape[0] > 1:\n",
    "            vars_ = num_cols.var(axis=0, ddof=1).values\n",
    "            if np.isfinite(vars_).sum() > 0:\n",
    "                mean_var = float(np.nanmean(vars_))\n",
    "                med_var  = float(np.nanmedian(vars_))\n",
    "\n",
    "            max_corr_features = min(num_cols.shape[1], 50)\n",
    "            corr = num_cols.iloc[:, :max_corr_features].corr().abs().values\n",
    "\n",
    "            upper = corr[np.triu_indices_from(corr, k=1)]\n",
    "            finite_upper = upper[np.isfinite(upper)]\n",
    "            if finite_upper.size > 0:\n",
    "                mean_corr = float(finite_upper.mean())\n",
    "                max_corr  = float(finite_upper.max())\n",
    "\n",
    "        meta[\"mean_feature_variance\"]   = mean_var\n",
    "        meta[\"median_feature_variance\"] = med_var\n",
    "        meta[\"mean_corr_abs\"]           = mean_corr\n",
    "        meta[\"max_corr_abs\"]            = max_corr\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"[meta] variance/corr error: {e}\")\n",
    "        meta[\"mean_feature_variance\"]   = 0.0\n",
    "        meta[\"median_feature_variance\"] = 0.0\n",
    "        meta[\"mean_corr_abs\"]           = 0.0\n",
    "        meta[\"max_corr_abs\"]            = 0.0\n",
    "\n",
    "    try:\n",
    "        # 1) feature_skewness_mean \n",
    "        num_cols = X.select_dtypes(include=[np.number])\n",
    "        if num_cols.shape[1] > 0:\n",
    "            skews = num_cols.skew(axis=0, skipna=True)\n",
    "            skews = skews.replace([np.inf, -np.inf], np.nan)\n",
    "            feature_skewness_mean = float(skews.mean(skipna=True)) if not skews.isna().all() else 0.0\n",
    "        else:\n",
    "            feature_skewness_mean = 0.0\n",
    "        meta[\"feature_skewness_mean\"] = feature_skewness_mean\n",
    "\n",
    "        # 2) feature_kurtosis_mean \n",
    "        if num_cols.shape[1] > 0:\n",
    "            kurts = num_cols.kurt(axis=0, skipna=True)\n",
    "            kurts = kurts.replace([np.inf, -np.inf], np.nan)\n",
    "            feature_kurtosis_mean = float(kurts.mean(skipna=True)) if not kurts.isna().all() else 0.0\n",
    "        else:\n",
    "            feature_kurtosis_mean = 0.0\n",
    "        meta[\"feature_kurtosis_mean\"] = feature_kurtosis_mean\n",
    "\n",
    "        # 3) missing_percentage\n",
    "        if n_samples is not None and n_features is not None and n_samples > 0 and n_features > 0:\n",
    "            total_cells = float(n_samples * n_features)\n",
    "            missing_count = float(X.isna().sum().sum())\n",
    "            missing_percentage = missing_count / total_cells\n",
    "        else:\n",
    "            missing_percentage = 0.0\n",
    "        meta[\"missing_percentage\"] = float(missing_percentage)\n",
    "\n",
    "        # 4) avg_cardinality_categorical\n",
    "        avg_card = 0.0\n",
    "        if numeric_mask is not None:\n",
    "            cat_cols = [col for col, isnum in zip(X.columns, numeric_mask) if not isnum]\n",
    "            if len(cat_cols) > 0:\n",
    "                cards = []\n",
    "                for col in cat_cols:\n",
    "                    try:\n",
    "                        cards.append(X[col].nunique(dropna=True))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if len(cards) > 0:\n",
    "                    avg_card = float(np.mean(cards))\n",
    "        meta[\"avg_cardinality_categorical\"] = avg_card\n",
    "\n",
    "        # 5) complexity_ratio \n",
    "        if n_samples is not None and n_features is not None and n_samples > 0:\n",
    "            complexity_ratio = float(n_features) / float(n_samples)\n",
    "        else:\n",
    "            complexity_ratio = 0.0\n",
    "        meta[\"complexity_ratio\"] = complexity_ratio\n",
    "\n",
    "        # 6) intrinsic_dim_estimate \n",
    "        intrinsic_dim = 0.0\n",
    "        try:\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            if num_cols.shape[1] >= 2 and num_cols.shape[0] >= 5:\n",
    "                X_pca = num_cols.to_numpy(dtype=np.float32)\n",
    "                col_means = np.nanmean(X_pca, axis=0)\n",
    "                inds = np.where(np.isnan(X_pca))\n",
    "                if inds[0].size > 0:\n",
    "                    X_pca[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "                n_components = min(X_pca.shape[0], X_pca.shape[1])\n",
    "                if n_components >= 1:\n",
    "                    pca = PCA(n_components=n_components)\n",
    "                    pca.fit(X_pca)\n",
    "                    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "                    k = int(np.searchsorted(cumsum, 0.95) + 1)\n",
    "                    intrinsic_dim = float(max(1, min(k, n_components)))\n",
    "        except Exception as e_pca:\n",
    "            log(f\"[meta] intrinsic_dim_estimate error: {e_pca}\")\n",
    "            intrinsic_dim = 0.0\n",
    "\n",
    "        meta[\"intrinsic_dim_estimate\"] = intrinsic_dim\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"[meta] extra_features error: {e}\")\n",
    "        meta.setdefault(\"feature_skewness_mean\", 0.0)\n",
    "        meta.setdefault(\"feature_kurtosis_mean\", 0.0)\n",
    "        meta.setdefault(\"missing_percentage\", 0.0)\n",
    "        meta.setdefault(\"avg_cardinality_categorical\", 0.0)\n",
    "        meta.setdefault(\"complexity_ratio\", 0.0)\n",
    "        meta.setdefault(\"intrinsic_dim_estimate\", 0.0)\n",
    "\n",
    "    return meta\n",
    "\n",
    "# ---------- Landmarks  ----------\n",
    "\n",
    "class LandmarkTimeout(Exception):\n",
    "    pass\n",
    "\n",
    "def _safe_stratify_or_none(y_enc: np.ndarray) -> np.ndarray | None:\n",
    "    counts = np.unique(y_enc, return_counts=True)\n",
    "    if counts.min() < 2:\n",
    "        return None\n",
    "    return y_enc\n",
    "\n",
    "def compute_landmarks(X_train: pd.DataFrame, y_train, numeric_mask: np.ndarray | None) -> dict | str:\n",
    "    if not LANDMARK_ENABLED:\n",
    "        return {}\n",
    "\n",
    "    n_rows = X_train.shape[0]\n",
    "    if n_rows < LANDMARK_MIN_ROWS:\n",
    "        log(f\"[landmarks] too few rows: {n_rows} < LANDMARK_MIN_ROWS={LANDMARK_MIN_ROWS}\")\n",
    "        return \"SKIP\"\n",
    "\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_num = X_train.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "        if X_num.shape[1] == 0:\n",
    "            log(\"[landmarks] no numeric columns → label-encoding all features for landmarks\")\n",
    "            X_num = pd.DataFrame(index=X_train.index)\n",
    "            for col in X_train.columns:\n",
    "                s = X_train[col]\n",
    "                if is_numeric_dtype(s):\n",
    "                    X_num[col] = pd.to_numeric(s, errors=\"coerce\").fillna(0)\n",
    "                else:\n",
    "                    le_col = LabelEncoder()\n",
    "                    X_num[col] = le_col.fit_transform(s.astype(str).fillna(\"__NA__\"))\n",
    "    else:\n",
    "        X_num = pd.DataFrame(X_train)\n",
    "\n",
    "    n_sub = min(LANDMARK_MAX_ROWS, int(LANDMARK_SUBSAMPLE_FRACTION * n_rows))\n",
    "    if n_sub < LANDMARK_MIN_ROWS:\n",
    "        n_sub = LANDMARK_MIN_ROWS\n",
    "    n_sub = min(n_sub, n_rows)\n",
    "\n",
    "    idx = RNG.choice(n_rows, size=n_sub, replace=False)\n",
    "    X_num_sub = X_num.iloc[idx].reset_index(drop=True).astype(np.float32)\n",
    "    y_sub = np.asarray(y_train)[idx]\n",
    "\n",
    "    # ---- Handle NaN / inf for LR/KNN/DT/FDR  ----\n",
    "    X_num_sub = X_num_sub.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    vals = X_num_sub.to_numpy(dtype=np.float32)\n",
    "\n",
    "    if np.isnan(vals).any() or not np.isfinite(vals).all():\n",
    "        vals[~np.isfinite(vals)] = np.nan\n",
    "\n",
    "        col_means = np.nanmean(vals, axis=0)\n",
    "\n",
    "        col_means = np.where(np.isnan(col_means), 0.0, col_means)\n",
    "\n",
    "        inds = np.where(np.isnan(vals))\n",
    "        if inds[0].size > 0:\n",
    "            vals[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "        X_num_sub = pd.DataFrame(vals, columns=X_num_sub.columns)\n",
    "\n",
    "        log(\"[landmarks] Missing/inf values imputed with column means (NaNs guaranteed removed) for LR/KNN/DT/FDR\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y_sub)\n",
    "    strat_labels = _safe_stratify_or_none(y_enc)\n",
    "\n",
    "    landmarks = {\n",
    "        \"landmark_lr_accuracy\": None,\n",
    "        \"landmark_dt_depth3_accuracy\": None,\n",
    "        \"landmark_knn3_accuracy\": None,\n",
    "        \"landmark_random_noise_accuracy\": None,\n",
    "        \"fisher_discriminant_ratio\": None,\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    def check_timeout():\n",
    "        if time.time() - start > LANDMARK_TIMEOUT_S:\n",
    "            raise LandmarkTimeout(f\"Landmarks exceeded {LANDMARK_TIMEOUT_S:.1f}s\")\n",
    "\n",
    "    # 1) Logistic Regression \n",
    "    try:\n",
    "        check_timeout()\n",
    "        if X_num_sub.shape[1] == 0:\n",
    "            raise RuntimeError(\"No numeric features for LR landmark\")\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_num_sub, y_enc, test_size=0.2,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=strat_labels,\n",
    "        )\n",
    "        clf = LogisticRegression(max_iter=50, C=0.1, solver=\"lbfgs\")\n",
    "        clf.fit(Xtr, ytr)\n",
    "        acc = accuracy_score(yte, clf.predict(Xte))\n",
    "        landmarks[\"landmark_lr_accuracy\"] = float(acc)\n",
    "        log(f\"[landmark] lr_accuracy ✓ acc={acc:.4f}\")\n",
    "    except LandmarkTimeout:\n",
    "        log(\"[landmarks] timeout during LR\")\n",
    "        return \"SKIP\"\n",
    "    except Exception as e:\n",
    "        log(f\"[landmark] lr_accuracy error: {e}\")\n",
    "        landmarks[\"landmark_lr_accuracy\"] = None\n",
    "\n",
    "    # 2) DecisionTree \n",
    "    try:\n",
    "        check_timeout()\n",
    "        if X_num_sub.shape[1] == 0:\n",
    "            raise RuntimeError(\"No numeric features for DT landmark\")\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_num_sub, y_enc, test_size=0.2,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=strat_labels,\n",
    "        )\n",
    "        clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=RANDOM_SEED)\n",
    "        clf.fit(Xtr, ytr)\n",
    "        acc = accuracy_score(yte, clf.predict(Xte))\n",
    "        landmarks[\"landmark_dt_depth3_accuracy\"] = float(acc)\n",
    "        log(f\"[landmark] dt_depth3_accuracy ✓ acc={acc:.4f}\")\n",
    "    except LandmarkTimeout:\n",
    "        log(\"[landmarks] timeout during DT\")\n",
    "        return \"SKIP\"\n",
    "    except Exception as e:\n",
    "        log(f\"[landmark] dt_depth3_accuracy error: {e}\")\n",
    "        landmarks[\"landmark_dt_depth3_accuracy\"] = None\n",
    "\n",
    "    # 3) KNN-3 \n",
    "    try:\n",
    "        check_timeout()\n",
    "        if X_num_sub.shape[1] == 0:\n",
    "            raise RuntimeError(\"No numeric features for KNN landmark\")\n",
    "        X_knn = X_num_sub\n",
    "        if X_knn.shape[1] > 30:\n",
    "            cols = RNG.choice(X_knn.shape[1], size=30, replace=False)\n",
    "            X_knn = X_knn.iloc[:, cols]\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_knn, y_enc, test_size=0.2,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=strat_labels,\n",
    "        )\n",
    "        clf = KNeighborsClassifier(n_neighbors=3)\n",
    "        clf.fit(Xtr, ytr)\n",
    "        acc = accuracy_score(yte, clf.predict(Xte))\n",
    "        landmarks[\"landmark_knn3_accuracy\"] = float(acc)\n",
    "        log(f\"[landmark] knn3_accuracy ✓ acc={acc:.4f}\")\n",
    "    except LandmarkTimeout:\n",
    "        log(\"[landmarks] timeout during KNN\")\n",
    "        return \"SKIP\"\n",
    "    except Exception as e:\n",
    "        log(f\"[landmark] knn3_accuracy error: {e}\")\n",
    "        landmarks[\"landmark_knn3_accuracy\"] = None\n",
    "\n",
    "    # 4) Random noise baseline \n",
    "    try:\n",
    "        check_timeout()\n",
    "        probs = np.bincount(y_enc) / len(y_enc)\n",
    "        preds = RNG.choice(np.arange(len(probs)), size=len(y_enc), p=probs)\n",
    "        acc = accuracy_score(y_enc, preds)\n",
    "        landmarks[\"landmark_random_noise_accuracy\"] = float(acc)\n",
    "        log(f\"[landmark] random_noise_accuracy ✓ acc={acc:.4f}\")\n",
    "    except LandmarkTimeout:\n",
    "        log(\"[landmarks] timeout during random baseline\")\n",
    "        return \"SKIP\"\n",
    "    except Exception as e:\n",
    "        log(f\"[landmark] random_noise_accuracy error: {e}\")\n",
    "        landmarks[\"landmark_random_noise_accuracy\"] = None\n",
    "\n",
    "    # 5) Fisher Discriminant Ratio\n",
    "    try:\n",
    "        check_timeout()\n",
    "        if X_num_sub.shape[1] == 0:\n",
    "            raise RuntimeError(\"No numeric features for FDR\")\n",
    "        fdr_values = []\n",
    "        for j in range(X_num_sub.shape[1]):\n",
    "            xj = X_num_sub.iloc[:, j].values.astype(float)\n",
    "            mu = xj.mean()\n",
    "            num = 0.0\n",
    "            den = 0.0\n",
    "            for c in np.unique(y_enc):\n",
    "                mask_c = (y_enc == c)\n",
    "                xc = xj[mask_c]\n",
    "                if xc.size == 0:\n",
    "                    continue\n",
    "                nc = xc.size\n",
    "                mu_c = xc.mean()\n",
    "                var_c = xc.var(ddof=1) if nc > 1 else 0.0\n",
    "                num += nc * (mu_c - mu) ** 2\n",
    "                den += nc * var_c\n",
    "            if den > 0:\n",
    "                fdr_values.append(num / (den + 1e-12))\n",
    "        fdr = float(np.mean(fdr_values)) if fdr_values else None\n",
    "        landmarks[\"fisher_discriminant_ratio\"] = fdr\n",
    "        log(f\"[landmark] fisher_discriminant_ratio ✓ value={fdr}\")\n",
    "    except LandmarkTimeout:\n",
    "        log(\"[landmarks] timeout during FDR\")\n",
    "        return \"SKIP\"\n",
    "    except Exception as e:\n",
    "        log(f\"[landmark] fisher_discriminant_ratio error: {e}\")\n",
    "        landmarks[\"fisher_discriminant_ratio\"] = None\n",
    "\n",
    "    log(f\"[landmarks] done in {time.time()-start:.2f}s\")\n",
    "    return landmarks\n",
    "\n",
    "def compute_meta(X_train: pd.DataFrame, y_train, numeric_mask: np.ndarray):\n",
    "    meta_ds = compute_dataset_features(X_train, y_train)\n",
    "    lm = compute_landmarks(X_train, y_train, numeric_mask)\n",
    "    if lm == \"SKIP\":\n",
    "        return None\n",
    "    if isinstance(lm, dict):\n",
    "        meta_ds.update(lm)\n",
    "    return meta_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1381c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 \n",
    "\n",
    "_JOBS = -1 if (MAX_THREADS is None or int(MAX_THREADS) == -1) else int(MAX_THREADS)\n",
    "\n",
    "def make_logreg():\n",
    "    return LogisticRegression(\n",
    "        max_iter=800,\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=_JOBS if hasattr(LogisticRegression(), \"n_jobs\") else None,\n",
    "    )\n",
    "\n",
    "def make_rf():\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=_JOBS,\n",
    "    )\n",
    "\n",
    "def make_xgboost():\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "    return xgb.XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=_JOBS,\n",
    "    )\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TinyConv1DNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(16)\n",
    "        self.fc = nn.Linear(8 * 16, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)          \n",
    "        x = self.conv(x)           \n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)            \n",
    "        x = x.view(x.size(0), -1)   \n",
    "        return self.fc(x)\n",
    "\n",
    "class TinyConvNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 4, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(8)\n",
    "        self.fc = nn.Linear(4 * 8, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)          \n",
    "        x = self.conv(x)           \n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)            \n",
    "        x = x.view(x.size(0), -1)   \n",
    "        return self.fc(x)\n",
    "\n",
    "class TinyRNNNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 32\n",
    "        self.rnn = nn.GRU(input_size=1, hidden_size=self.hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        out, h = self.rnn(x)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "def make_cnn1d(input_dim, n_classes):\n",
    "    if device is None:\n",
    "        return None\n",
    "    model = TinyConv1DNet(input_dim, n_classes)\n",
    "    return model.to(device)\n",
    "\n",
    "def make_tinyconv(input_dim, n_classes):\n",
    "\n",
    "    if device is None:\n",
    "        return None\n",
    "    model = TinyConvNet(input_dim, n_classes)\n",
    "    return model.to(device)\n",
    "\n",
    "def make_tiny_rnn(input_dim, n_classes):\n",
    "    if device is None:\n",
    "        return None\n",
    "    model = TinyRNNNet(input_dim, n_classes)\n",
    "    return model.to(device)\n",
    "\n",
    "def make_mlp(input_dim, n_classes):\n",
    "    if device is None:\n",
    "        return None\n",
    "    model = MLPNet(input_dim, n_classes)\n",
    "    return model.to(device)\n",
    "\n",
    "MODELS = {\n",
    "    \"logreg\":   (\"classic\", make_logreg),\n",
    "    \"rf\":       (\"classic\", make_rf),\n",
    "    \"xgboost\":  (\"classic\", make_xgboost),\n",
    "    \"cnn1d\":    (\"deep\",    make_cnn1d),\n",
    "    \"tiny_rnn\": (\"deep\",    make_tiny_rnn),\n",
    "    \"mlp\":      (\"deep\",    make_mlp),\n",
    "    \"tinyconv\": (\"deep\",    make_tinyconv),\n",
    "}\n",
    "\n",
    "MODEL_IDS = {\n",
    "    \"logreg\":   1,\n",
    "    \"rf\":       2,\n",
    "    \"xgboost\":  3,\n",
    "    \"cnn1d\":    4,\n",
    "    \"tiny_rnn\": 5,\n",
    "    \"mlp\":      6,\n",
    "    \"tinyconv\": 7,\n",
    "}\n",
    "\n",
    "# ---------- Model capabilities & static model features ----------\n",
    "\n",
    "MODEL_CAPABILITIES = {\n",
    "    \"logreg\": {\n",
    "        \"model_id\":          MODEL_IDS[\"logreg\"],\n",
    "        \"model_name\":        \"logreg\",\n",
    "        \"is_deep_learning\":  False,\n",
    "        \"is_tree_based\":     False,\n",
    "        \"is_linear\":         True,\n",
    "        \"model_family\":      \"Linear\",\n",
    "\n",
    "        \"parameterization_type\":        \"linear-in-features\",\n",
    "        \"complexity_training_big_o\":    \"O(n · d)\",\n",
    "        \"complexity_inference_big_o\":   \"O(d)\",\n",
    "        \"is_probabilistic\":             True,\n",
    "        \"is_ensemble_model\":            False,\n",
    "        \"regularization_supported\":     \"L2\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  False,\n",
    "        \"tree_growth_strategy\":     \"none\",\n",
    "        \"default_max_depth\":        0,\n",
    "        \"supports_pruning\":         False,\n",
    "        \"splitting_criterion\":      \"none\",\n",
    "\n",
    "        \"architecture_type\":          \"none\",\n",
    "        \"supports_dropout\":           False,\n",
    "        \"supports_batchnorm\":         False,\n",
    "        \"default_activation\":         \"none\",\n",
    "        \"supports_cuda_acceleration\": False,\n",
    "\n",
    "        \"supports_non_linearity\":        False,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       False,\n",
    "        \"supports_gpu\":                  False,\n",
    "\n",
    "        \"n_estimators\":     0,\n",
    "        \"avg_tree_depth\":   0.0,\n",
    "        \"max_tree_depth\":   0,\n",
    "        \"n_leaves_mean\":    0.0,\n",
    "\n",
    "        \"n_layers\":             0,\n",
    "        \"hidden_units_mean\":    0.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"none\",\n",
    "        \"batch_size\":           0,\n",
    "        \"epochs\":               0,\n",
    "    },\n",
    "\n",
    "    \"rf\": {\n",
    "        \"model_id\":          MODEL_IDS[\"rf\"],\n",
    "        \"model_name\":        \"rf\",\n",
    "        \"is_deep_learning\":  False,\n",
    "        \"is_tree_based\":     True,\n",
    "        \"is_linear\":         False,\n",
    "        \"model_family\":      \"TreeEnsemble\",\n",
    "\n",
    "        \"parameterization_type\":        \"fixed-per-estimator\",\n",
    "        \"complexity_training_big_o\":    \"O(n · log n · trees)\",\n",
    "        \"complexity_inference_big_o\":   \"O(trees · depth)\",\n",
    "        \"is_probabilistic\":             True,   \n",
    "        \"is_ensemble_model\":            True,\n",
    "        \"regularization_supported\":     \"None\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  True,\n",
    "        \"tree_growth_strategy\":     \"depth-based\",\n",
    "        \"default_max_depth\":        0,            # 0 = unlimited\n",
    "        \"supports_pruning\":         False,\n",
    "        \"splitting_criterion\":      \"gini\",\n",
    "\n",
    "        \"architecture_type\":          \"none\",\n",
    "        \"supports_dropout\":           False,\n",
    "        \"supports_batchnorm\":         False,\n",
    "        \"default_activation\":         \"none\",\n",
    "        \"supports_cuda_acceleration\": False,\n",
    "\n",
    "        \"supports_non_linearity\":        True,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       False,\n",
    "        \"supports_gpu\":                  False,\n",
    "\n",
    "        \"n_estimators\":     200,\n",
    "        \"avg_tree_depth\":   0.0,   # 0 = dataset-dependent\n",
    "        \"max_tree_depth\":   0,     # 0 = None\n",
    "        \"n_leaves_mean\":    0.0,   # 0 = dataset-dependent\n",
    "\n",
    "        \"n_layers\":             0,\n",
    "        \"hidden_units_mean\":    0.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"none\",\n",
    "        \"batch_size\":           0,\n",
    "        \"epochs\":               0,\n",
    "    },\n",
    "\n",
    "    \"xgboost\": {\n",
    "        \"model_id\":          MODEL_IDS[\"xgboost\"],\n",
    "        \"model_name\":        \"xgboost\",\n",
    "        \"is_deep_learning\":  False,\n",
    "        \"is_tree_based\":     True,\n",
    "        \"is_linear\":         False,\n",
    "        \"model_family\":      \"BoostedTrees\",\n",
    "\n",
    "        \"parameterization_type\":        \"fixed-per-estimator\",\n",
    "        \"complexity_training_big_o\":    \"O(n · log n · trees)\",\n",
    "        \"complexity_inference_big_o\":   \"O(trees · depth)\",\n",
    "        \"is_probabilistic\":             True,\n",
    "        \"is_ensemble_model\":            True,\n",
    "        \"regularization_supported\":     \"L1/L2\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  True,\n",
    "        \"tree_growth_strategy\":     \"leaf-based\",\n",
    "        \"default_max_depth\":        6,\n",
    "        \"supports_pruning\":         True,\n",
    "        \"splitting_criterion\":      \"gain\",\n",
    "\n",
    "        \"architecture_type\":          \"none\",\n",
    "        \"supports_dropout\":           False,\n",
    "        \"supports_batchnorm\":         False,\n",
    "        \"default_activation\":         \"none\",\n",
    "        \"supports_cuda_acceleration\": True,   \n",
    "\n",
    "        \"supports_non_linearity\":        True,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       True,\n",
    "        \"supports_gpu\":                  True,\n",
    "\n",
    "        \"n_estimators\":     200,\n",
    "        \"avg_tree_depth\":   6.0,   \n",
    "        \"max_tree_depth\":   6,\n",
    "        \"n_leaves_mean\":    0.0,  \n",
    "\n",
    "        \"n_layers\":             0,\n",
    "        \"hidden_units_mean\":    0.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"none\",\n",
    "        \"batch_size\":           0,\n",
    "        \"epochs\":               0,\n",
    "    },\n",
    "\n",
    "    \"cnn1d\": {\n",
    "        \"model_id\":          MODEL_IDS[\"cnn1d\"],\n",
    "        \"model_name\":        \"cnn1d\",\n",
    "        \"is_deep_learning\":  True,\n",
    "        \"is_tree_based\":     False,\n",
    "        \"is_linear\":         False,\n",
    "        \"model_family\":      \"CNN\",\n",
    "\n",
    "        \"parameterization_type\":        \"linear-in-features\",\n",
    "        \"complexity_training_big_o\":    \"O(n · d · epochs)\",\n",
    "        \"complexity_inference_big_o\":   \"O(d · filters)\",\n",
    "        \"is_probabilistic\":             True,   \n",
    "        \"is_ensemble_model\":            False,\n",
    "        \"regularization_supported\":     \"L2\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  False,\n",
    "        \"tree_growth_strategy\":     \"none\",\n",
    "        \"default_max_depth\":        0,\n",
    "        \"supports_pruning\":         False,\n",
    "        \"splitting_criterion\":      \"none\",\n",
    "\n",
    "        \"architecture_type\":          \"CNN1D\",\n",
    "        \"supports_dropout\":           False,   \n",
    "        \"supports_batchnorm\":         False,    \n",
    "        \"default_activation\":         \"relu\",\n",
    "        \"supports_cuda_acceleration\": True,     \n",
    "\n",
    "        \"supports_non_linearity\":        True,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       False,\n",
    "        \"supports_gpu\":                  True,\n",
    "\n",
    "        \"n_estimators\":     0,\n",
    "        \"avg_tree_depth\":   0.0,\n",
    "        \"max_tree_depth\":   0,\n",
    "        \"n_leaves_mean\":    0.0,\n",
    "\n",
    "        \"n_layers\":             2,\n",
    "        \"hidden_units_mean\":    8.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"relu\",\n",
    "        \"batch_size\":           DL_BATCH_SIZE,\n",
    "        \"epochs\":               DL_MAX_EPOCHS,\n",
    "    },\n",
    "\n",
    "    \"tiny_rnn\": {\n",
    "        \"model_id\":          MODEL_IDS[\"tiny_rnn\"],\n",
    "        \"model_name\":        \"tiny_rnn\",\n",
    "        \"is_deep_learning\":  True,\n",
    "        \"is_tree_based\":     False,\n",
    "        \"is_linear\":         False,\n",
    "        \"model_family\":      \"RNN\",\n",
    "\n",
    "        \"parameterization_type\":        \"linear-in-features\",\n",
    "        \"complexity_training_big_o\":    \"O(n · d · hidden_dim · epochs)\",\n",
    "        \"complexity_inference_big_o\":   \"O(d · hidden_dim)\",\n",
    "        \"is_probabilistic\":             True,\n",
    "        \"is_ensemble_model\":            False,\n",
    "        \"regularization_supported\":     \"L2\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  False,\n",
    "        \"tree_growth_strategy\":     \"none\",\n",
    "        \"default_max_depth\":        0,\n",
    "        \"supports_pruning\":         False,\n",
    "        \"splitting_criterion\":      \"none\",\n",
    "\n",
    "        \"architecture_type\":          \"RNN-GRU\",\n",
    "        \"supports_dropout\":           False,\n",
    "        \"supports_batchnorm\":         False,\n",
    "        \"default_activation\":         \"tanh\",\n",
    "        \"supports_cuda_acceleration\": True,\n",
    "\n",
    "        \"supports_non_linearity\":        True,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       False,\n",
    "        \"supports_gpu\":                  True,\n",
    "\n",
    "        \"n_estimators\":     0,\n",
    "        \"avg_tree_depth\":   0.0,\n",
    "        \"max_tree_depth\":   0,\n",
    "        \"n_leaves_mean\":    0.0,\n",
    "\n",
    "        \"n_layers\":             2,\n",
    "        \"hidden_units_mean\":    32.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"tanh\",\n",
    "        \"batch_size\":           DL_BATCH_SIZE,\n",
    "        \"epochs\":               DL_MAX_EPOCHS,\n",
    "    },\n",
    "\n",
    "    \"mlp\": {\n",
    "        \"model_id\":          MODEL_IDS[\"mlp\"],\n",
    "        \"model_name\":        \"mlp\",\n",
    "        \"is_deep_learning\":  True,\n",
    "        \"is_tree_based\":     False,\n",
    "        \"is_linear\":         False,\n",
    "        \"model_family\":      \"MLP\",\n",
    "\n",
    "        \"parameterization_type\":        \"linear-in-features\",\n",
    "        \"complexity_training_big_o\":    \"O(n · Σ(layer_dims) · epochs)\",\n",
    "        \"complexity_inference_big_o\":   \"O(Σ(layer_dims))\",\n",
    "        \"is_probabilistic\":             True,\n",
    "        \"is_ensemble_model\":            False,\n",
    "        \"regularization_supported\":     \"L2\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  False,\n",
    "        \"tree_growth_strategy\":     \"none\",\n",
    "        \"default_max_depth\":        0,\n",
    "        \"supports_pruning\":         False,\n",
    "        \"splitting_criterion\":      \"none\",\n",
    "\n",
    "        \"architecture_type\":          \"MLP\",\n",
    "        \"supports_dropout\":           False,\n",
    "        \"supports_batchnorm\":         False,\n",
    "        \"default_activation\":         \"relu\",\n",
    "        \"supports_cuda_acceleration\": True,\n",
    "\n",
    "        \"supports_non_linearity\":        True,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       False,\n",
    "        \"supports_gpu\":                  True,\n",
    "\n",
    "        \"n_estimators\":     0,\n",
    "        \"avg_tree_depth\":   0.0,\n",
    "        \"max_tree_depth\":   0,\n",
    "        \"n_leaves_mean\":    0.0,\n",
    "\n",
    "        \"n_layers\":             3,\n",
    "        \"hidden_units_mean\":    (128.0 + 64.0) / 2.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"relu\",\n",
    "        \"batch_size\":           DL_BATCH_SIZE,\n",
    "        \"epochs\":               DL_MAX_EPOCHS,\n",
    "    },\n",
    "\n",
    "    \"tinyconv\": {\n",
    "        \"model_id\":          MODEL_IDS[\"tinyconv\"],\n",
    "        \"model_name\":        \"tinyconv\",\n",
    "        \"is_deep_learning\":  True,\n",
    "        \"is_tree_based\":     False,\n",
    "        \"is_linear\":         False,\n",
    "        \"model_family\":      \"CNN\",\n",
    "\n",
    "        \"parameterization_type\":        \"linear-in-features\",\n",
    "        \"complexity_training_big_o\":    \"O(n · d · epochs)\",\n",
    "        \"complexity_inference_big_o\":   \"O(d · filters)\",\n",
    "        \"is_probabilistic\":             True,\n",
    "        \"is_ensemble_model\":            False,\n",
    "        \"regularization_supported\":     \"L2\",\n",
    "        \"supports_multiclass_natively\": True,\n",
    "        \"supports_online_learning\":     False,\n",
    "\n",
    "        \"supports_multiple_trees\":  False,\n",
    "        \"tree_growth_strategy\":     \"none\",\n",
    "        \"default_max_depth\":        0,\n",
    "        \"supports_pruning\":         False,\n",
    "        \"splitting_criterion\":      \"none\",\n",
    "\n",
    "        \"architecture_type\":          \"CNN1D\",\n",
    "        \"supports_dropout\":           False,\n",
    "        \"supports_batchnorm\":         False,\n",
    "        \"default_activation\":         \"relu\",\n",
    "        \"supports_cuda_acceleration\": True,\n",
    "\n",
    "        \"supports_non_linearity\":        True,\n",
    "        \"supports_categorical_directly\": False,\n",
    "        \"supports_missing_values\":       False,\n",
    "        \"supports_gpu\":                  True,\n",
    "\n",
    "        \"n_estimators\":     0,\n",
    "        \"avg_tree_depth\":   0.0,\n",
    "        \"max_tree_depth\":   0,\n",
    "        \"n_leaves_mean\":    0.0,\n",
    "\n",
    "        \"n_layers\":             2,\n",
    "        \"hidden_units_mean\":    4.0,\n",
    "        \"dropout_rate_mean\":    0.0,\n",
    "        \"activation_type\":      \"relu\",\n",
    "        \"batch_size\":           DL_BATCH_SIZE,\n",
    "        \"epochs\":               DL_MAX_EPOCHS,\n",
    "    },\n",
    "}\n",
    "\n",
    "# ---------- Utility ----------\n",
    "\n",
    "def count_parameters(model) -> int:\n",
    "    try:\n",
    "        if torch is not None and isinstance(model, nn.Module):\n",
    "            return sum(p.numel() for p in model.parameters())\n",
    "        if hasattr(model, \"coef_\"):\n",
    "            return int(np.prod(model.coef_.shape))\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            return sum(\n",
    "                getattr(est, \"tree_\", None).node_count\n",
    "                for est in model.estimators_\n",
    "                if getattr(est, \"tree_\", None) is not None\n",
    "            )\n",
    "        return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# ---------- Training  ----------\n",
    "\n",
    "def train_and_eval_model(\n",
    "    model_name: str,\n",
    "    preprocessor,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test,\n",
    "):\n",
    "    \n",
    "    model_type, ctor = MODELS[model_name]\n",
    "    if ctor is None:\n",
    "        log(f\"[model] {model_name} ctor=None → skipping\")\n",
    "        return None\n",
    "\n",
    "    proc = psutil.Process(os.getpid())\n",
    "\n",
    "    try:\n",
    "        if model_type == \"classic\":\n",
    "            base_model = ctor()\n",
    "            if base_model is None:\n",
    "                log(f\"[model] {model_name} unavailable (e.g., xgboost missing)\")\n",
    "                return None\n",
    "\n",
    "            pipe = Pipeline([(\"preprocess\", preprocessor), (\"clf\", base_model)])\n",
    "\n",
    "            y_train_local = np.asarray(y_train)\n",
    "            y_test_local  = np.asarray(y_test)\n",
    "\n",
    "            if model_name == \"xgboost\":\n",
    "                le = LabelEncoder()\n",
    "                y_train_local = le.fit_transform(y_train_local)\n",
    "                y_test_local  = le.transform(y_test_local)\n",
    "\n",
    "                classes = np.unique(y_train_local)\n",
    "                n_classes = len(classes)\n",
    "\n",
    "                if n_classes < 2:\n",
    "                    log(\n",
    "                        f\"[warn][model] did model=xgboost skipped: \"\n",
    "                        f\"only {n_classes} distinct class(es) in y_train\"\n",
    "                    )\n",
    "                    return None\n",
    "\n",
    "                try:\n",
    "                    if n_classes == 2:\n",
    "                        pipe.set_params(clf__objective=\"binary:logistic\")\n",
    "                        try:\n",
    "                            pipe.set_params(clf__num_class=None)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pipe.set_params(\n",
    "                            clf__objective=\"multi:softprob\",\n",
    "                            clf__num_class=int(n_classes),\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    log(f\"[warn][model] xgboost set_params failed: {e}\")\n",
    "\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                pipe.fit(X_train, y_train_local)\n",
    "            except Exception as e:\n",
    "                log(f\"[error][model_fit] model={model_name} error={type(e).__name__}: {e}\")\n",
    "                return None\n",
    "\n",
    "            training_time = time.time() - t0\n",
    "\n",
    "            # ---- Evaluate on test ----\n",
    "\n",
    "            # Capture CPU RAM before inference\n",
    "            rss_before = proc.memory_info().rss\n",
    "\n",
    "            t1 = time.time()\n",
    "            y_pred = pipe.predict(X_test)\n",
    "            infer_time = time.time() - t1\n",
    "\n",
    "            # Capture CPU RAM after inference\n",
    "            rss_after = proc.memory_info().rss\n",
    "            \n",
    "            # Dynamic RAM: extra KB used temporarily during inference\n",
    "            dynamic_ram_kb = max(0.0, (rss_after - rss_before) / 1024.0) \n",
    "\n",
    "            n_test = len(y_test_local)\n",
    "            inf_ms_per_sample = (infer_time / max(1, n_test)) * 1000.0\n",
    "\n",
    "            acc = accuracy_score(y_test_local, y_pred)\n",
    "            f1m = f1_score(y_test_local, y_pred, average=\"macro\", zero_division=0)\n",
    "            pm = precision_score(y_test_local, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "            # Serialize model for size\n",
    "            import tempfile, joblib\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\") as tmp:\n",
    "                tmp_path = Path(tmp.name)\n",
    "            joblib.dump(pipe, tmp_path)\n",
    "            size_kb = tmp_path.stat().st_size / 1024.0\n",
    "            tmp_path.unlink(missing_ok=True)\n",
    "\n",
    "            n_params = count_parameters(pipe.named_steps[\"clf\"])\n",
    "\n",
    "            static_ram_kb = size_kb\n",
    "\n",
    "            total_peak_ram_kb = static_ram_kb + dynamic_ram_kb\n",
    "\n",
    "            result = {\n",
    "                \"model_name\": model_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"f1_macro\": f1m,\n",
    "                \"precision_macro\": pm,\n",
    "                \"trained_model_size_kb\": size_kb,\n",
    "                \"inference_speed_ms\": inf_ms_per_sample,\n",
    "                \"ram_usage_kb\": total_peak_ram_kb,               \n",
    "                \"static_ram_kb\": static_ram_kb,             \n",
    "                \"dynamic_ram_kb\": dynamic_ram_kb,           \n",
    "                \"training_time_seconds\": training_time,\n",
    "                \"inference_time_ms_per_sample\": inf_ms_per_sample,\n",
    "                \"model_n_parameters\": n_params,\n",
    "            }\n",
    "\n",
    "            log(\n",
    "                f\"[model] did model={model_name} \"\n",
    "                f\"train={training_time:.2f}s inf={inf_ms_per_sample:.3f}ms/row \"\n",
    "                f\"size={size_kb:.1f}KB acc={acc:.4f} f1={f1m:.4f}\"\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            if torch is None or device is None:\n",
    "                log(f\"[model] {model_name} deep model but torch/device unavailable → skipping\")\n",
    "                return None\n",
    "\n",
    "            from sklearn.base import clone as sk_clone\n",
    "            preproc_dl = sk_clone(preprocessor)\n",
    "\n",
    "            def _train_eval_deep_on_device(run_device: torch.device):\n",
    "                t0_p = time.time()\n",
    "                X_train_trans = preproc_dl.fit_transform(X_train)\n",
    "                X_test_trans  = preproc_dl.transform(X_test)\n",
    "\n",
    "                if hasattr(X_train_trans, \"toarray\"):\n",
    "                    X_train_arr = X_train_trans.astype(np.float32).toarray()\n",
    "                    X_test_arr  = X_test_trans.astype(np.float32).toarray()\n",
    "                else:\n",
    "                    X_train_arr = np.asarray(X_train_trans, dtype=np.float32)\n",
    "                    X_test_arr  = np.asarray(X_test_trans,  dtype=np.float32)\n",
    "                    \n",
    "                log(f\"[dl] preprocessor for {model_name} fit+transform on {run_device} in {time.time()-t0_p:.2f}s\")\n",
    "\n",
    "                X_train_arr = np.asarray(X_train_trans, dtype=np.float32)\n",
    "                X_test_arr  = np.asarray(X_test_trans, dtype=np.float32)\n",
    "\n",
    "                le = LabelEncoder()\n",
    "                y_train_enc = le.fit_transform(y_train)\n",
    "                y_test_enc  = le.transform(y_test)\n",
    "\n",
    "                n_classes = len(le.classes_)\n",
    "                input_dim = X_train_arr.shape[1]\n",
    "\n",
    "                mdl = ctor(input_dim, n_classes)\n",
    "                if mdl is None:\n",
    "                    log(f\"[model] {model_name} ctor returned None → skipping\")\n",
    "                    return None\n",
    "                mdl = mdl.to(run_device)\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "\n",
    "                train_ds = TensorDataset(\n",
    "                    torch.from_numpy(X_train_arr),\n",
    "                    torch.from_numpy(y_train_enc.astype(np.int64)),\n",
    "                )\n",
    "                test_ds = TensorDataset(\n",
    "                    torch.from_numpy(X_test_arr),\n",
    "                    torch.from_numpy(y_test_enc.astype(np.int64)),\n",
    "                )\n",
    "                train_loader = DataLoader(train_ds, batch_size=DL_BATCH_SIZE, shuffle=True)\n",
    "                test_loader  = DataLoader(test_ds, batch_size=DL_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                best_val_acc = -np.inf\n",
    "                best_state = None\n",
    "                no_improve = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "                for epoch in range(DL_MAX_EPOCHS):\n",
    "                    mdl.train()\n",
    "                    for xb, yb in train_loader:\n",
    "                        xb = xb.to(run_device)\n",
    "                        yb = yb.to(run_device)\n",
    "                        optimizer.zero_grad()\n",
    "                        out = mdl(xb)\n",
    "                        loss = criterion(out, yb)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    mdl.eval()\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    with torch.no_grad():\n",
    "                        for xb, yb in test_loader:\n",
    "                            xb = xb.to(run_device)\n",
    "                            yb = yb.to(run_device)\n",
    "                            out = mdl(xb)\n",
    "                            preds = out.argmax(dim=1)\n",
    "                            correct += (preds == yb).sum().item()\n",
    "                            total += yb.size(0)\n",
    "                    val_acc = correct / max(1, total)\n",
    "                    if val_acc > best_val_acc + 1e-4:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_state = {k: v.cpu().clone() for k, v in mdl.state_dict().items()}\n",
    "                        no_improve = 0\n",
    "                    else:\n",
    "                        no_improve += 1\n",
    "                        if no_improve >= DL_EARLY_STOP_PATIENCE:\n",
    "                            break\n",
    "\n",
    "                training_time = time.time() - t0\n",
    "\n",
    "                if best_state is not None:\n",
    "                    mdl.load_state_dict(best_state)\n",
    "                    mdl = mdl.to(run_device)\n",
    "\n",
    "                # ---- Evaluate on test ----\n",
    "                rss_before = proc.memory_info().rss\n",
    "                if torch.cuda.is_available():\n",
    "                    try:\n",
    "                        torch.cuda.reset_peak_memory_stats(run_device)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                mdl.eval()\n",
    "                all_preds = []\n",
    "                all_true  = []\n",
    "                t1 = time.time()\n",
    "                with torch.no_grad():\n",
    "                    for xb, yb in test_loader:\n",
    "                        xb = xb.to(run_device)\n",
    "                        yb = yb.to(run_device)\n",
    "                        out = mdl(xb)\n",
    "                        preds = out.argmax(dim=1).cpu().numpy()\n",
    "                        all_preds.append(preds)\n",
    "                        all_true.append(yb.cpu().numpy())\n",
    "                infer_time = time.time() - t1\n",
    "\n",
    "                rss_after = proc.memory_info().rss\n",
    "                cpu_delta_kb = max(0.0, (rss_after - rss_before) / 1024.0)\n",
    "\n",
    "                vram_delta_kb = 0.0\n",
    "                if torch.cuda.is_available():\n",
    "                    try:\n",
    "                        vram_delta_kb = torch.cuda.max_memory_allocated(run_device) / 1024.0\n",
    "                    except Exception:\n",
    "                        vram_delta_kb = 0.0\n",
    "\n",
    "                dynamic_ram_kb = max(cpu_delta_kb, vram_delta_kb)\n",
    "\n",
    "                y_pred_enc = np.concatenate(all_preds) if all_preds else np.array([], dtype=int)\n",
    "                y_true_enc = np.concatenate(all_true) if all_true else np.array([], dtype=int)\n",
    "\n",
    "                try:\n",
    "                    y_pred = le.inverse_transform(y_pred_enc)\n",
    "                    y_true = le.inverse_transform(y_true_enc)\n",
    "                except Exception:\n",
    "                    y_pred = y_pred_enc\n",
    "                    y_true = y_true_enc\n",
    "\n",
    "                n_test = len(y_true)\n",
    "                inf_ms_per_sample = (infer_time / max(1, n_test)) * 1000.0\n",
    "\n",
    "                acc = accuracy_score(y_true, y_pred)\n",
    "                f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "                pm = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "                n_params = count_parameters(mdl)\n",
    "\n",
    "                import tempfile\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n",
    "                    tmp_path = Path(tmp.name)\n",
    "                torch.save(mdl.state_dict(), tmp_path)\n",
    "                size_kb = tmp_path.stat().st_size / 1024.0\n",
    "                tmp_path.unlink(missing_ok=True)\n",
    "\n",
    "                bytes_per_param = 4 \n",
    "                static_ram_kb = (n_params * bytes_per_param) / 1024.0\n",
    "\n",
    "                total_peak_ram_kb = static_ram_kb + dynamic_ram_kb\n",
    "\n",
    "                res = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"f1_macro\": f1m,\n",
    "                    \"precision_macro\": pm,\n",
    "                    \"trained_model_size_kb\": size_kb,\n",
    "                    \"inference_speed_ms\": inf_ms_per_sample,\n",
    "                    \"ram_usage_kb\": total_peak_ram_kb,         \n",
    "                    \"static_ram_kb\": static_ram_kb,              \n",
    "                    \"dynamic_ram_kb\": dynamic_ram_kb,            \n",
    "                    \"training_time_seconds\": training_time,\n",
    "                    \"inference_time_ms_per_sample\": inf_ms_per_sample,\n",
    "                    \"model_n_parameters\": n_params,\n",
    "                }\n",
    "\n",
    "                log(\n",
    "                    f\"[model] did model={model_name} (DL, device={run_device.type}) \"\n",
    "                    f\"train={training_time:.2f}s inf={inf_ms_per_sample:.3f}ms/row \"\n",
    "                    f\"size={size_kb:.1f}KB acc={acc:.4f} f1={f1m:.4f}\"\n",
    "                )\n",
    "\n",
    "                return res\n",
    "\n",
    "            try:\n",
    "                if device is not None and device.type == \"cuda\":\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    return _train_eval_deep_on_device(device)\n",
    "                else:\n",
    "                    return _train_eval_deep_on_device(torch.device(\"cpu\"))\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                err_msg = str(e).lower()\n",
    "                \n",
    "                if \"out of memory\" in err_msg:\n",
    "                    log(f\"[warn][model_oom] model={model_name} CUDA OOM on {device}, retrying on CPU\")\n",
    "                    try:\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                            if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "                                torch.cuda.ipc_collect()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return _train_eval_deep_on_device(torch.device(\"cpu\"))\n",
    "                else:\n",
    "                    log(f\"[error][model_dl] model={model_name} RuntimeError: {e}\")\n",
    "                    return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        log(f\"[error][model] model={model_name} error={type(e).__name__}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if torch is not None and torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "                    torch.cuda.ipc_collect()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ---------- Per-dataset scoring helper ----------\n",
    "\n",
    "def apply_scores_per_dataset(model_results: list[dict]) -> list[dict]:\n",
    "    if not model_results:\n",
    "        return model_results\n",
    "\n",
    "    sizes = np.array([r[\"trained_model_size_kb\"]      for r in model_results], dtype=float)\n",
    "    infs  = np.array([r[\"inference_speed_ms\"] for r in model_results], dtype=float)\n",
    "    rams  = np.array([r[\"ram_usage_kb\"]       for r in model_results], dtype=float)\n",
    "    accs  = np.array([r[\"accuracy\"]           for r in model_results], dtype=float)\n",
    "\n",
    "    size_min, size_max = sizes.min(), sizes.max()\n",
    "    inf_min,  inf_max  = infs.min(),  infs.max()\n",
    "    ram_min,  ram_max  = rams.min(),  rams.max()\n",
    "\n",
    "    for r in model_results:\n",
    "        acc = r[\"accuracy\"]\n",
    "\n",
    "        # Normalize & invert for cost metrics\n",
    "        def norm_inv(val, vmin, vmax):\n",
    "            if vmax <= vmin:\n",
    "                return 1.0\n",
    "            x = (val - vmin) / (vmax - vmin + 1e-9)\n",
    "            return float(1.0 - x)\n",
    "\n",
    "        size_reward = norm_inv(r[\"trained_model_size_kb\"],      size_min, size_max)\n",
    "        inf_reward  = norm_inv(r[\"inference_speed_ms\"], inf_min,  inf_max)\n",
    "        ram_reward  = norm_inv(r[\"ram_usage_kb\"],       ram_min,  ram_max)\n",
    "\n",
    "        score = (\n",
    "            SCORE_WEIGHTS[\"accuracy\"]           * acc +\n",
    "            SCORE_WEIGHTS[\"trained_model_size_kb\"]      * size_reward +\n",
    "            SCORE_WEIGHTS[\"inference_speed_ms\"] * inf_reward +\n",
    "            SCORE_WEIGHTS[\"ram_usage_kb\"]       * ram_reward\n",
    "        )\n",
    "        r[\"score\"] = float(score)\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "finished_datasets = load_finished_datasets()\n",
    "\n",
    "log(\"[init] Listing OpenML tasks…\")\n",
    "\n",
    "def list_tasks_with_retry(retries=3, backoff=3):\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            log(f\"[stage] list_tasks attempt {attempt}/{retries}\")\n",
    "            t0 = time.time()\n",
    "\n",
    "            tasks = openml.tasks.list_tasks(output_format=\"dataframe\")  # ✔️ no filters\n",
    "\n",
    "            log(f\"[stage] list_tasks ✓ in {time.time() - t0:.2f}s, got {len(tasks)} tasks\")\n",
    "            return tasks\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"[warn] list_tasks failed ({type(e).__name__}): {e}\")\n",
    "            if attempt == retries:\n",
    "                raise\n",
    "            time.sleep(backoff * attempt)\n",
    "\n",
    "task_list = list_tasks_with_retry()\n",
    "\n",
    "df = task_list\n",
    "\n",
    "if \"task_type\" in df.columns:\n",
    "    before = len(df)\n",
    "    df = df[df[\"task_type\"] == \"Supervised Classification\"]\n",
    "    log(f\"[filter] task_type='Supervised Classification' → kept {len(df)}/{before}\")\n",
    "else:\n",
    "    log(\"[filter] WARNING: task_type column missing\")\n",
    "\n",
    "if ONLY_ACTIVE and \"status\" in df.columns:\n",
    "    before = len(df)\n",
    "    df = df[df[\"status\"].str.lower() == \"active\"]\n",
    "    log(f\"[filter] status='active' → kept {len(df)}/{before}\")\n",
    "\n",
    "if \"did\" in df.columns:\n",
    "    before = len(df)\n",
    "    df = df[~df[\"did\"].isin(SKIP_DATASET_IDS)]\n",
    "    log(f\"[filter] dataset blacklist → kept {len(df)}/{before}\")\n",
    "\n",
    "if \"NumberOfInstances\" in df.columns:\n",
    "    before = len(df)\n",
    "    df = df[\n",
    "        (df[\"NumberOfInstances\"] >= MIN_TRAIN_ROWS) &\n",
    "        (df[\"NumberOfInstances\"] <= MAX_TRAIN_ROWS)\n",
    "    ]\n",
    "    log(f\"[filter] {MIN_TRAIN_ROWS} ≤ NumberOfInstances ≤ {MAX_TRAIN_ROWS} → kept {len(df)}/{before}\")\n",
    "else:\n",
    "    log(\"[filter] NumberOfInstances column missing → CANNOT filter by instance count\")\n",
    "\n",
    "if \"NumberOfFeatures\" in df.columns:\n",
    "    before = len(df)\n",
    "    df = df[\n",
    "        (df[\"NumberOfFeatures\"] >= MIN_FEATURES) &\n",
    "        (df[\"NumberOfFeatures\"] <= MAX_FEATURES)\n",
    "    ]\n",
    "    log(\n",
    "        f\"[filter] {MIN_FEATURES} ≤ NumberOfFeatures ≤ {MAX_FEATURES} \"\n",
    "        f\"→ kept {len(df)}/{before}\"\n",
    "    )\n",
    "else:\n",
    "    log(\"[filter] NumberOfFeatures column missing → CANNOT filter by feature count\")\n",
    "\n",
    "task_ids = [int(t) for t in df[\"tid\"].tolist()]\n",
    "\n",
    "if SHUFFLE_TASKS:\n",
    "    RNG.shuffle(task_ids)\n",
    "\n",
    "if MAX_TASKS is not None:\n",
    "    task_ids = task_ids[:MAX_TASKS]\n",
    "\n",
    "log(f\"[init] Selected {len(task_ids)} tasks for processing\")\n",
    "\n",
    "processed_count = 0\n",
    "\n",
    "for idx, task_id in enumerate(task_ids, start=1):\n",
    "    if task_id in SKIP_TASK_IDS:\n",
    "        continue\n",
    "\n",
    "    dataset_id = None\n",
    "    name = \"UNKNOWN\"\n",
    "    try:\n",
    "        tmp_task = openml.tasks.get_task(task_id)\n",
    "        dataset_id = int(tmp_task.dataset_id)\n",
    "        name = None        \n",
    "    except Exception:\n",
    "        dataset_id = None\n",
    "        name = None\n",
    "\n",
    "    if dataset_id is not None and dataset_id in finished_datasets:\n",
    "        continue\n",
    "\n",
    "    if dataset_id is not None and dataset_id in SKIP_DATASET_IDS:\n",
    "        log(f\"[skip][dataset_blacklist] did={dataset_id} (in SKIP_DATASET_IDS)\")\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "\n",
    "    log(f\"[dataset] #{idx}/{len(task_ids)} → tid={task_id} did={dataset_id} name='{name}'\")\n",
    "\n",
    "    try:\n",
    "        task, dataset, X, y, cat_ind, attr_names, dataset_name = fetch_task_and_data(task_id)\n",
    "        dataset_id = int(dataset.dataset_id)\n",
    "        name = dataset_name\n",
    "        log(f\"[fetch] did={dataset_id} name='{name}' n_samples={X.shape[0]} n_features={X.shape[1]}\")\n",
    "    except TaskNotSupported as e:\n",
    "        log(f\"[skip][task_type] tid={task_id} reason={e}\")\n",
    "        if dataset_id is not None:\n",
    "            mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        log(f\"[skip][fetch_error] tid={task_id} did={dataset_id} error={type(e).__name__}: {e}\")\n",
    "        if dataset_id is not None:\n",
    "            mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "\n",
    "    if dataset_id in SKIP_DATASET_IDS:\n",
    "        log(f\"[skip][dataset_blacklist] did={dataset_id} (in SKIP_DATASET_IDS)\")\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "\n",
    "    if MAX_TRAIN_ROWS is not None and X.shape[0] > MAX_TRAIN_ROWS:\n",
    "        log(f\"[dataset] did={dataset_id} subsampling from n={X.shape[0]} to {MAX_TRAIN_ROWS}\")\n",
    "        idx_rows = RNG.choice(X.shape[0], size=MAX_TRAIN_ROWS, replace=False)\n",
    "        X = X.iloc[idx_rows].reset_index(drop=True)\n",
    "        y = np.asarray(y)[idx_rows]\n",
    "\n",
    "    try:\n",
    "        X, y = sk_shuffle(X, y, random_state=RANDOM_SEED)\n",
    "        y_array = np.asarray(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_array, test_size=0.25, random_state=RANDOM_SEED,\n",
    "            stratify=y_array if np.unique(y_array).shape[0] > 1 else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(f\"[skip][split] did={dataset_id} error={type(e).__name__}: {e}\")\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        preprocessor, numeric_mask = build_preprocessor(X_train)\n",
    "    except Exception as e:\n",
    "        log(f\"[skip][preprocess] did={dataset_id} error={type(e).__name__}: {e}\")\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        meta = compute_meta(X_train, y_train, numeric_mask)\n",
    "        if meta is None:\n",
    "            log(f\"[skip][meta] did={dataset_id} reason=landmark_timeout_or_too_few_rows\")\n",
    "            mark_dataset_finished(dataset_id)\n",
    "            continue\n",
    "        log(f\"[meta] did={dataset_id} meta-features+landmarks computed\")\n",
    "    except Exception as e:\n",
    "        log(f\"[skip][meta_error] did={dataset_id} error={type(e).__name__}: {e}\")\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        continue\n",
    "\n",
    "    t_start = time.time()\n",
    "    model_results = []\n",
    "    timed_out = False\n",
    "    model_failed = False  \n",
    "\n",
    "    for i, model_name in enumerate(TRAINING_ORDER, start=1):\n",
    "        if torch is not None and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if time.time() - t_start > TRAIN_TIMEOUT_S:\n",
    "            timed_out = True\n",
    "            break\n",
    "\n",
    "        log(f\"[model] did={dataset_id} [{i}/{len(ENABLED_MODELS)}] starting model={model_name}\")\n",
    "        result = train_and_eval_model(\n",
    "            model_name=model_name,\n",
    "            preprocessor=preprocessor,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "        )\n",
    "\n",
    "        if result is None:\n",
    "            model_failed = True\n",
    "            log(\n",
    "                f\"[warn][model] did={dataset_id} model={model_name} failed or unavailable \"\n",
    "                f\"→ aborting remaining models for this dataset\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        model_results.append(result)\n",
    "\n",
    "    if timed_out:\n",
    "        log(f\"[timeout] did={dataset_id} training exceeded {TRAIN_TIMEOUT_S:.1f}s → skipping dataset (no rows)\")\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        prune_cache_if_needed()\n",
    "        free_memory()\n",
    "        continue\n",
    "\n",
    "    if model_failed or len(model_results) != len(ENABLED_MODELS):\n",
    "        log(\n",
    "            f\"[skip][models_incomplete] did={dataset_id} \"\n",
    "            f\"success_models={len(model_results)}/{len(ENABLED_MODELS)}\"\n",
    "        )\n",
    "        mark_dataset_finished(dataset_id)\n",
    "        prune_cache_if_needed()\n",
    "        free_memory()\n",
    "        continue\n",
    "\n",
    "    model_results = apply_scores_per_dataset(model_results)\n",
    "\n",
    "    model_order_map = {name: i for i, name in enumerate(ENABLED_MODELS)}\n",
    "\n",
    "    def sort_key(result):\n",
    "        return model_order_map.get(result[\"model_name\"], float('inf'))\n",
    "\n",
    "    model_results.sort(key=sort_key)\n",
    "\n",
    "    for res in model_results:\n",
    "        try:\n",
    "            log(\n",
    "                f\"[score] did={dataset_id} model={res['model_name']} \"\n",
    "                f\"acc={res['accuracy']:.4f} f1={res['f1_macro']:.4f} score={res['score']:.4f}\"\n",
    "            )\n",
    "        except Exception:\n",
    "            log(\n",
    "                f\"[score] did={dataset_id} model={res.get('model_name','?')} \"\n",
    "                f\"acc={res.get('accuracy')} f1={res.get('f1_macro')} score={res.get('score')}\"\n",
    "            )\n",
    "\n",
    "    rows_to_append = []\n",
    "    for res in model_results:\n",
    "        mname = res[\"model_name\"]\n",
    "        caps = MODEL_CAPABILITIES.get(mname, {})\n",
    "\n",
    "        row = {\n",
    "            \"Task_id\":          int(task_id),\n",
    "            \"dataset_id\":       int(dataset_id),\n",
    "            \"dataset_name\":     name,\n",
    "\n",
    "            \"n_samples\":                    meta.get(\"n_samples\"),\n",
    "            \"n_features\":                   meta.get(\"n_features\"),\n",
    "            \"n_numeric_features\":           meta.get(\"n_numeric_features\"),\n",
    "            \"n_categorical_features\":       meta.get(\"n_categorical_features\"),\n",
    "            \"n_binary_features\":            meta.get(\"n_binary_features\"),\n",
    "            \"n_classes\":                    meta.get(\"n_classes\"),\n",
    "            \"class_balance_std\":            meta.get(\"class_balance_std\"),\n",
    "            \"class_entropy\":                meta.get(\"class_entropy\"),\n",
    "            \"mean_feature_variance\":        meta.get(\"mean_feature_variance\"),\n",
    "            \"median_feature_variance\":      meta.get(\"median_feature_variance\"),\n",
    "            \"mean_corr_abs\":                meta.get(\"mean_corr_abs\"),\n",
    "            \"max_corr_abs\":                 meta.get(\"max_corr_abs\"),\n",
    "            \"feature_skewness_mean\":        meta.get(\"feature_skewness_mean\"),\n",
    "            \"feature_kurtosis_mean\":        meta.get(\"feature_kurtosis_mean\"),\n",
    "            \"missing_percentage\":           meta.get(\"missing_percentage\"),\n",
    "            \"avg_cardinality_categorical\":  meta.get(\"avg_cardinality_categorical\"),\n",
    "            \"complexity_ratio\":             meta.get(\"complexity_ratio\"),\n",
    "            \"intrinsic_dim_estimate\":       meta.get(\"intrinsic_dim_estimate\"),\n",
    "\n",
    "            \"landmark_lr_accuracy\":          meta.get(\"landmark_lr_accuracy\"),\n",
    "            \"landmark_dt_depth3_accuracy\":   meta.get(\"landmark_dt_depth3_accuracy\"),\n",
    "            \"landmark_knn3_accuracy\":        meta.get(\"landmark_knn3_accuracy\"),\n",
    "            \"landmark_random_noise_accuracy\": meta.get(\"landmark_random_noise_accuracy\"),\n",
    "            \"fisher_discriminant_ratio\":     meta.get(\"fisher_discriminant_ratio\"),\n",
    "\n",
    "            \"model_id\":                    caps.get(\"model_id\"),\n",
    "            \"model_name\":                  caps.get(\"model_name\", mname),\n",
    "            \"model_family\":                caps.get(\"model_family\"),\n",
    "            \"is_deep_learning\":            caps.get(\"is_deep_learning\"),\n",
    "            \"is_tree_based\":               caps.get(\"is_tree_based\"),\n",
    "            \"is_linear\":                   caps.get(\"is_linear\"),\n",
    "\n",
    "            \"parameterization_type\":        caps.get(\"parameterization_type\"),\n",
    "            \"complexity_training_big_o\":    caps.get(\"complexity_training_big_o\"),\n",
    "            \"complexity_inference_big_o\":   caps.get(\"complexity_inference_big_o\"),\n",
    "            \"is_probabilistic\":             caps.get(\"is_probabilistic\"),\n",
    "            \"is_ensemble_model\":            caps.get(\"is_ensemble_model\"),\n",
    "            \"regularization_supported\":     caps.get(\"regularization_supported\"),\n",
    "            \"supports_multiclass_natively\": caps.get(\"supports_multiclass_natively\"),\n",
    "            \"supports_online_learning\":     caps.get(\"supports_online_learning\"),\n",
    "\n",
    "            \"supports_multiple_trees\":  caps.get(\"supports_multiple_trees\"),\n",
    "            \"tree_growth_strategy\":     caps.get(\"tree_growth_strategy\"),\n",
    "            \"default_max_depth\":        caps.get(\"default_max_depth\"),\n",
    "            \"supports_pruning\":         caps.get(\"supports_pruning\"),\n",
    "            \"splitting_criterion\":      caps.get(\"splitting_criterion\"),\n",
    "\n",
    "            \"architecture_type\":          caps.get(\"architecture_type\"),\n",
    "            \"supports_dropout\":           caps.get(\"supports_dropout\"),\n",
    "            \"supports_batchnorm\":         caps.get(\"supports_batchnorm\"),\n",
    "            \"default_activation\":         caps.get(\"default_activation\"),\n",
    "            \"supports_cuda_acceleration\": caps.get(\"supports_cuda_acceleration\"),\n",
    "\n",
    "            \"supports_non_linearity\":        caps.get(\"supports_non_linearity\"),\n",
    "            \"supports_categorical_directly\": caps.get(\"supports_categorical_directly\"),\n",
    "            \"supports_missing_values\":       caps.get(\"supports_missing_values\"),\n",
    "            \"supports_gpu\":                  caps.get(\"supports_gpu\"),\n",
    "\n",
    "            \"n_estimators\":     caps.get(\"n_estimators\", 0),\n",
    "            \"avg_tree_depth\":   caps.get(\"avg_tree_depth\", 0.0),\n",
    "            \"max_tree_depth\":   caps.get(\"max_tree_depth\", 0),\n",
    "            \"n_leaves_mean\":    caps.get(\"n_leaves_mean\", 0.0),\n",
    "\n",
    "            \"n_layers\":             caps.get(\"n_layers\", 0),\n",
    "            \"hidden_units_mean\":    caps.get(\"hidden_units_mean\", 0.0),\n",
    "            \"dropout_rate_mean\":    caps.get(\"dropout_rate_mean\", 0.0),\n",
    "            \"activation_type\":      caps.get(\"activation_type\", \"none\"),\n",
    "            \"batch_size\":           caps.get(\"batch_size\", 0),\n",
    "            \"epochs\":               caps.get(\"epochs\", 0),\n",
    "\n",
    "            \"accuracy\":                res[\"accuracy\"],\n",
    "            \"f1_macro\":                res[\"f1_macro\"],\n",
    "            \"precision_macro\":         res[\"precision_macro\"],\n",
    "            \"trained_model_size_kb\":   res[\"trained_model_size_kb\"],       \n",
    "            \"inference_speed_ms\":      res[\"inference_speed_ms\"],\n",
    "            \"static_usage_ram_kb\":     res[\"static_ram_kb\"],             \n",
    "            \"dynamic_usage_ram_kb\":    res[\"dynamic_ram_kb\"],            \n",
    "            \"full_ram_usage_kb\":       res[\"ram_usage_kb\"],\n",
    "            \"model_n_parameters\":      res[\"model_n_parameters\"],\n",
    "            \"score\":                   res[\"score\"],\n",
    "        }\n",
    "\n",
    "        rows_to_append.append(row)\n",
    "\n",
    "    _append_csv(OUTPUT_CSV, rows_to_append)\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    best = max(model_results, key=lambda r: r[\"score\"])\n",
    "    log(\n",
    "        f\"[dataset] did={dataset_id} completed {len(rows_to_append)} models in {elapsed:.1f}s \"\n",
    "        f\"| best_model={best['model_name']} acc={best['accuracy']:.4f} score={best['score']:.4f}\"\n",
    "    )\n",
    "\n",
    "    mark_dataset_finished(dataset_id)\n",
    "    processed_count += 1\n",
    "\n",
    "    if MAX_DATASETS is not None and processed_count >= MAX_DATASETS:\n",
    "        log(f\"[done] Reached MAX_DATASETS={MAX_DATASETS}, stopping early\")\n",
    "        break\n",
    "\n",
    "    prune_cache_if_needed()\n",
    "    free_memory()\n",
    "\n",
    "log(f\"[done] Processed {processed_count} datasets (full pipeline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(OUTPUT_CSV).exists():\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    print(f\"CSV shape: {df.shape}\")\n",
    "    display(df.tail(20))\n",
    "    print(\"Unique dataset_ids:\", df[\"dataset_id\"].nunique())\n",
    "else:\n",
    "    print(f\"No CSV found at {OUTPUT_CSV} yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
