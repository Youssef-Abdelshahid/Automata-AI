{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Dataset config\n",
    "DATA_ROOT = \"./data\"\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# These will be inferred automatically\n",
    "TARGET_SR: Optional[int] = None\n",
    "CLIP_SECONDS: Optional[float] = None\n",
    "TARGET_SAMPLES: Optional[int] = None\n",
    "\n",
    "# Will be filled in get_dataloaders and used by the models\n",
    "NUM_CLASSES: Optional[int] = None\n",
    "\n",
    "\n",
    "def _seed_worker(worker_id: int):\n",
    "    worker_seed = (SEED + worker_id) % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def pad_or_trim(waveform: torch.Tensor, target_len: int) -> torch.Tensor:\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    T = waveform.shape[-1]\n",
    "    if T == target_len:\n",
    "        return waveform\n",
    "    if T > target_len:\n",
    "        return waveform[:, :target_len]\n",
    "    pad = target_len - T\n",
    "    return F.pad(waveform, (0, pad))\n",
    "\n",
    "def print_preprocessing_report(config, profile, img_size: int):\n",
    "    durations = profile[\"durations\"]\n",
    "    rms_vals = profile[\"rms_values\"]\n",
    "    sr_counts = profile[\"sr_counts\"]\n",
    "    files_per_class = profile[\"files_per_class\"]\n",
    "\n",
    "    # --- basic dataset stats ---\n",
    "    num_files = profile[\"num_files\"]\n",
    "    num_classes = len(files_per_class) if files_per_class else 0\n",
    "    min_per_class = min(files_per_class.values()) if files_per_class else 0\n",
    "    median_per_class = (\n",
    "        int(np.median(list(files_per_class.values())))\n",
    "        if files_per_class else 0\n",
    "    )\n",
    "\n",
    "    # --- duration stats ---\n",
    "    if durations.size > 0:\n",
    "        dur_mean = float(durations.mean())\n",
    "        dur_p5 = float(np.percentile(durations, 5))\n",
    "        dur_p50 = float(np.percentile(durations, 50))\n",
    "        dur_p95 = float(np.percentile(durations, 95))\n",
    "    else:\n",
    "        dur_mean = dur_p5 = dur_p50 = dur_p95 = 0.0\n",
    "\n",
    "    # --- RMS stats ---\n",
    "    if rms_vals.size > 0:\n",
    "        rms_mean = float(rms_vals.mean())\n",
    "        rms_std = float(rms_vals.std())\n",
    "    else:\n",
    "        rms_mean = rms_std = 0.0\n",
    "\n",
    "    # --- SR distribution string ---\n",
    "    if sr_counts:\n",
    "        sr_items = sorted(sr_counts.items(), key=lambda kv: -kv[1])\n",
    "        sr_str = \", \".join(f\"{sr}Hz({cnt})\" for sr, cnt in sr_items)\n",
    "    else:\n",
    "        sr_str = \"N/A\"\n",
    "\n",
    "    # --- spectrogram shape estimates ---\n",
    "    target_samples = int(config.clip_seconds * config.target_sr)\n",
    "    est_frames = max(\n",
    "        1,\n",
    "        1 + (target_samples - config.win_length) // config.hop_length\n",
    "    )\n",
    "    spec_before = f\"(1, {config.n_mels}, {est_frames})\"\n",
    "    spec_after = f\"(1, {img_size}, {img_size})\"\n",
    "\n",
    "    # --- one-line summary ---\n",
    "    summary = (\n",
    "        f\"[Audio Preproc] SR={config.target_sr} | \"\n",
    "        f\"Clip={config.clip_seconds:.2f}s | \"\n",
    "        f\"Mels={config.n_mels} | \"\n",
    "        f\"GlobalNorm={'ON' if config.use_global_norm else 'OFF'} | \"\n",
    "        f\"SpecAug={'ON' if config.use_spec_augment else 'OFF'} \"\n",
    "        f\"(Tmask={config.time_mask_param}×{config.num_time_masks}, \"\n",
    "        f\"Fmask={config.freq_mask_param}×{config.num_freq_masks}) | \"\n",
    "        f\"Files={num_files}, Classes={num_classes}, Min/class={min_per_class}\"\n",
    "    )\n",
    "\n",
    "    print(summary)\n",
    "    print(\n",
    "        f\"  Durations (s): mean={dur_mean:.3f}, p5={dur_p5:.3f}, \"\n",
    "        f\"p50={dur_p50:.3f}, p95={dur_p95:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  RMS: mean={rms_mean:.5f}, std={rms_std:.5f} | \"\n",
    "        f\"Files/class median={median_per_class}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Sample rates: {sr_str}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Spectrogram shapes: raw={spec_before} -> resized={spec_after}\"\n",
    "    )\n",
    "\n",
    "#   Automated Audio Preprocessing \n",
    "@dataclass\n",
    "class AudioPreprocessConfig:\n",
    "    target_sr: int = 16000\n",
    "    clip_seconds: float = 1.0\n",
    "    n_mels: int = 64\n",
    "    n_fft: int = 512\n",
    "    hop_length: int = 160\n",
    "    win_length: int = 400\n",
    "    use_global_norm: bool = True\n",
    "    use_spec_augment: bool = True\n",
    "    time_mask_param: int = 8\n",
    "    freq_mask_param: int = 8\n",
    "    num_time_masks: int = 1\n",
    "    num_freq_masks: int = 1\n",
    "\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, config: Optional[AudioPreprocessConfig] = None):\n",
    "        self.config = config or AudioPreprocessConfig()\n",
    "        self._mel = None\n",
    "        self._to_db = AT.AmplitudeToDB(stype=\"power\")\n",
    "        self.global_mean: Optional[float] = None\n",
    "        self.global_std: Optional[float] = None\n",
    "        self._time_mask = None\n",
    "        self._freq_mask = None\n",
    "\n",
    "    def _ensure_transforms(self):\n",
    "        if self._mel is None:\n",
    "            cfg = self.config\n",
    "            self._mel = AT.MelSpectrogram(\n",
    "                sample_rate=cfg.target_sr,\n",
    "                n_fft=cfg.n_fft,\n",
    "                hop_length=cfg.hop_length,\n",
    "                win_length=cfg.win_length,\n",
    "                n_mels=cfg.n_mels,\n",
    "            )\n",
    "        if self.config.use_spec_augment and self._time_mask is None:\n",
    "            cfg = self.config\n",
    "            self._time_mask = AT.TimeMasking(time_mask_param=cfg.time_mask_param)\n",
    "            self._freq_mask = AT.FrequencyMasking(freq_mask_param=cfg.freq_mask_param)\n",
    "\n",
    "    def _waveform_to_logmel(self, waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "        cfg = self.config\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        if sr != cfg.target_sr:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, cfg.target_sr)\n",
    "        target_len = int(cfg.clip_seconds * cfg.target_sr)\n",
    "        waveform = pad_or_trim(waveform, target_len)\n",
    "\n",
    "        self._ensure_transforms()\n",
    "        spec = self._mel(waveform)     \n",
    "        spec = self._to_db(spec)       \n",
    "        return spec\n",
    "\n",
    "    def fit(self, dataset, max_items: int = 512) -> \"AudioPreprocessor\":\n",
    "        sums = 0.0\n",
    "        sumsq = 0.0\n",
    "        count = 0\n",
    "\n",
    "        n = len(dataset)\n",
    "        indices = list(range(n))\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:max_items]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in indices:\n",
    "                sample = dataset[i]\n",
    "                waveform, sr = sample[0], sample[1]\n",
    "                spec = self._waveform_to_logmel(waveform, sr) \n",
    "                v = spec.reshape(-1)\n",
    "                sums += float(v.sum())\n",
    "                sumsq += float((v ** 2).sum())\n",
    "                count += v.numel()\n",
    "\n",
    "        if count > 0:\n",
    "            self.global_mean = sums / count\n",
    "            self.global_std = max(1e-6, (sumsq / count - self.global_mean ** 2) ** 0.5)\n",
    "        else:\n",
    "            self.global_mean, self.global_std = 0.0, 1.0\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, waveform: torch.Tensor, sr: int, img_size: int, augment: bool = False) -> torch.Tensor:\n",
    "        cfg = self.config\n",
    "        spec = self._waveform_to_logmel(waveform, sr)  \n",
    "\n",
    "        # Normalization\n",
    "        if cfg.use_global_norm and self.global_mean is not None:\n",
    "            spec = (spec - self.global_mean) / (self.global_std + 1e-6)\n",
    "        else:\n",
    "            # Per-sample normalization fallback\n",
    "            mean = spec.mean()\n",
    "            std = spec.std()\n",
    "            spec = (spec - mean) / (std + 1e-6)\n",
    "\n",
    "        # SpecAugment\n",
    "        if augment and cfg.use_spec_augment:\n",
    "            self._ensure_transforms()\n",
    "            x = spec\n",
    "            x = self._time_mask(x)\n",
    "            x = self._freq_mask(x)\n",
    "            spec = x\n",
    "\n",
    "        # Resize to square image for the NAS/CNN backbone\n",
    "        spec = spec.unsqueeze(0)  \n",
    "        spec = F.interpolate(spec, size=(img_size, img_size),\n",
    "                             mode=\"bilinear\", align_corners=False)\n",
    "        spec = spec.squeeze(0)    \n",
    "        return spec\n",
    "\n",
    "\n",
    "class AutoAudioPreprocessor:\n",
    "    def __init__(self, base_config: Optional[AudioPreprocessConfig] = None):\n",
    "        self.base_config = base_config or AudioPreprocessConfig()\n",
    "        self.config: Optional[AudioPreprocessConfig] = None\n",
    "        self.preproc: Optional[AudioPreprocessor] = None\n",
    "        self.profile: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    def _profile_dataset(self, dataset, max_items: int = 256) -> Dict[str, Any]:\n",
    "        sr_counts: Dict[int, int] = {}\n",
    "        durations = []\n",
    "        rms_values = []\n",
    "        files_per_class: Dict[str, int] = {}\n",
    "\n",
    "        n = len(dataset)\n",
    "        idxs = list(range(n))\n",
    "        random.shuffle(idxs)\n",
    "        idxs = idxs[:max_items]\n",
    "\n",
    "        for i in idxs:\n",
    "            waveform, sr, label, *_ = dataset[i]\n",
    "            if waveform.dim() == 1:\n",
    "                waveform = waveform.unsqueeze(0)\n",
    "            sr_counts[sr] = sr_counts.get(sr, 0) + 1\n",
    "            dur = waveform.shape[-1] / sr\n",
    "            durations.append(float(dur))\n",
    "            rms = float(torch.sqrt((waveform ** 2).mean()))\n",
    "            rms_values.append(rms)\n",
    "            files_per_class[label] = files_per_class.get(label, 0) + 1\n",
    "\n",
    "        return {\n",
    "            \"sr_counts\": sr_counts,\n",
    "            \"durations\": np.array(durations, dtype=np.float32),\n",
    "            \"rms_values\": np.array(rms_values, dtype=np.float32),\n",
    "            \"files_per_class\": files_per_class,\n",
    "            \"num_files\": n,\n",
    "        }\n",
    "\n",
    "    def _choose_config(self, img_size: int) -> AudioPreprocessConfig:\n",
    "        p = self.profile\n",
    "        cfg = self.base_config\n",
    "\n",
    "        # Target sample rate: most common in the dataset, fallback to 16 kHz\n",
    "        if p[\"sr_counts\"]:\n",
    "            cfg.target_sr = max(p[\"sr_counts\"].keys(), key=lambda s: p[\"sr_counts\"][s])\n",
    "        else:\n",
    "            cfg.target_sr = 16000\n",
    "\n",
    "        # Clip duration: 95th percentile of durations, clamped\n",
    "        if p[\"durations\"].size > 0:\n",
    "            p95 = float(np.percentile(p[\"durations\"], 95))\n",
    "            cfg.clip_seconds = float(max(0.5, min(5.0, p95)))\n",
    "        else:\n",
    "            cfg.clip_seconds = 1.0\n",
    "\n",
    "        # Global normalization\n",
    "        cfg.use_global_norm = p[\"num_files\"] > 500\n",
    "\n",
    "        # Augmentation strength based on per-class counts\n",
    "        if p[\"files_per_class\"]:\n",
    "            min_per_class = min(p[\"files_per_class\"].values())\n",
    "        else:\n",
    "            min_per_class = 0\n",
    "\n",
    "        # Base mask size roughly proportional to image size\n",
    "        base_mask = max(2, img_size // 10)\n",
    "        if min_per_class < 50:\n",
    "            cfg.use_spec_augment = True\n",
    "            cfg.time_mask_param = base_mask * 2\n",
    "            cfg.freq_mask_param = base_mask * 2\n",
    "            cfg.num_time_masks = 2\n",
    "            cfg.num_freq_masks = 2\n",
    "        else:\n",
    "            cfg.use_spec_augment = True\n",
    "            cfg.time_mask_param = base_mask\n",
    "            cfg.freq_mask_param = base_mask\n",
    "            cfg.num_time_masks = 1\n",
    "            cfg.num_freq_masks = 1\n",
    "\n",
    "        self.config = cfg\n",
    "        return cfg\n",
    "\n",
    "    def fit(self, dataset, img_size: int) -> \"AutoAudioPreprocessor\":\n",
    "        # 1) Profile dataset\n",
    "        self.profile = self._profile_dataset(dataset)\n",
    "        # 2) Choose config based on profile + image size\n",
    "        cfg = self._choose_config(img_size)\n",
    "        # 3) Build preprocessor and fit its global normalization\n",
    "        self.preproc = AudioPreprocessor(cfg)\n",
    "        if cfg.use_global_norm:\n",
    "            self.preproc.fit(dataset)\n",
    "        return self\n",
    "\n",
    "    def make_transforms(self, img_size: int):\n",
    "        assert self.preproc is not None, \"Call fit(...) before make_transforms().\"\n",
    "\n",
    "        def train_tf(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "            return self.preproc.transform(waveform, sr, img_size=img_size, augment=True)\n",
    "\n",
    "        def test_tf(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "            return self.preproc.transform(waveform, sr, img_size=img_size, augment=False)\n",
    "\n",
    "        return train_tf, test_tf\n",
    "\n",
    "\n",
    "\n",
    "#   Dataset wrapper + dataloaders \n",
    "\n",
    "\n",
    "class SpeechCommandsWrapped(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_ds, label2idx, tf):\n",
    "        self.ds = base_ds\n",
    "        self.label2idx = label2idx\n",
    "        self.tf = tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr, label, *_ = self.ds[idx]\n",
    "        x = self.tf(waveform, sr)\n",
    "        y = self.label2idx[label]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def _build_label_mapping(train_base):\n",
    "    labels = set()\n",
    "    for i in range(len(train_base)):\n",
    "        _, _, label, *_ = train_base[i]\n",
    "        labels.add(label)\n",
    "    labels = sorted(list(labels))\n",
    "    label2idx = {lab: i for i, lab in enumerate(labels)}\n",
    "    idx2label = {i: lab for lab, i in label2idx.items()}\n",
    "    return label2idx, idx2label\n",
    "\n",
    "\n",
    "def get_dataloaders(img_size: int):\n",
    "    global NUM_CLASSES, TARGET_SR, CLIP_SECONDS, TARGET_SAMPLES\n",
    "\n",
    "    # Built-in SpeechCommands splits \n",
    "    train_base = torchaudio.datasets.SPEECHCOMMANDS(\n",
    "        root=DATA_ROOT, download=True, subset=\"training\"\n",
    "    )\n",
    "    val_base = torchaudio.datasets.SPEECHCOMMANDS(\n",
    "        root=DATA_ROOT, download=True, subset=\"validation\"\n",
    "    )\n",
    "    test_base = torchaudio.datasets.SPEECHCOMMANDS(\n",
    "        root=DATA_ROOT, download=True, subset=\"testing\"\n",
    "    )\n",
    "\n",
    "    # Label mapping based on training subset\n",
    "    label2idx, idx2label = _build_label_mapping(train_base)\n",
    "    NUM_CLASSES = len(label2idx)\n",
    "\n",
    "    # Automated audio preprocessing\n",
    "    auto_preproc = AutoAudioPreprocessor()\n",
    "    auto_preproc.fit(train_base, img_size=img_size)\n",
    "    # print report here\n",
    "    print_preprocessing_report(auto_preproc.config, auto_preproc.profile, img_size)\n",
    "\n",
    "    train_tf, test_tf = auto_preproc.make_transforms(img_size=img_size)\n",
    "\n",
    "    # Expose inferred low-level parameters if you still need them anywhere\n",
    "    TARGET_SR = auto_preproc.config.target_sr\n",
    "    CLIP_SECONDS = auto_preproc.config.clip_seconds\n",
    "    TARGET_SAMPLES = int(TARGET_SR * CLIP_SECONDS)\n",
    "\n",
    "    # Wrap datasets\n",
    "    train_set = SpeechCommandsWrapped(train_base, label2idx, train_tf)\n",
    "    val_set   = SpeechCommandsWrapped(val_base,   label2idx, test_tf)\n",
    "    test_set  = SpeechCommandsWrapped(test_base,  label2idx, test_tf)\n",
    "\n",
    "    # Dataloaders\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        worker_init_fn=_seed_worker, generator=g\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        worker_init_fn=_seed_worker, generator=g\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "        worker_init_fn=_seed_worker, generator=g\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
