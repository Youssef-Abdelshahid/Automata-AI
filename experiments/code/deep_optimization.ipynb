{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f100e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_fp16(model: nn.Module) -> nn.Module:\n",
    "    m = copy.deepcopy(model)\n",
    "    m.eval()\n",
    "    return m.half()\n",
    "\n",
    "def tabular_int8_dynamic(model: nn.Module) -> nn.Module:\n",
    "    m = copy.deepcopy(model)\n",
    "    m = m.eval().cpu()\n",
    "    qmodel = torch.ao.quantization.quantize_dynamic(\n",
    "        m,\n",
    "        qconfig_spec={nn.Linear, nn.GRU},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    return qmodel\n",
    "\n",
    "def tabular_int8_static(model: nn.Module, calibration_loader, num_calib_batches: int = 20, backend: str = \"fbgemm\") -> nn.Module:\n",
    "    m = copy.deepcopy(model)\n",
    "    m = m.eval().cpu()\n",
    "    if hasattr(m, \"conv\") and hasattr(m, \"relu\"):\n",
    "        try: torch.ao.quantization.fuse_modules(m, [[\"conv\", \"relu\"]], inplace=True)\n",
    "        except: pass\n",
    "    if hasattr(m, \"net\") and isinstance(m.net, nn.Sequential):\n",
    "        try: torch.ao.quantization.fuse_modules(m.net, [[\"0\", \"1\"], [\"2\", \"3\"]], inplace=True)\n",
    "        except: pass\n",
    "    m = torch.ao.quantization.QuantWrapper(m)\n",
    "    torch.backends.quantized.engine = backend\n",
    "    m.qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
    "    prepared = torch.ao.quantization.prepare(m, inplace=False)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(calibration_loader):\n",
    "            if i >= num_calib_batches: break\n",
    "            x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            prepared(x.cpu().float())\n",
    "    quantized = torch.ao.quantization.convert(prepared, inplace=False)\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_fp16(model: nn.Module) -> nn.Module:\n",
    "    model = model.eval()\n",
    "    return model.half()\n",
    "\n",
    "def audio_int8_dynamic(model):\n",
    "    m = copy.deepcopy(model).cpu()\n",
    "    q_model = torch.quantization.quantize_dynamic(m, {nn.Linear, nn.LSTM, nn.GRU, nn.RNN}, dtype=torch.qint8)\n",
    "    return q_model\n",
    "\n",
    "def audio_int8_static(model, calibration_loader, backend=\"fbgemm\"):\n",
    "    if backend == \"qnnpack\":\n",
    "        torch.backends.quantized.engine = \"qnnpack\"\n",
    "    else:\n",
    "        torch.backends.quantized.engine = \"fbgemm\"\n",
    "    \n",
    "    m = copy.deepcopy(model).cpu()\n",
    "    m.eval()\n",
    "    \n",
    "    num_calib_batches = 8\n",
    "    def get_x(batch):\n",
    "        if isinstance(batch, (list, tuple)): return batch[0]\n",
    "        if isinstance(batch, dict): return next(iter(batch.values()))\n",
    "        return batch\n",
    "\n",
    "    x0 = get_x(next(iter(calibration_loader))).cpu().float()\n",
    "    example_inputs = (x0,)\n",
    "\n",
    "    qconfig_mapping = get_default_qconfig_mapping(backend)\n",
    "    prepared = prepare_fx(m, qconfig_mapping, example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(calibration_loader):\n",
    "            if i >= num_calib_batches:\n",
    "                break\n",
    "            prepared(get_x(batch).cpu().float())\n",
    "\n",
    "    return convert_fx(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c422238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_fp16(model: nn.Module) -> nn.Module:\n",
    "    model = model.eval()\n",
    "    return model.half()\n",
    "\n",
    "def image_int8_dynamic(model):\n",
    "    m = copy.deepcopy(model).cpu()\n",
    "    q_model = torch.quantization.quantize_dynamic(m, {nn.Linear, nn.LSTM, nn.GRU, nn.RNN}, dtype=torch.qint8)\n",
    "    return q_model\n",
    "\n",
    "def image_int8_static(model, calibration_loader, backend=\"fbgemm\"):\n",
    "    if backend == \"qnnpack\":\n",
    "        torch.backends.quantized.engine = \"qnnpack\"\n",
    "    else:\n",
    "        torch.backends.quantized.engine = \"fbgemm\"\n",
    "    \n",
    "    m = copy.deepcopy(model).cpu()\n",
    "    m.eval()\n",
    "    \n",
    "    num_calib_batches = 8\n",
    "    def get_x(batch):\n",
    "        if isinstance(batch, (list, tuple)): return batch[0]\n",
    "        if isinstance(batch, dict): return next(iter(batch.values()))\n",
    "        return batch\n",
    "\n",
    "    x0 = get_x(next(iter(calibration_loader))).cpu().float()\n",
    "    example_inputs = (x0,)\n",
    "\n",
    "    qconfig_mapping = get_default_qconfig_mapping(backend)\n",
    "    prepared = prepare_fx(m, qconfig_mapping, example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(calibration_loader):\n",
    "            if i >= num_calib_batches:\n",
    "                break\n",
    "            prepared(get_x(batch).cpu().float())\n",
    "\n",
    "    return convert_fx(prepared)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
