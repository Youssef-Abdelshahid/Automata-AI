{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1V3-_kMwbbpuS1GeiSvN1r_AE-ZidW8Tn","authorship_tag":"ABX9TyMPVTos96msvnBliUk9a/4L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import Optional\n","\n","@dataclass\n","class PreprocessConfig:\n","    report: bool = True\n","    report_path: str = \"prep_dynamic.pdf\"\n","    project_name: str = \"Automata AI - Preprocessing Report\"\n","    logo_path: Optional[str] = None\n","    verbose: bool = True\n"],"metadata":{"id":"wGNSpVcuu-i_","executionInfo":{"status":"ok","timestamp":1765998853397,"user_tz":-120,"elapsed":44,"user":{"displayName":"Ammar Alaa Mostafa Bektash","userId":"17155881455724446425"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","execution_count":36,"metadata":{"id":"FtkFk9__BR2p","executionInfo":{"status":"ok","timestamp":1765994988963,"user_tz":-120,"elapsed":116,"user":{"displayName":"Ammar Alaa Mostafa Bektash","userId":"17155881455724446425"}}},"outputs":[],"source":["# =========================\n","# CELL 1 — AutomataPreprocessor (main code) [IMPROVED + FIXED]\n","# =========================\n","\n","from dataclasses import dataclass, asdict\n","from typing import List, Dict, Optional, Tuple, Any, Callable\n","import re\n","import joblib\n","import logging\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import importlib\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_selection import SelectKBest, mutual_info_classif\n","\n","# ---- logging ----\n","logger = logging.getLogger(\"AutomataPreprocessor\")\n","if not logger.handlers:\n","    ch = logging.StreamHandler()\n","    ch.setFormatter(logging.Formatter(\"[%(levelname)s] %(name)s: %(message)s\"))\n","    logger.addHandler(ch)\n","logger.setLevel(logging.INFO)\n","\n","\n","@dataclass\n","class PreprocessingConfig:\n","    drop_missing_threshold: float = 0.8\n","    high_cardinality_threshold: float = 0.2   # kept for backward-compat (no longer used in split)\n","    high_cardinality_min_unique: int = 15\n","\n","    numeric_imputer: str = \"median\"          # 'mean','median','most_frequent'\n","    numeric_scaler: str = \"standard\"         # 'standard','minmax','robust','none'\n","\n","    categorical_imputer: str = \"most_frequent\"  # 'most_frequent','constant'\n","\n","    # OHE controls (applied only if your sklearn supports them)\n","    ohe_min_frequency: Optional[float] = None   # e.g. 0.01 (or an int count in newer sklearn)\n","    ohe_max_categories: Optional[int] = None\n","\n","    high_cardinality_encoder: str = \"frequency\" # currently: 'frequency'\n","\n","    # datetime handling: drop or extract into numeric parts\n","    datetime_handling: str = \"drop\"  # 'drop' | 'extract'\n","    datetime_extract_parts: Tuple[str, ...] = (\"year\", \"month\", \"day\", \"dayofweek\")\n","\n","    feature_selection: str = \"auto\"          # 'auto','none','mutual_info'\n","    feature_fraction: float = 0.75\n","\n","    balancing: str = \"class_weight\"          # 'none','class_weight'\n","    imbalance_threshold: float = 1.5         # apply class_weight only if imbalance_ratio > this\n","\n","\n","class DatasetAnalyzer:\n","    DATE_REGEXES = [\n","        re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\"),        # YYYY-MM-DD\n","        re.compile(r\"^\\d{2}/\\d{2}/\\d{4}\"),        # MM/DD/YYYY\n","        re.compile(r\"^\\d{4}/\\d{2}/\\d{2}\"),        # YYYY/MM/DD\n","        re.compile(r\"^\\d{2}-[A-Za-z]{3}-\\d{4}\"),  # 01-Jan-2020\n","        re.compile(r\"^\\d{8}$\"),                   # YYYYMMDD\n","    ]\n","\n","    def _looks_like_datetime(self, series: pd.Series, sample_n: int = 50) -> bool:\n","        if pd.api.types.is_datetime64_any_dtype(series):\n","            return True\n","\n","        s = series.dropna().astype(str)\n","        if s.shape[0] == 0:\n","            return False\n","\n","        sample = s.head(sample_n)\n","\n","        hit = 0\n","        for v in sample:\n","            for rx in self.DATE_REGEXES:\n","                if rx.search(v):\n","                    hit += 1\n","                    break\n","        if (hit / max(len(sample), 1)) > 0.6:\n","            return True\n","\n","        with warnings.catch_warnings():\n","            warnings.filterwarnings(\"ignore\", message=\"Could not infer format\")\n","            parsed = pd.to_datetime(sample, errors=\"coerce\")\n","        return parsed.notna().mean() > 0.8\n","\n","    def infer_column_types(self, X: pd.DataFrame) -> Dict[str, str]:\n","        types: Dict[str, str] = {}\n","        for col in X.columns:\n","            s = X[col]\n","            if pd.api.types.is_bool_dtype(s):\n","                types[col] = \"categorical\"\n","            elif pd.api.types.is_numeric_dtype(s):\n","                types[col] = \"numeric\"\n","            elif self._looks_like_datetime(s):\n","                types[col] = \"datetime\"\n","            else:\n","                types[col] = \"categorical\"\n","        return types\n","\n","    def compute_meta(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> Dict[str, Any]:\n","        n_rows, n_cols = X.shape\n","        col_types = self.infer_column_types(X)\n","        missing_frac = X.isna().mean().to_dict()\n","        cardinality = X.nunique(dropna=True).to_dict()\n","\n","        meta: Dict[str, Any] = {\n","            \"n_rows\": n_rows,\n","            \"n_cols\": n_cols,\n","            \"col_types\": col_types,\n","            \"missing_frac\": missing_frac,\n","            \"cardinality\": cardinality,\n","        }\n","\n","        if y is not None:\n","            y_s = pd.Series(y)\n","            counts = y_s.value_counts().to_dict()\n","            if counts:\n","                majority = max(counts.values())\n","                minority = min(counts.values())\n","                imbalance_ratio = majority / max(1, minority)\n","            else:\n","                counts = {}\n","                imbalance_ratio = 1.0\n","            meta[\"class_counts\"] = counts\n","            meta[\"imbalance_ratio\"] = imbalance_ratio\n","\n","        return meta\n","\n","\n","class FrequencyEncoder(BaseEstimator, TransformerMixin):\n","    def __init__(self, unseen_value: float = 0.0):\n","        self.unseen_value = unseen_value\n","        self.mappings_: List[Dict[Any, float]] = []\n","        self.n_features_in_: Optional[int] = None\n","\n","    def fit(self, X, y=None):\n","        arr = self._to_2d(X)\n","        self.n_features_in_ = arr.shape[1]\n","        self.mappings_ = []\n","        for j in range(arr.shape[1]):\n","            col = pd.Series(arr[:, j])\n","            self.mappings_.append(col.value_counts(normalize=True).to_dict())\n","        return self\n","\n","    def transform(self, X):\n","        arr = self._to_2d(X)\n","        out = np.zeros((arr.shape[0], arr.shape[1]), dtype=float)\n","        for j in range(arr.shape[1]):\n","            mapping = self.mappings_[j]\n","            out[:, j] = pd.Series(arr[:, j]).map(mapping).fillna(self.unseen_value).to_numpy(dtype=float)\n","        return out\n","\n","    def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]:\n","        if input_features is None:\n","            n = self.n_features_in_ or 0\n","            return [f\"x{j}_freq\" for j in range(n)]\n","        return [f\"{name}_freq\" for name in input_features]\n","\n","    @staticmethod\n","    def _to_2d(X):\n","        if isinstance(X, pd.DataFrame):\n","            return X.to_numpy()\n","        if isinstance(X, pd.Series):\n","            return X.to_numpy().reshape(-1, 1)\n","        arr = np.asarray(X)\n","        if arr.ndim == 1:\n","            arr = arr.reshape(-1, 1)\n","        return arr\n","\n","\n","def _make_ohe(cfg: PreprocessingConfig):\n","    base = {\"handle_unknown\": \"ignore\"}\n","    candidates = []\n","\n","    kw = dict(base)\n","    kw[\"sparse_output\"] = False\n","    if cfg.ohe_min_frequency is not None:\n","        kw[\"min_frequency\"] = cfg.ohe_min_frequency\n","    if cfg.ohe_max_categories is not None:\n","        kw[\"max_categories\"] = cfg.ohe_max_categories\n","    candidates.append(kw)\n","\n","    kw = dict(base)\n","    kw[\"sparse\"] = False\n","    if cfg.ohe_min_frequency is not None:\n","        kw[\"min_frequency\"] = cfg.ohe_min_frequency\n","    if cfg.ohe_max_categories is not None:\n","        kw[\"max_categories\"] = cfg.ohe_max_categories\n","    candidates.append(kw)\n","\n","    candidates.append({**base, \"sparse_output\": False})\n","    candidates.append({**base, \"sparse\": False})\n","    candidates.append(base)\n","\n","    last_err = None\n","    for c in candidates:\n","        try:\n","            return OneHotEncoder(**c)\n","        except TypeError as e:\n","            last_err = e\n","    raise last_err\n","\n","\n","def build_rule_based_config(meta: Dict[str, Any], cfg: Optional[PreprocessingConfig] = None) -> Tuple[PreprocessingConfig, Dict[str, Any]]:\n","    cfg = cfg or PreprocessingConfig()\n","    cfg = PreprocessingConfig(**asdict(cfg))\n","\n","    if cfg.feature_selection == \"auto\":\n","        cfg.feature_selection = \"mutual_info\" if meta.get(\"n_cols\", 0) > 30 else \"none\"\n","\n","    drop_cols: List[str] = []\n","    for col, frac in meta.get(\"missing_frac\", {}).items():\n","        if frac > cfg.drop_missing_threshold:\n","            drop_cols.append(col)\n","    for col, c in meta.get(\"cardinality\", {}).items():\n","        if c <= 1 and col not in drop_cols:\n","            drop_cols.append(col)\n","\n","    return cfg, {\"drop_cols\": drop_cols}\n","\n","\n","class AutomataPreprocessor(BaseEstimator, TransformerMixin):\n","    def __init__(\n","        self,\n","        config: Optional[PreprocessingConfig] = None,\n","        strategy: str = \"auto\",\n","        verbose: bool = False,\n","\n","        report: bool = False,\n","        report_path: str = \"prep_report.pdf\",\n","        project_name: str = \"Automata AI Project\",\n","        author: str = \"\",\n","        logo_path: Optional[str] = None,\n","    ):\n","        self.config = config\n","        self.strategy = strategy\n","        self.verbose = verbose\n","        if self.verbose:\n","            logger.setLevel(logging.DEBUG)\n","\n","        self.report = report\n","        self.report_path = report_path\n","        self.project_name = project_name\n","        self.author = author\n","        self.logo_path = logo_path\n","\n","        # NEW: safe reference to report generator (module, function_name)\n","        self.report_generator_ref_: Optional[Tuple[str, str]] = None\n","\n","        self.analyzer_ = DatasetAnalyzer()\n","\n","        self.meta_: Optional[Dict[str, Any]] = None\n","        self.meta_used_: Optional[Dict[str, Any]] = None\n","\n","        self.config_: Optional[PreprocessingConfig] = None\n","        self.aux_: Optional[Dict[str, Any]] = None\n","        self.pipeline_: Optional[Pipeline] = None\n","\n","        self.feature_names_in_: Optional[List[str]] = None\n","        self.output_feature_names_: Optional[List[str]] = None\n","\n","        self.class_weights_: Optional[Dict[Any, float]] = None\n","        self.applied_: Dict[str, Any] = {}\n","        self.fs_k_: Optional[int] = None\n","\n","        self.drop_cols_: List[str] = []\n","        self.datetime_cols_: List[str] = []\n","        self.datetime_generated_cols_: List[str] = []\n","\n","    # NEW: lets you set the generator reliably (works locally + notebook)\n","    def set_report_generator(self, fn: Callable):\n","        self.report_generator_ref_ = (fn.__module__, fn.__name__)\n","        return self\n","\n","    def fit(self, X, y=None):\n","        X = self._ensure_df(X)\n","        self.feature_names_in_ = list(X.columns)\n","\n","        self.meta_ = self.analyzer_.compute_meta(X, y)\n","        if self.strategy == \"auto\" or self.config is None:\n","            self.config_, self.aux_ = build_rule_based_config(self.meta_, None)\n","        else:\n","            self.config_, self.aux_ = build_rule_based_config(self.meta_, self.config)\n","\n","        self.drop_cols_ = list((self.aux_ or {}).get(\"drop_cols\", []))\n","        X_used = X.drop(columns=self.drop_cols_, errors=\"ignore\")\n","\n","        X_used = self._handle_datetime_fit(X_used)\n","\n","        col_types_used = self.analyzer_.infer_column_types(X_used)\n","        numeric_cols = [c for c, t in col_types_used.items() if t == \"numeric\"]\n","        categorical_cols = [c for c, t in col_types_used.items() if t == \"categorical\"]\n","\n","        missing_frac_used = X_used.isna().mean().to_dict()\n","        cardinality_used = X_used.nunique(dropna=True).to_dict()\n","        self.meta_used_ = {\n","            \"n_rows\": int(X_used.shape[0]),\n","            \"n_cols\": int(X_used.shape[1]),\n","            \"col_types\": col_types_used,\n","            \"missing_frac\": missing_frac_used,\n","            \"cardinality\": cardinality_used,\n","        }\n","\n","        numeric_missing_cols = [c for c in numeric_cols if missing_frac_used.get(c, 0.0) > 0.0]\n","\n","        # split cat into low/high  (CHANGED: remove fraction rule; use min_unique only)\n","        card = X_used[categorical_cols].nunique(dropna=True) if categorical_cols else pd.Series(dtype=int)\n","        low_card_cols: List[str] = []\n","        high_card_cols: List[str] = []\n","        for c in categorical_cols:\n","            uniq = int(card.get(c, 0))\n","            if uniq >= int(self.config_.high_cardinality_min_unique):\n","                high_card_cols.append(c)\n","            else:\n","                low_card_cols.append(c)\n","\n","        transformers = []\n","\n","        if numeric_cols:\n","            num_steps = []\n","            if numeric_missing_cols:\n","                num_steps.append((\"imputer\", SimpleImputer(strategy=self.config_.numeric_imputer)))\n","\n","            if self.config_.numeric_scaler == \"standard\":\n","                num_steps.append((\"scaler\", StandardScaler()))\n","            elif self.config_.numeric_scaler == \"minmax\":\n","                num_steps.append((\"scaler\", MinMaxScaler()))\n","            elif self.config_.numeric_scaler == \"robust\":\n","                num_steps.append((\"scaler\", RobustScaler()))\n","\n","            transformers.append((\"num\", Pipeline(num_steps), numeric_cols) if num_steps else (\"num\", \"passthrough\", numeric_cols))\n","\n","        if low_card_cols:\n","            low_missing_cols = [c for c in low_card_cols if missing_frac_used.get(c, 0.0) > 0.0]\n","            low_steps = []\n","            if low_missing_cols:\n","                low_steps.append((\"imputer\", SimpleImputer(strategy=self.config_.categorical_imputer)))\n","            low_steps.append((\"ohe\", _make_ohe(self.config_)))\n","            transformers.append((\"cat_low\", Pipeline(low_steps), low_card_cols))\n","\n","        if high_card_cols:\n","            high_missing_cols = [c for c in high_card_cols if missing_frac_used.get(c, 0.0) > 0.0]\n","            high_steps = []\n","            if high_missing_cols:\n","                high_steps.append((\"imputer\", SimpleImputer(strategy=self.config_.categorical_imputer)))\n","            high_steps.append((\"freq\", FrequencyEncoder(unseen_value=0.0)))\n","            transformers.append((\"cat_high\", Pipeline(high_steps), high_card_cols))\n","\n","        if not transformers:\n","            empty = FunctionTransformer(lambda Z: np.zeros((len(Z), 0)), validate=False)\n","            self.pipeline_ = Pipeline([(\"empty\", empty)])\n","            self.pipeline_.fit(X_used, y) if y is not None else self.pipeline_.fit(X_used)\n","            self.output_feature_names_ = []\n","            self.class_weights_ = None\n","            self._populate_applied(\n","                numeric_cols, categorical_cols, numeric_missing_cols,\n","                low_card_cols, high_card_cols, False, False, missing_frac_used\n","            )\n","            self._maybe_generate_report()\n","            return self\n","\n","        ct = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n","\n","        pre_pipe = Pipeline([(\"preprocess\", ct)])\n","        pre_pipe.fit(X_used, y) if y is not None else pre_pipe.fit(X_used)\n","\n","        fs_used = False\n","        self.fs_k_ = None\n","        self.pipeline_ = pre_pipe\n","\n","        names_pre = self._safe_get_preprocess_feature_names(X_used) or []\n","        n_pre = len(names_pre) if names_pre else int(pre_pipe.transform(X_used.head(1)).shape[1])\n","\n","        if (self.config_.feature_selection == \"mutual_info\") and (y is not None) and (n_pre > 0):\n","            k = max(int(self.config_.feature_fraction * max(1, n_pre)), 1)\n","            k = min(k, n_pre)\n","            fs = SelectKBest(score_func=mutual_info_classif, k=k)\n","            full_pipe = Pipeline([(\"preprocess\", ct), (\"fs\", fs)])\n","            try:\n","                full_pipe.fit(X_used, y)\n","                self.pipeline_ = full_pipe\n","                fs_used = True\n","                self.fs_k_ = k\n","            except Exception as e:\n","                if self.verbose:\n","                    logger.debug(\"Feature selection disabled due to error: %s\", e)\n","                self.pipeline_ = pre_pipe\n","                fs_used = False\n","                self.fs_k_ = None\n","\n","        self.output_feature_names_ = self._compute_output_feature_names(X_used)\n","\n","        self.class_weights_ = None\n","        balancing_used = False\n","        if y is not None and self.config_.balancing == \"class_weight\":\n","            ir = float(self.meta_.get(\"imbalance_ratio\", 1.0))\n","            if ir > float(self.config_.imbalance_threshold):\n","                y_s = pd.Series(y)\n","                counts = y_s.value_counts().to_dict()\n","                total = int(len(y_s))\n","                n_classes = max(len(counts), 1)\n","                self.class_weights_ = {cls: total / (n_classes * cnt) for cls, cnt in counts.items()}\n","                balancing_used = True\n","\n","        self._populate_applied(\n","            numeric_cols, categorical_cols, numeric_missing_cols,\n","            low_card_cols, high_card_cols, fs_used, balancing_used, missing_frac_used\n","        )\n","\n","        self._maybe_generate_report()\n","        return self\n","\n","    def transform(self, X):\n","        if self.pipeline_ is None:\n","            raise RuntimeError(\"Call fit before transform.\")\n","\n","        X = self._ensure_df(X)\n","        X = self._align_schema(X)\n","\n","        X_used = X.drop(columns=self.drop_cols_, errors=\"ignore\")\n","        X_used = self._handle_datetime_transform(X_used)\n","\n","        return self.pipeline_.transform(X_used)\n","\n","    def fit_transform(self, X, y=None, **kwargs):\n","        return self.fit(X, y).transform(X)\n","\n","    def get_feature_names_out(self) -> List[str]:\n","        if self.output_feature_names_ is None:\n","            raise RuntimeError(\"Feature names not available; fit first.\")\n","        return list(self.output_feature_names_)\n","\n","    def save(self, path: str):\n","        joblib.dump(self, path)\n","        logger.info(\"Saved AutomataPreprocessor to %s\", path)\n","\n","    @staticmethod\n","    def load(path: str):\n","        obj = joblib.load(path)\n","        if not isinstance(obj, AutomataPreprocessor):\n","            raise ValueError(\"Loaded object is not AutomataPreprocessor\")\n","        logger.info(\"Loaded AutomataPreprocessor from %s\", path)\n","        return obj\n","\n","    @staticmethod\n","    def _ensure_df(X):\n","        if isinstance(X, pd.DataFrame):\n","            return X.copy()\n","        return pd.DataFrame(X)\n","\n","    def _align_schema(self, X: pd.DataFrame) -> pd.DataFrame:\n","        if not self.feature_names_in_:\n","            return X\n","        X2 = X.copy()\n","        for c in self.feature_names_in_:\n","            if c not in X2.columns:\n","                X2[c] = np.nan\n","        X2 = X2[self.feature_names_in_]\n","        return X2\n","\n","    def _handle_datetime_fit(self, X: pd.DataFrame) -> pd.DataFrame:\n","        cfg = self.config_ or PreprocessingConfig()\n","        col_types = self.analyzer_.infer_column_types(X)\n","        dt_cols = [c for c, t in col_types.items() if t == \"datetime\"]\n","        self.datetime_cols_ = dt_cols\n","\n","        if not dt_cols:\n","            self.datetime_generated_cols_ = []\n","            return X\n","\n","        if cfg.datetime_handling == \"drop\":\n","            self.datetime_generated_cols_ = []\n","            return X.drop(columns=dt_cols, errors=\"ignore\")\n","\n","        out = X.copy()\n","        gen_cols: List[str] = []\n","        for c in dt_cols:\n","            ser = pd.to_datetime(out[c], errors=\"coerce\")\n","            if \"year\" in cfg.datetime_extract_parts:\n","                out[f\"{c}__year\"] = ser.dt.year; gen_cols.append(f\"{c}__year\")\n","            if \"month\" in cfg.datetime_extract_parts:\n","                out[f\"{c}__month\"] = ser.dt.month; gen_cols.append(f\"{c}__month\")\n","            if \"day\" in cfg.datetime_extract_parts:\n","                out[f\"{c}__day\"] = ser.dt.day; gen_cols.append(f\"{c}__day\")\n","            if \"dayofweek\" in cfg.datetime_extract_parts:\n","                out[f\"{c}__dayofweek\"] = ser.dt.dayofweek; gen_cols.append(f\"{c}__dayofweek\")\n","\n","        out = out.drop(columns=dt_cols, errors=\"ignore\")\n","        self.datetime_generated_cols_ = gen_cols\n","        return out\n","\n","    def _handle_datetime_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n","        cfg = self.config_ or PreprocessingConfig()\n","        if not self.datetime_cols_:\n","            return X\n","\n","        if cfg.datetime_handling == \"drop\":\n","            return X.drop(columns=self.datetime_cols_, errors=\"ignore\")\n","\n","        out = X.copy()\n","        for c in self.datetime_cols_:\n","            if c not in out.columns:\n","                out[c] = pd.NaT\n","            ser = pd.to_datetime(out[c], errors=\"coerce\")\n","            if f\"{c}__year\" in self.datetime_generated_cols_:\n","                out[f\"{c}__year\"] = ser.dt.year\n","            if f\"{c}__month\" in self.datetime_generated_cols_:\n","                out[f\"{c}__month\"] = ser.dt.month\n","            if f\"{c}__day\" in self.datetime_generated_cols_:\n","                out[f\"{c}__day\"] = ser.dt.day\n","            if f\"{c}__dayofweek\" in self.datetime_generated_cols_:\n","                out[f\"{c}__dayofweek\"] = ser.dt.dayofweek\n","\n","        out = out.drop(columns=self.datetime_cols_, errors=\"ignore\")\n","        return out\n","\n","    def _populate_applied(\n","        self,\n","        numeric_cols: List[str],\n","        categorical_cols: List[str],\n","        numeric_missing_cols: List[str],\n","        low_card_cols: List[str],\n","        high_card_cols: List[str],\n","        feature_selection_used: bool,\n","        balancing_used: bool,\n","        missing_frac_used: Dict[str, float],\n","    ):\n","        cfg = self.config_ or PreprocessingConfig()\n","        low_missing_cols = [c for c in low_card_cols if missing_frac_used.get(c, 0.0) > 0.0]\n","        high_missing_cols = [c for c in high_card_cols if missing_frac_used.get(c, 0.0) > 0.0]\n","\n","        self.applied_ = {\n","            \"drop_cols\": list(self.drop_cols_),\n","            \"datetime_cols\": list(self.datetime_cols_),\n","            \"datetime_handling\": cfg.datetime_handling,\n","            \"datetime_generated_cols\": list(self.datetime_generated_cols_),\n","\n","            \"numeric_cols\": list(numeric_cols),\n","            \"numeric_missing_cols\": list(numeric_missing_cols),\n","            \"numeric_imputer_used\": bool(numeric_missing_cols),\n","            \"numeric_scaler_used\": bool(numeric_cols) and (cfg.numeric_scaler != \"none\"),\n","\n","            \"categorical_cols\": list(categorical_cols),\n","\n","            \"low_card_cols\": list(low_card_cols),\n","            \"low_card_missing_cols\": list(low_missing_cols),\n","            \"low_card_imputer_used\": bool(low_missing_cols),\n","            \"low_card_encoder_used\": bool(low_card_cols),\n","\n","            \"high_card_cols\": list(high_card_cols),\n","            \"high_card_missing_cols\": list(high_missing_cols),\n","            \"high_card_imputer_used\": bool(high_missing_cols),\n","            \"high_card_encoder_used\": bool(high_card_cols),\n","\n","            \"feature_selection_used\": bool(feature_selection_used),\n","            \"feature_selection_method\": (cfg.feature_selection if feature_selection_used else \"none\"),\n","            \"feature_fraction\": float(cfg.feature_fraction),\n","            \"fs_k\": self.fs_k_,\n","\n","            \"balancing_used\": bool(balancing_used),\n","            \"balancing_method\": (\"class_weight\" if balancing_used else \"none\"),\n","            \"imbalance_threshold\": float(cfg.imbalance_threshold),\n","        }\n","\n","    def _compute_output_feature_names(self, X_used: pd.DataFrame) -> List[str]:\n","        names_pre = self._safe_get_preprocess_feature_names(X_used) or []\n","\n","        if self.pipeline_ is not None and \"fs\" in getattr(self.pipeline_, \"named_steps\", {}):\n","            fs = self.pipeline_.named_steps[\"fs\"]\n","            try:\n","                mask = fs.get_support()\n","                if len(names_pre) == len(mask):\n","                    return list(np.array(names_pre, dtype=object)[mask])\n","            except Exception:\n","                pass\n","\n","        return names_pre\n","\n","    def _safe_get_preprocess_feature_names(self, X_used: pd.DataFrame) -> Optional[List[str]]:\n","        try:\n","            if self.pipeline_ is None or \"preprocess\" not in self.pipeline_.named_steps:\n","                return []\n","            ct = self.pipeline_.named_steps[\"preprocess\"]\n","\n","            if hasattr(ct, \"get_feature_names_out\"):\n","                try:\n","                    return list(ct.get_feature_names_out(input_features=list(X_used.columns)))\n","                except Exception:\n","                    try:\n","                        return list(ct.get_feature_names_out())\n","                    except Exception:\n","                        pass\n","\n","            names: List[str] = []\n","            for name, trans, cols in getattr(ct, \"transformers_\", []):\n","                if name == \"remainder\":\n","                    continue\n","                if trans == \"passthrough\":\n","                    if isinstance(cols, (list, tuple)):\n","                        names.extend([f\"{name}__{c}\" for c in cols])\n","                    else:\n","                        names.append(f\"{name}__{cols}\")\n","                    continue\n","                if hasattr(trans, \"get_feature_names_out\"):\n","                    try:\n","                        out = trans.get_feature_names_out(cols if isinstance(cols, (list, tuple)) else None)\n","                        names.extend(list(out))\n","                        continue\n","                    except Exception:\n","                        pass\n","                if isinstance(cols, (list, tuple)):\n","                    names.extend([f\"{name}__{c}\" for c in cols])\n","                else:\n","                    names.append(f\"{name}__{cols}\")\n","\n","            return names\n","        except Exception:\n","            return None\n","\n","    def _maybe_generate_report(self):\n","        if not self.report:\n","            return\n","\n","        # 1) preferred: use stored generator ref (works local + notebook)\n","        if self.report_generator_ref_:\n","            mod_name, fn_name = self.report_generator_ref_\n","            try:\n","                mod = importlib.import_module(mod_name)\n","                fn = getattr(mod, fn_name)\n","                fn(self, self.report_path)\n","                return\n","            except Exception as e:\n","                logger.warning(\"Report generator ref failed (%s.%s): %s\", mod_name, fn_name, e)\n","\n","        # 2) fallback: old behavior (notebook-friendly if present)\n","        try:\n","            from __main__ import generate_preprocessing_report\n","            generate_preprocessing_report(self, self.report_path)\n","        except Exception as e:\n","            logger.warning(\"Report requested but generator is unavailable or failed: %s\", e)\n"]},{"cell_type":"code","source":["# =========================\n","# CELL 2 — PDF Report Generator (charts + better tables + richer overview)\n","# =========================\n","\n","import os\n","import tempfile\n","import datetime\n","from collections import Counter\n","\n","from PIL import Image as PILImage\n","\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","\n","from reportlab.platypus import (\n","    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak,\n","    Image as RLImage, LongTable, KeepTogether\n",")\n","from reportlab.lib.pagesizes import A4\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib import colors\n","from reportlab.lib.units import cm\n","\n","\n","def _wrap_token(s: str) -> str:\n","    return str(s)\n","\n","\n","# Define styles globally so mono_wrap can reference `small`\n","styles = getSampleStyleSheet()\n","H1 = ParagraphStyle(\"H1\", parent=styles[\"Heading1\"], alignment=1, spaceAfter=10)\n","H2 = ParagraphStyle(\"H2\", parent=styles[\"Heading2\"], spaceBefore=6, spaceAfter=8)\n","body = ParagraphStyle(\"body\", parent=styles[\"BodyText\"], spaceAfter=6, leading=13)\n","small = ParagraphStyle(\"small\", parent=styles[\"BodyText\"], fontSize=9, leading=11, spaceAfter=6)\n","caption = ParagraphStyle(\"cap\", parent=styles[\"BodyText\"], fontSize=9, leading=11, alignment=1, spaceAfter=10)\n","sub = ParagraphStyle(\"sub\", parent=body, leftIndent=18, spaceAfter=6)\n","\n","# Cover page styles (defined here as well)\n","cover_title = ParagraphStyle(\"cover_title\", parent=styles[\"Title\"], alignment=1, fontSize=24, spaceAfter=18, leading=30)\n","cover_subtitle = ParagraphStyle(\"cover_subtitle\", parent=styles[\"h2\"], alignment=1, fontSize=16, spaceAfter=12, leading=20)\n","cover_meta = ParagraphStyle(\"cover_meta\", parent=styles[\"Normal\"], alignment=1, fontSize=10, textColor=colors.gray, spaceAfter=24)\n","cover_desc = ParagraphStyle(\"cover_desc\", parent=styles[\"Normal\"], alignment=1, fontSize=12, leading=16, spaceAfter=0)\n","\n","mono_wrap = ParagraphStyle(\n","    \"mono_wrap\",\n","    parent=small,\n","    fontName=\"Courier\",\n","    fontSize=7.5,\n","    leading=9,\n","    wordWrap=\"CJK\",\n","    splitLongWords=1,   # <-- key line\n",")\n","\n","\n","\n","def _convert_image_to_png(img_path: str) -> str:\n","    img = PILImage.open(img_path).convert(\"RGBA\")\n","    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n","    tmp.close()\n","    img.save(tmp.name, format=\"PNG\")\n","    return tmp.name\n","\n","def _save_fig_to_png(fig) -> str:\n","    \"\"\"Save a matplotlib figure as a high-DPI PNG suitable for embedding in PDF.\"\"\"\n","    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n","    tmp.close()\n","    fig.savefig(\n","        tmp.name,\n","        dpi=300,\n","        bbox_inches=\"tight\",\n","        pad_inches=0.06,\n","        facecolor=\"white\",\n","    )\n","    plt.close(fig)\n","    return tmp.name\n","\n","\n","def _fit_rl_image(img_path: str, max_w: float, max_h: float) -> RLImage:\n","    im = PILImage.open(img_path)\n","    w, h = im.size\n","    im.close()\n","    scale = min(max_w / float(w), max_h / float(h))\n","    return RLImage(img_path, width=w * scale, height=h * scale)\n","\n","\n","def _set_pub_rcparams():\n","    # Light, publication-friendly defaults (kept conservative to avoid layout surprises).\n","    plt.rcParams.update({\n","        \"font.size\": 9,\n","        \"axes.titlesize\": 11,\n","        \"axes.labelsize\": 9,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8,\n","        \"legend.fontsize\": 8,\n","        \"figure.dpi\": 150,\n","        \"savefig.dpi\": 300,\n","        \"axes.linewidth\": 0.8,\n","    })\n","\n","_set_pub_rcparams()\n","\n","\n","def _style_axes(ax, grid_axis: str):\n","    ax.set_axisbelow(True)\n","    ax.grid(True, axis=grid_axis, linestyle=\"--\", linewidth=0.6, alpha=0.35)\n","    for s in (\"top\", \"right\"):\n","        ax.spines[s].set_visible(False)\n","    for s in (\"left\", \"bottom\"):\n","        ax.spines[s].set_linewidth(0.8)\n","        ax.spines[s].set_alpha(0.7)\n","    ax.tick_params(axis=\"both\", which=\"both\", length=3, width=0.8)\n","    return ax\n","\n","\n","def _add_bar_labels(ax, bars, labels, orientation: str):\n","    \"\"\"labels is a list[str] with same length as bars.\"\"\"\n","    try:\n","        ax.bar_label(bars, labels=labels, padding=3, fontsize=8)\n","    except Exception:\n","        pass\n","\n","\n","\n","def _chart_missingness(missing_frac: dict, top_n: int = 12, threshold: float | None = None) -> str | None:\n","    if not missing_frac:\n","        return None\n","    items = sorted(missing_frac.items(), key=lambda x: x[1], reverse=True)[:top_n]\n","    if not items or max(v for _, v in items) <= 0:\n","        return None\n","\n","    cols_, vals_ = zip(*items)\n","    vals_pct = [v * 100 for v in vals_]\n","\n","    def _short(s: str, n: int = 22) -> str:\n","        s = str(s)\n","        return s if len(s) <= n else (s[: n - 1] + \"…\")\n","\n","    cols_disp = [_short(c) for c in cols_]\n","\n","    fig, ax = plt.subplots(figsize=(7.1, 3.9))\n","    bars = ax.barh(range(len(cols_disp)), vals_pct)\n","    ax.set_yticks(range(len(cols_disp)))\n","    ax.set_yticklabels(cols_disp)\n","    ax.invert_yaxis()\n","    ax.set_xlim(0, max(100, max(vals_pct) * 1.15))\n","    ax.set_xlabel(\"Missing values (%)\")\n","    ax.set_title(\"Missingness (top columns)\")\n","    _style_axes(ax, grid_axis=\"x\")\n","\n","    if threshold is not None:\n","        try:\n","            t = float(threshold) * 100.0\n","            ax.axvline(t, linestyle=\":\", linewidth=1.2)\n","            ax.text(t, ax.get_ylim()[0], f\"  drop @ {t:.0f}%\", va=\"bottom\", fontsize=8)\n","        except Exception:\n","            pass\n","\n","    _add_bar_labels(ax, bars, [f\"{v:.1f}%\" for v in vals_pct], orientation=\"h\")\n","    fig.tight_layout()\n","    return _save_fig_to_png(fig)\n","\n","\n","def _chart_coltype_counts(col_types: dict) -> str | None:\n","    if not col_types:\n","        return None\n","    c = Counter(col_types.values())\n","\n","    preferred = [\"numeric\", \"categorical\", \"datetime\"]\n","    labels = [k for k in preferred if k in c] + [k for k in sorted(c.keys()) if k not in preferred]\n","    values = [c[k] for k in labels]\n","    if sum(values) == 0:\n","        return None\n","\n","    fig, ax = plt.subplots(figsize=(6.6, 3.4))\n","    bars = ax.bar(labels, values, width=0.58)\n","    ax.set_ylabel(\"Number of columns\")\n","    ax.set_title(\"Feature type composition (before encoding)\")\n","    _style_axes(ax, grid_axis=\"y\")\n","    _add_bar_labels(ax, bars, [str(v) for v in values], orientation=\"v\")\n","    fig.tight_layout()\n","    return _save_fig_to_png(fig)\n","\n","\n","def _chart_class_counts(class_counts: dict) -> str | None:\n","    if not class_counts:\n","        return None\n","    items = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n","    labels = [str(k) for k, _ in items]\n","    values = [v for _, v in items]\n","    total = sum(values)\n","    if total == 0:\n","        return None\n","\n","    fig, ax = plt.subplots(figsize=(6.9, 3.4))\n","    bars = ax.bar(labels, values, width=0.58)\n","    ax.set_ylabel(\"Samples\")\n","    ax.set_title(\"Target class distribution\")\n","    _style_axes(ax, grid_axis=\"y\")\n","\n","    bar_labels = [f\"{v}\\n({(v/total)*100:.1f}%)\" for v in values]\n","    _add_bar_labels(ax, bars, bar_labels, orientation=\"v\")\n","\n","    ax.tick_params(axis=\"x\", rotation=0)\n","    fig.tight_layout()\n","    return _save_fig_to_png(fig)\n","\n","\n","def _chart_feature_source_breakdown(feature_names: list[str]) -> str | None:\n","    if not feature_names:\n","        return None\n","\n","    def src(n: str) -> str:\n","        return n.split(\"__\", 1)[0] if \"__\" in n else \"features\"\n","\n","    c = Counter(src(n) for n in feature_names)\n","    items = sorted(c.items(), key=lambda x: x[1], reverse=True)\n","    labels = [k for k, _ in items]\n","    values = [v for _, v in items]\n","    if sum(values) == 0:\n","        return None\n","\n","    fig, ax = plt.subplots(figsize=(6.8, 3.4))\n","    bars = ax.barh(labels, values, height=0.58)\n","    ax.invert_yaxis()\n","    ax.set_xlabel(\"Number of output features\")\n","    ax.set_title(\"Final feature contribution by transformer\")\n","    _style_axes(ax, grid_axis=\"x\")\n","    _add_bar_labels(ax, bars, [str(v) for v in values], orientation=\"h\")\n","    fig.tight_layout()\n","    return _save_fig_to_png(fig)\n","\n","\n","def generate_preprocessing_report(prep, path: str = \"prep_report.pdf\"):\n","    meta = getattr(prep, \"meta_\", None) or {}\n","    meta_used = getattr(prep, \"meta_used_\", None) or {}\n","    applied = getattr(prep, \"applied_\", None) or {}\n","    cfg = getattr(prep, \"config_\", None)\n","\n","    doc = SimpleDocTemplate(\n","        path,\n","        pagesize=A4,\n","        rightMargin=2*cm,\n","        leftMargin=2*cm,\n","        topMargin=3.1*cm,   # room for header\n","        bottomMargin=2.0*cm\n","    )\n","\n","    story = []\n","    tmp_files = []\n","\n","    # -------------------------\n","    # Logo handling (supports .webp via PIL conversion)\n","    # -------------------------\n","    logo_path = getattr(prep, \"logo_path\", None)\n","    logo_png = None\n","    if logo_path and os.path.exists(logo_path):\n","        try:\n","            logo_png = _convert_image_to_png(logo_path)\n","            tmp_files.append(logo_png)\n","        except Exception:\n","            logo_png = None\n","\n","    # -------------------------\n","    # Header / Footer\n","    # -------------------------\n","    def draw_header_footer(canvas, doc_):\n","        canvas.saveState()\n","\n","        # header baseline\n","        header_top = doc_.pagesize[1] - 1.0*cm\n","        header_bottom = doc_.pagesize[1] - doc_.topMargin + 0.25*cm\n","\n","        # logo (smaller + better placement)\n","        if logo_png:\n","            lw, lh = (1.25*cm, 1.25*cm)\n","            x = doc_.leftMargin\n","            y = header_top - lh\n","            canvas.drawImage(\n","                logo_png, x, y,\n","                width=lw, height=lh,\n","                preserveAspectRatio=True, mask=\"auto\"\n","            )\n","\n","        # title on the right\n","        canvas.setFont(\"Helvetica-Bold\", 10)\n","        canvas.drawRightString(\n","            doc_.pagesize[0] - doc_.rightMargin,\n","            doc_.pagesize[1] - 1.35*cm,\n","            getattr(prep, \"project_name\", \"Automata AI - Preprocessing Report\")\n","        )\n","\n","        # subtle header line\n","        canvas.setLineWidth(0.4)\n","        canvas.setStrokeColor(colors.grey)\n","        canvas.line(doc_.leftMargin, header_bottom, doc_.pagesize[0] - doc_.rightMargin, header_bottom)\n","\n","        # footer\n","        canvas.setFont(\"Helvetica\", 8)\n","        canvas.setFillColor(colors.black)\n","        canvas.drawString(doc_.leftMargin, 1.15*cm, f\"Page {doc_.page}\")\n","        canvas.drawRightString(\n","            doc_.pagesize[0] - doc_.rightMargin,\n","            1.15*cm,\n","            f\"© {datetime.datetime.now().year} Automata AI — All rights reserved\"\n","        )\n","\n","        canvas.restoreState()\n","\n","    # -------------------------\n","    # Cover (final polished)\n","    # -------------------------\n","\n","    # Push content down from top\n","    story.append(Spacer(1, 2.6 * cm))\n","\n","    # Bigger logo, centered, lower on page\n","    if logo_png:\n","        cover_logo = _fit_rl_image(logo_png, max_w=6.5 * cm, max_h=6.5 * cm)\n","        cover_logo.hAlign = \"CENTER\"\n","        story.append(cover_logo)\n","\n","    # Space between logo and title\n","    story.append(Spacer(1, 1.2 * cm))\n","\n","    # Title block (centered)\n","    story.append(Paragraph(getattr(prep, \"project_name\", \"Automata AI\"), cover_title))\n","    story.append(Paragraph(\"Automated Preprocessing Report\", cover_subtitle))\n","    story.append(Paragraph(\n","        f\"Generated on {datetime.datetime.now():%Y-%m-%d %H:%M:%S}\",\n","        cover_meta\n","    ))\n","\n","    # EXTRA breathing room before description (you asked for this)\n","    story.append(Spacer(1, 1.2 * cm))\n","\n","    story.append(Paragraph(\n","        \"This report summarizes dataset characteristics, preprocessing decisions, and the resulting feature space.\",\n","        cover_desc\n","    ))\n","\n","    story.append(PageBreak())\n","\n","\n","    # -------------------------\n","    # 1) Dataset Overview (richer)\n","    # -------------------------\n","    story.append(Paragraph(\"1. Dataset Overview\", H2))\n","\n","    # prefer \"used\" meta when available\n","    n_rows = meta_used.get(\"n_rows\", meta.get(\"n_rows\", \"\"))\n","    n_cols_raw = meta.get(\"n_cols\", \"\")\n","    n_cols_used = meta_used.get(\"n_cols\", \"\")\n","\n","    col_types_used = meta_used.get(\"col_types\", meta.get(\"col_types\", {})) or {}\n","    missing_used = meta_used.get(\"missing_frac\", meta.get(\"missing_frac\", {})) or {}\n","    card_used = meta_used.get(\"cardinality\", meta.get(\"cardinality\", {})) or {}\n","\n","    type_counts = Counter(col_types_used.values()) if col_types_used else Counter()\n","    dropped_cols = applied.get(\"drop_cols\", []) or []\n","    dt_cols = applied.get(\"datetime_cols\", []) or []\n","    low_cols = applied.get(\"low_card_cols\", []) or []\n","    high_cols = applied.get(\"high_card_cols\", []) or []\n","\n","    avg_missing = (sum(missing_used.values()) / max(len(missing_used), 1)) if missing_used else 0.0\n","    max_missing = max(missing_used.values()) if missing_used else 0.0\n","\n","    overview_rows = [\n","        [\"Samples\", str(n_rows)],\n","        [\"Raw features (before drops)\", str(n_cols_raw)],\n","        [\"Features used (after drops/datetime)\", str(n_cols_used)],\n","        [\"Numeric columns\", str(type_counts.get(\"numeric\", 0))],\n","        [\"Categorical columns\", str(type_counts.get(\"categorical\", 0))],\n","        [\"Datetime columns (detected)\", str(type_counts.get(\"datetime\", 0))],\n","        [\"Dropped columns\", str(len(dropped_cols))],\n","        [\"Avg missing rate (across columns)\", f\"{avg_missing*100:.2f}%\"],\n","        [\"Max missing rate (single column)\", f\"{max_missing*100:.2f}%\"],\n","        [\"Low-card categorical columns\", str(len(low_cols))],\n","        [\"High-card categorical columns\", str(len(high_cols))],\n","    ]\n","\n","    if \"class_counts\" in meta:\n","        counts = meta.get(\"class_counts\", {}) or {}\n","        ir = meta.get(\"imbalance_ratio\", None)\n","        overview_rows += [\n","            [\"# Classes\", str(len(counts))],\n","            [\"Imbalance ratio\", f\"{float(ir):.3f}\" if ir is not None else \"\"],\n","        ]\n","\n","    t = Table(overview_rows, colWidths=[7.5*cm, 8.5*cm])\n","    t.setStyle(TableStyle([\n","        (\"GRID\", (0,0), (-1,-1), 0.3, colors.grey),\n","        (\"BACKGROUND\", (0,0), (-1,0), colors.whitesmoke),\n","        (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n","        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n","        (\"ROWBACKGROUNDS\", (0,1), (-1,-1), [colors.white, colors.Color(0.97,0.97,0.97)]),\n","        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n","        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n","        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n","        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n","    ]))\n","    story.append(t)\n","    story.append(Spacer(1, 0.4*cm))\n","\n","    # charts\n","    chart_paths = []\n","    try:\n","        p1 = _chart_coltype_counts(col_types_used)\n","        if p1: chart_paths.append((\"Column types\", p1))\n","        p2 = _chart_missingness(missing_used, top_n=12, threshold=getattr(cfg, \"drop_missing_threshold\", None))\n","        if p2: chart_paths.append((\"Missingness\", p2))\n","        p3 = _chart_class_counts(meta.get(\"class_counts\", {}) if \"class_counts\" in meta else {}) # type: ignore\n","        if p3: chart_paths.append((\"Target classes\", p3))\n","        for _, p in chart_paths:\n","            tmp_files.append(p)\n","\n","        for title, p in chart_paths:\n","            img = _fit_rl_image(p, max_w=doc.width, max_h=8.2*cm)\n","            story.append(img)\n","            story.append(Paragraph(title, caption))\n","    except Exception:\n","        pass\n","\n","    # optional: list dropped/datetime columns (compact)\n","    if dropped_cols:\n","        story.append(Paragraph(f\"<b>Dropped columns ({len(dropped_cols)}):</b> {', '.join(map(str, dropped_cols[:80]))}\"\n","                               + (\" ...\" if len(dropped_cols) > 80 else \"\") + \".\",\n","            small))\n","    if dt_cols and applied.get(\"datetime_handling\", \"\") == \"drop\":\n","        story.append(Paragraph(f\"<b>Datetime columns dropped ({len(dt_cols)}):</b> {', '.join(map(str, dt_cols[:80]))}\"\n","                               + (\" ...\" if len(dt_cols) > 80 else \"\") + \".\",\n","            small))\n","\n","    # config summary (uses your cfg object if present)\n","    story.append(PageBreak())\n","    if cfg is not None:\n","        cfg_rows = [\n","            [\"drop_missing_threshold\", str(getattr(cfg, \"drop_missing_threshold\", \"\"))],\n","            [\"high_cardinality_min_unique\", str(getattr(cfg, \"high_cardinality_min_unique\", \"\"))],\n","            [\"high_cardinality_threshold\", str(getattr(cfg, \"high_cardinality_threshold\", \"\"))],\n","            [\"numeric_imputer\", str(getattr(cfg, \"numeric_imputer\", \"\"))],\n","            [\"numeric_scaler\", str(getattr(cfg, \"numeric_scaler\", \"\"))],\n","            [\"categorical_imputer\", str(getattr(cfg, \"categorical_imputer\", \"\"))],\n","            [\"datetime_handling\", str(getattr(cfg, \"datetime_handling\", \"\"))],\n","            [\"feature_selection\", str(getattr(cfg, \"feature_selection\", \"\"))],\n","            [\"feature_fraction\", str(getattr(cfg, \"feature_fraction\", \"\"))],\n","            [\"balancing\", str(getattr(cfg, \"balancing\", \"\"))],\n","            [\"imbalance_threshold\", str(getattr(cfg, \"imbalance_threshold\", \"\"))],\n","        ]\n","        story.append(Spacer(1, 0.2*cm))\n","        story.append(Paragraph(\"Configuration Snapshot\", ParagraphStyle(\"h3\", parent=styles[\"Heading3\"], spaceAfter=6)))\n","        tc = Table(cfg_rows, colWidths=[7.5*cm, 8.5*cm])\n","        tc.setStyle(TableStyle([\n","            (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n","            (\"BACKGROUND\", (0,0), (-1,0), colors.whitesmoke),\n","            (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n","            (\"ROWBACKGROUNDS\", (0,1), (-1,-1), [colors.white, colors.Color(0.97,0.97,0.97)]),\n","            (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n","            (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n","            (\"TOPPADDING\", (0,0), (-1,-1), 4),\n","            (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n","        ]))\n","        story.append(tc)\n","\n","    story.append(PageBreak())\n","\n","    # -------------------------\n","    # 2) Preprocessing Steps Applied (your narrative, kept + slightly cleaner)\n","    # -------------------------\n","    story.append(Paragraph(\"2. Preprocessing Steps Applied\", H2))\n","\n","    if applied.get(\"drop_cols\"):\n","        story.append(Paragraph(\n","            f\"<b>• Column removal:</b> Columns with excessive missing values or constant values were removed. \"\n","            f\"Dropped: <b>{', '.join(applied['drop_cols'][:120])}</b>\"\n","            + (\" ...\" if len(applied[\"drop_cols\"]) > 120 else \"\") + \".\",\n","            body\n","        ))\n","\n","    if applied.get(\"datetime_cols\"):\n","        story.append(Paragraph(\n","            f\"<b>• Datetime handling:</b> Detected datetime columns: <b>{', '.join(applied['datetime_cols'])}</b>. \"\n","            f\"Handling mode: <b>{applied.get('datetime_handling','')}</b>.\",\n","            body\n","        ))\n","        if applied.get(\"datetime_generated_cols\"):\n","            story.append(Paragraph(\n","                f\"– Extracted datetime parts into: <b>{', '.join(applied['datetime_generated_cols'])}</b>.\",\n","                sub\n","            ))\n","\n","    if applied.get(\"numeric_cols\"):\n","        story.append(Paragraph(\n","            f\"<b>• Numeric processing:</b> Numeric features: <b>{', '.join(applied['numeric_cols'][:80])}</b>\"\n","            + (\" ...\" if len(applied[\"numeric_cols\"]) > 80 else \"\") + \".\",\n","            body\n","        ))\n","        if applied.get(\"numeric_imputer_used\"):\n","            story.append(Paragraph(\n","                f\"– Missing numeric values imputed using <b>{getattr(cfg,'numeric_imputer','')}</b> for: \"\n","                f\"<b>{', '.join(applied.get('numeric_missing_cols', []))}</b>.\",\n","                sub\n","            ))\n","        if applied.get(\"numeric_scaler_used\"):\n","            story.append(Paragraph(\n","                f\"– Scaling applied using <b>{getattr(cfg,'numeric_scaler','')}</b>.\",\n","                sub\n","            ))\n","\n","    if applied.get(\"low_card_cols\"):\n","        story.append(Paragraph(\n","            f\"<b>• Low-card categorical:</b> <b>{', '.join(applied['low_card_cols'][:80])}</b>\"\n","            + (\" ...\" if len(applied[\"low_card_cols\"]) > 80 else \"\") + \".\",\n","            body\n","        ))\n","        if applied.get(\"low_card_imputer_used\"):\n","            story.append(Paragraph(\n","                f\"– Missing values imputed using <b>{getattr(cfg,'categorical_imputer','')}</b>.\",\n","                sub\n","            ))\n","        story.append(Paragraph(\"– One-hot encoding applied.\", sub))\n","\n","    if applied.get(\"high_card_cols\"):\n","        story.append(Paragraph(\n","            f\"<b>• High-card categorical:</b> <b>{', '.join(applied['high_card_cols'][:80])}</b>\"\n","            + (\" ...\" if len(applied[\"high_card_cols\"]) > 80 else \"\") + \".\",\n","            body\n","        ))\n","        story.append(Paragraph(\"– Frequency encoding applied to avoid feature explosion.\", sub))\n","\n","    if applied.get(\"feature_selection_used\"):\n","        story.append(Paragraph(\n","            f\"<b>• Feature selection:</b> Method <b>{applied.get('feature_selection_method','')}</b>, \"\n","            f\"kept fraction <b>{applied.get('feature_fraction','')}</b>, k = <b>{applied.get('fs_k','')}</b>.\",\n","            body\n","        ))\n","\n","    if applied.get(\"balancing_used\"):\n","        story.append(Paragraph(\n","            f\"<b>• Imbalance handling:</b> Class weights computed (threshold {applied.get('imbalance_threshold','')}).\",\n","            body\n","        ))\n","\n","    story.append(PageBreak())\n","\n","    # -------------------------\n","    # 3) Output Feature Summary (fixed header + chart)\n","    # -------------------------\n","    story.append(Paragraph(\"3. Output Feature Summary\", H2))\n","\n","    try:\n","        out_names = list(prep.get_feature_names_out())\n","    except Exception:\n","        out_names = list(getattr(prep, \"output_feature_names_\", []) or [])\n","\n","    story.append(Paragraph(f\"The preprocessing pipeline produced <b>{len(out_names)}</b> final features.\", body))\n","\n","    # chart: feature breakdown by transformer prefix\n","    try:\n","        p_feat = _chart_feature_source_breakdown(out_names)\n","        if p_feat:\n","            tmp_files.append(p_feat)\n","            img = _fit_rl_image(p_feat, max_w=doc.width, max_h=7.8*cm)\n","            story.append(img)\n","            story.append(Paragraph(\"Final feature contribution by transformer (count)\", caption))\n","    except Exception:\n","        pass\n","\n","    # output feature summary (grouped + publication-friendly)\n","    if out_names:\n","        # total_feats = len(out_names) # Already defined above and used\n","        # story.append(Paragraph(f\"The preprocessing pipeline produced <b>{total_feats}</b> final features.\", body)) # Duplicate\n","        story.append(Spacer(1, 0.15*cm))\n","\n","        # ---- group by transformer prefix ----\n","        def _grp(n: str) -> str:\n","            n = str(n)\n","            if n.startswith(\"num__\"):\n","                return \"Numeric\"\n","            if n.startswith(\"cat_low__\"):\n","                return \"Low-card categorical (one-hot)\"\n","            if n.startswith(\"cat_high__\"):\n","                return \"High-card categorical (frequency)\"\n","            if \"__\" in n:\n","                return n.split(\"__\", 1)[0]\n","            return \"Other\"\n","\n","        groups: dict[str, list[str]] = {}\n","        for n in out_names:\n","            groups.setdefault(_grp(n), []).append(str(n))\n","\n","        preferred_order = [\"Numeric\", \"Low-card categorical (one-hot)\", \"High-card categorical (frequency)\"]\n","        ordered_groups = [g for g in preferred_order if g in groups] + [g for g in sorted(groups.keys()) if g not in preferred_order]\n","\n","        # ---- compact summary table ----\n","        summary_rows = [[\"Group\", \"Count\", \"Examples (first 3)\"]]\n","        for g in ordered_groups:\n","            feats = groups[g]\n","            ex = \", \".join(_wrap_token(x) for x in feats[:3]) + (\" ...\" if len(feats) > 3 else \"\")\n","            summary_rows.append([Paragraph(g, small), str(len(feats)), Paragraph(ex, mono_wrap)])\n","\n","\n","        summary_tbl = Table(summary_rows, colWidths=[6.0*cm, 1.6*cm, doc.width - 7.6*cm])\n","        summary_tbl.setStyle(TableStyle([\n","            (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n","            (\"BACKGROUND\", (0,0), (-1,0), colors.whitesmoke),\n","            (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n","            (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n","            (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n","            (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n","            (\"TOPPADDING\", (0,0), (-1,-1), 4),\n","            (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n","            (\"ROWBACKGROUNDS\", (0,1), (-1,-1), [colors.white, colors.Color(0.97,0.97,0.97)]),\n","        ]))\n","        story.append(summary_tbl)\n","        story.append(Spacer(1, 0.35*cm))\n","\n","        # ---- detailed per-group tables (multi-column) ----\n","        max_items_per_group = 240   # keep PDF readable for very wide feature spaces\n","        ncols = 3\n","        colw = doc.width / float(ncols)\n","\n","        # mono defined here\n","\n","        for g in ordered_groups:\n","            feats = groups[g]\n","            shown = feats[:max_items_per_group]\n","            truncated = len(feats) > max_items_per_group\n","\n","            header = Paragraph(f\"<b>{g} ({len(feats)})</b>\" + (f\" — showing first {max_items_per_group}\" if truncated else \"\"), small)\n","            data = [[header, \"\", \"\"]]\n","            for i in range(0, len(shown), ncols):\n","                row = shown[i:i+ncols]\n","                if len(row) < ncols:\n","                    row += [\"\"] * (ncols - len(row))\n","                data.append([Paragraph(_wrap_token(x), mono_wrap) if x else \"\" for x in row])\n","\n","            tf = LongTable(data, colWidths=[colw]*ncols, repeatRows=1)\n","            tf.setStyle(TableStyle([\n","                (\"SPAN\", (0,0), (-1,0)),\n","                (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n","                (\"BACKGROUND\", (0,0), (-1,0), colors.whitesmoke),\n","                (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n","                (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n","                (\"ROWBACKGROUNDS\", (0,1), (-1,-1), [colors.white, colors.Color(0.97,0.97,0.97)]),\n","                (\"LEFTPADDING\", (0,0), (-1,-1), 5),\n","                (\"RIGHTPADDING\", (0,0), (-1,-1), 5),\n","                (\"TOPPADDING\", (0,0), (-1,-1), 3),\n","                (\"BOTTOMPADDING\", (0,0), (-1,-1), 3),\n","            ]))\n","            story.append(tf)\n","            story.append(Spacer(1, 0.30*cm))\n","\n","    story.append(Paragraph(\"End of report.\", ParagraphStyle(\"end\", fontSize=9, alignment=1)))\n","\n","    # -------------------------\n","    # Build\n","    # -------------------------\n","    doc.build(story, onFirstPage=draw_header_footer, onLaterPages=draw_header_footer)\n","\n","    for f in tmp_files:\n","        try:\n","            os.remove(f)\n","        except Exception:\n","            pass\n","\n","    print(f\"[INFO] Preprocessing report saved to {path}\")\n"],"metadata":{"id":"RZsP3vKSBZIL","executionInfo":{"status":"ok","timestamp":1765994996205,"user_tz":-120,"elapsed":88,"user":{"displayName":"Ammar Alaa Mostafa Bektash","userId":"17155881455724446425"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def run_preprocessing(X, y, cfg=None):\n","    if cfg is None:\n","        cfg = PreprocessConfig()\n","\n","    prep = AutomataPreprocessor(\n","        verbose=cfg.verbose,\n","        report=cfg.report,\n","        report_path=cfg.report_path,\n","        project_name=cfg.project_name,\n","        logo_path=cfg.logo_path,\n","    )\n","\n","    prep.set_report_generator(generate_preprocessing_report)\n","\n","    Xp = prep.fit_transform(X, y)\n","    yp = y\n","\n","    return Xp, yp\n"],"metadata":{"id":"s_bFC_gKdW-l","executionInfo":{"status":"ok","timestamp":1765999333890,"user_tz":-120,"elapsed":43,"user":{"displayName":"Ammar Alaa Mostafa Bektash","userId":"17155881455724446425"}}},"execution_count":43,"outputs":[]}]}