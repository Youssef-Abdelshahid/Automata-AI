{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K51HqBSROc7d"
      },
      "outputs": [],
      "source": [
        "import random, numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as AT\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Dataset config\n",
        "DATA_ROOT = \"./data\"\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# These will be inferred automatically\n",
        "TARGET_SR: Optional[int] = None\n",
        "CLIP_SECONDS: Optional[float] = None\n",
        "TARGET_SAMPLES: Optional[int] = None\n",
        "\n",
        "# Will be filled in get_dataloaders and used by the models\n",
        "NUM_CLASSES: Optional[int] = None\n",
        "\n",
        "\n",
        "def _seed_worker(worker_id: int):\n",
        "    worker_seed = (SEED + worker_id) % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def pad_or_trim(waveform: torch.Tensor, target_len: int) -> torch.Tensor:\n",
        "    if waveform.dim() == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "    T = waveform.shape[-1]\n",
        "    if T == target_len:\n",
        "        return waveform\n",
        "    if T > target_len:\n",
        "        return waveform[:, :target_len]\n",
        "    pad = target_len - T\n",
        "    return F.pad(waveform, (0, pad))\n",
        "\n",
        "def print_preprocessing_report(config, profile, img_size: int):\n",
        "    \"\"\"\n",
        "    Rich report of what the automatic audio preprocessing decided.\n",
        "    Shows:\n",
        "      - SR and clip length\n",
        "      - duration distribution\n",
        "      - RMS distribution\n",
        "      - SR distribution\n",
        "      - min/median files per class\n",
        "      - spectrogram shapes (before/after resize)\n",
        "      - SpecAugment configuration\n",
        "    \"\"\"\n",
        "    durations = profile[\"durations\"]\n",
        "    rms_vals = profile[\"rms_values\"]\n",
        "    sr_counts = profile[\"sr_counts\"]\n",
        "    files_per_class = profile[\"files_per_class\"]\n",
        "\n",
        "    # --- basic dataset stats ---\n",
        "    num_files = profile[\"num_files\"]\n",
        "    num_classes = len(files_per_class) if files_per_class else 0\n",
        "    min_per_class = min(files_per_class.values()) if files_per_class else 0\n",
        "    median_per_class = (\n",
        "        int(np.median(list(files_per_class.values())))\n",
        "        if files_per_class else 0\n",
        "    )\n",
        "\n",
        "    # --- duration stats ---\n",
        "    if durations.size > 0:\n",
        "        dur_mean = float(durations.mean())\n",
        "        dur_p5 = float(np.percentile(durations, 5))\n",
        "        dur_p50 = float(np.percentile(durations, 50))\n",
        "        dur_p95 = float(np.percentile(durations, 95))\n",
        "    else:\n",
        "        dur_mean = dur_p5 = dur_p50 = dur_p95 = 0.0\n",
        "\n",
        "    # --- RMS stats ---\n",
        "    if rms_vals.size > 0:\n",
        "        rms_mean = float(rms_vals.mean())\n",
        "        rms_std = float(rms_vals.std())\n",
        "    else:\n",
        "        rms_mean = rms_std = 0.0\n",
        "\n",
        "    # --- SR distribution string ---\n",
        "    if sr_counts:\n",
        "        sr_items = sorted(sr_counts.items(), key=lambda kv: -kv[1])\n",
        "        sr_str = \", \".join(f\"{sr}Hz({cnt})\" for sr, cnt in sr_items)\n",
        "    else:\n",
        "        sr_str = \"N/A\"\n",
        "\n",
        "    # --- spectrogram shape estimates ---\n",
        "    target_samples = int(config.clip_seconds * config.target_sr)\n",
        "    est_frames = max(\n",
        "        1,\n",
        "        1 + (target_samples - config.win_length) // config.hop_length\n",
        "    )\n",
        "    spec_before = f\"(1, {config.n_mels}, {est_frames})\"\n",
        "    spec_after = f\"(1, {img_size}, {img_size})\"\n",
        "\n",
        "    # --- one-line summary ---\n",
        "    summary = (\n",
        "        f\"[Audio Preproc] SR={config.target_sr} | \"\n",
        "        f\"Clip={config.clip_seconds:.2f}s | \"\n",
        "        f\"Mels={config.n_mels} | \"\n",
        "        f\"GlobalNorm={'ON' if config.use_global_norm else 'OFF'} | \"\n",
        "        f\"SpecAug={'ON' if config.use_spec_augment else 'OFF'} \"\n",
        "        f\"(Tmask={config.time_mask_param}Ã—{config.num_time_masks}, \"\n",
        "        f\"Fmask={config.freq_mask_param}Ã—{config.num_freq_masks}) | \"\n",
        "        f\"Files={num_files}, Classes={num_classes}, Min/class={min_per_class}\"\n",
        "    )\n",
        "\n",
        "    print(summary)\n",
        "    print(\n",
        "        f\"  Durations (s): mean={dur_mean:.3f}, p5={dur_p5:.3f}, \"\n",
        "        f\"p50={dur_p50:.3f}, p95={dur_p95:.3f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  RMS: mean={rms_mean:.5f}, std={rms_std:.5f} | \"\n",
        "        f\"Files/class median={median_per_class}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  Sample rates: {sr_str}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  Spectrogram shapes: raw={spec_before} -> resized={spec_after}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#   Automated Audio Preprocessing (classes + best practices)\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class AudioPreprocessConfig:\n",
        "    \"\"\"Configuration for audio preprocessing.\"\"\"\n",
        "    target_sr: int = 16000\n",
        "    clip_seconds: float = 1.0\n",
        "    n_mels: int = 64\n",
        "    n_fft: int = 512\n",
        "    hop_length: int = 160\n",
        "    win_length: int = 400\n",
        "    use_global_norm: bool = True\n",
        "    use_spec_augment: bool = True\n",
        "    time_mask_param: int = 8\n",
        "    freq_mask_param: int = 8\n",
        "    num_time_masks: int = 1\n",
        "    num_freq_masks: int = 1\n",
        "\n",
        "\n",
        "class AudioPreprocessor:\n",
        "    \"\"\"\n",
        "    Scikit-learn style:\n",
        "      - fit(dataset) learns global stats over log-Mel specs\n",
        "      - transform(waveform, sr) -> normalized log-Mel image\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[AudioPreprocessConfig] = None):\n",
        "        self.config = config or AudioPreprocessConfig()\n",
        "        self._mel = None\n",
        "        self._to_db = AT.AmplitudeToDB(stype=\"power\")\n",
        "        self.global_mean: Optional[float] = None\n",
        "        self.global_std: Optional[float] = None\n",
        "        self._time_mask = None\n",
        "        self._freq_mask = None\n",
        "\n",
        "    def _ensure_transforms(self):\n",
        "        \"\"\"Create MelSpectrogram and SpecAugment modules lazily.\"\"\"\n",
        "        if self._mel is None:\n",
        "            cfg = self.config\n",
        "            self._mel = AT.MelSpectrogram(\n",
        "                sample_rate=cfg.target_sr,\n",
        "                n_fft=cfg.n_fft,\n",
        "                hop_length=cfg.hop_length,\n",
        "                win_length=cfg.win_length,\n",
        "                n_mels=cfg.n_mels,\n",
        "            )\n",
        "        if self.config.use_spec_augment and self._time_mask is None:\n",
        "            cfg = self.config\n",
        "            self._time_mask = AT.TimeMasking(time_mask_param=cfg.time_mask_param)\n",
        "            self._freq_mask = AT.FrequencyMasking(freq_mask_param=cfg.freq_mask_param)\n",
        "\n",
        "    def _waveform_to_logmel(self, waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Convert arbitrary waveform to log-Mel spectrogram without resizing.\n",
        "        Returns tensor of shape (1, n_mels, time_frames).\n",
        "        \"\"\"\n",
        "        cfg = self.config\n",
        "        if waveform.dim() == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "        if sr != cfg.target_sr:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, cfg.target_sr)\n",
        "        target_len = int(cfg.clip_seconds * cfg.target_sr)\n",
        "        waveform = pad_or_trim(waveform, target_len)\n",
        "\n",
        "        self._ensure_transforms()\n",
        "        spec = self._mel(waveform)      # (1, n_mels, time)\n",
        "        spec = self._to_db(spec)        # log-mel\n",
        "        return spec\n",
        "\n",
        "    def fit(self, dataset, max_items: int = 512) -> \"AudioPreprocessor\":\n",
        "        \"\"\"\n",
        "        Learn global mean/std over log-Mel spectrograms from a dataset.\n",
        "        Dataset is expected to yield (waveform, sr, label, ...).\n",
        "        \"\"\"\n",
        "        sums = 0.0\n",
        "        sumsq = 0.0\n",
        "        count = 0\n",
        "\n",
        "        n = len(dataset)\n",
        "        indices = list(range(n))\n",
        "        random.shuffle(indices)\n",
        "        indices = indices[:max_items]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in indices:\n",
        "                sample = dataset[i]\n",
        "                # SpeechCommands returns: (waveform, sample_rate, label, speaker_id, utt_num)\n",
        "                waveform, sr = sample[0], sample[1]\n",
        "                spec = self._waveform_to_logmel(waveform, sr)  # (1, n_mels, time)\n",
        "                v = spec.reshape(-1)\n",
        "                sums += float(v.sum())\n",
        "                sumsq += float((v ** 2).sum())\n",
        "                count += v.numel()\n",
        "\n",
        "        if count > 0:\n",
        "            self.global_mean = sums / count\n",
        "            self.global_std = max(1e-6, (sumsq / count - self.global_mean ** 2) ** 0.5)\n",
        "        else:\n",
        "            self.global_mean, self.global_std = 0.0, 1.0\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, waveform: torch.Tensor, sr: int, img_size: int, augment: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Full preprocessing pipeline: waveform -> normalized log-Mel image.\n",
        "        Returns tensor of shape (1, img_size, img_size).\n",
        "        \"\"\"\n",
        "        cfg = self.config\n",
        "        spec = self._waveform_to_logmel(waveform, sr)  # (1, n_mels, time)\n",
        "\n",
        "        # Normalization\n",
        "        if cfg.use_global_norm and self.global_mean is not None:\n",
        "            spec = (spec - self.global_mean) / (self.global_std + 1e-6)\n",
        "        else:\n",
        "            # Per-sample normalization fallback\n",
        "            mean = spec.mean()\n",
        "            std = spec.std()\n",
        "            spec = (spec - mean) / (std + 1e-6)\n",
        "\n",
        "        # SpecAugment (only on training)\n",
        "        if augment and cfg.use_spec_augment:\n",
        "            self._ensure_transforms()\n",
        "            x = spec\n",
        "            x = self._time_mask(x)\n",
        "            x = self._freq_mask(x)\n",
        "            spec = x\n",
        "\n",
        "        # Resize to square image for the NAS/CNN backbone\n",
        "        spec = spec.unsqueeze(0)  # (B=1, C=1, H, W)\n",
        "        spec = F.interpolate(spec, size=(img_size, img_size),\n",
        "                             mode=\"bilinear\", align_corners=False)\n",
        "        spec = spec.squeeze(0)    # (C=1, H, W)\n",
        "        return spec\n",
        "\n",
        "\n",
        "class AutoAudioPreprocessor:\n",
        "    \"\"\"\n",
        "    Wrapper that:\n",
        "    - profiles the dataset\n",
        "    - chooses sensible config values\n",
        "    - fits global normalization statistics\n",
        "    - exposes train/test transforms compatible with the existing pipeline\n",
        "    \"\"\"\n",
        "    def __init__(self, base_config: Optional[AudioPreprocessConfig] = None):\n",
        "        self.base_config = base_config or AudioPreprocessConfig()\n",
        "        self.config: Optional[AudioPreprocessConfig] = None\n",
        "        self.preproc: Optional[AudioPreprocessor] = None\n",
        "        self.profile: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    def _profile_dataset(self, dataset, max_items: int = 256) -> Dict[str, Any]:\n",
        "        sr_counts: Dict[int, int] = {}\n",
        "        durations = []\n",
        "        rms_values = []\n",
        "        files_per_class: Dict[str, int] = {}\n",
        "\n",
        "        n = len(dataset)\n",
        "        idxs = list(range(n))\n",
        "        random.shuffle(idxs)\n",
        "        idxs = idxs[:max_items]\n",
        "\n",
        "        for i in idxs:\n",
        "            waveform, sr, label, *_ = dataset[i]\n",
        "            if waveform.dim() == 1:\n",
        "                waveform = waveform.unsqueeze(0)\n",
        "            sr_counts[sr] = sr_counts.get(sr, 0) + 1\n",
        "            dur = waveform.shape[-1] / sr\n",
        "            durations.append(float(dur))\n",
        "            rms = float(torch.sqrt((waveform ** 2).mean()))\n",
        "            rms_values.append(rms)\n",
        "            files_per_class[label] = files_per_class.get(label, 0) + 1\n",
        "\n",
        "        return {\n",
        "            \"sr_counts\": sr_counts,\n",
        "            \"durations\": np.array(durations, dtype=np.float32),\n",
        "            \"rms_values\": np.array(rms_values, dtype=np.float32),\n",
        "            \"files_per_class\": files_per_class,\n",
        "            \"num_files\": n,\n",
        "        }\n",
        "\n",
        "    def _choose_config(self, img_size: int) -> AudioPreprocessConfig:\n",
        "        p = self.profile\n",
        "        cfg = self.base_config\n",
        "\n",
        "        # Target sample rate: most common in the dataset, fallback to 16 kHz\n",
        "        if p[\"sr_counts\"]:\n",
        "            cfg.target_sr = max(p[\"sr_counts\"].keys(), key=lambda s: p[\"sr_counts\"][s])\n",
        "        else:\n",
        "            cfg.target_sr = 16000\n",
        "\n",
        "        # Clip duration: 95th percentile of durations, clamped\n",
        "        if p[\"durations\"].size > 0:\n",
        "            p95 = float(np.percentile(p[\"durations\"], 95))\n",
        "            cfg.clip_seconds = float(max(0.5, min(5.0, p95)))\n",
        "        else:\n",
        "            cfg.clip_seconds = 1.0\n",
        "\n",
        "        # Global normalization: good when you have enough data\n",
        "        cfg.use_global_norm = p[\"num_files\"] > 500\n",
        "\n",
        "        # Augmentation strength based on per-class counts\n",
        "        if p[\"files_per_class\"]:\n",
        "            min_per_class = min(p[\"files_per_class\"].values())\n",
        "        else:\n",
        "            min_per_class = 0\n",
        "\n",
        "        # Base mask size roughly proportional to image size\n",
        "        base_mask = max(2, img_size // 10)\n",
        "        if min_per_class < 50:\n",
        "            # Fewer examples per class -> stronger augmentation\n",
        "            cfg.use_spec_augment = True\n",
        "            cfg.time_mask_param = base_mask * 2\n",
        "            cfg.freq_mask_param = base_mask * 2\n",
        "            cfg.num_time_masks = 2\n",
        "            cfg.num_freq_masks = 2\n",
        "        else:\n",
        "            cfg.use_spec_augment = True\n",
        "            cfg.time_mask_param = base_mask\n",
        "            cfg.freq_mask_param = base_mask\n",
        "            cfg.num_time_masks = 1\n",
        "            cfg.num_freq_masks = 1\n",
        "\n",
        "        self.config = cfg\n",
        "        return cfg\n",
        "\n",
        "    def fit(self, dataset, img_size: int) -> \"AutoAudioPreprocessor\":\n",
        "        # 1) Profile dataset\n",
        "        self.profile = self._profile_dataset(dataset)\n",
        "        # 2) Choose config based on profile + image size\n",
        "        cfg = self._choose_config(img_size)\n",
        "        # 3) Build preprocessor and fit its global normalization (if enabled)\n",
        "        self.preproc = AudioPreprocessor(cfg)\n",
        "        if cfg.use_global_norm:\n",
        "            self.preproc.fit(dataset)\n",
        "        return self\n",
        "\n",
        "    def make_transforms(self, img_size: int):\n",
        "        assert self.preproc is not None, \"Call fit(...) before make_transforms().\"\n",
        "\n",
        "        def train_tf(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
        "            return self.preproc.transform(waveform, sr, img_size=img_size, augment=True)\n",
        "\n",
        "        def test_tf(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
        "            return self.preproc.transform(waveform, sr, img_size=img_size, augment=False)\n",
        "\n",
        "        return train_tf, test_tf\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#   Dataset wrapper + dataloaders (API unchanged)\n",
        "# ============================================================\n",
        "\n",
        "class SpeechCommandsWrapped(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_ds, label2idx, tf):\n",
        "        self.ds = base_ds\n",
        "        self.label2idx = label2idx\n",
        "        self.tf = tf\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, sr, label, *_ = self.ds[idx]\n",
        "        x = self.tf(waveform, sr)\n",
        "        y = self.label2idx[label]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def _build_label_mapping(train_base):\n",
        "    labels = set()\n",
        "    for i in range(len(train_base)):\n",
        "        _, _, label, *_ = train_base[i]\n",
        "        labels.add(label)\n",
        "    labels = sorted(list(labels))\n",
        "    label2idx = {lab: i for i, lab in enumerate(labels)}\n",
        "    idx2label = {i: lab for lab, i in label2idx.items()}\n",
        "    return label2idx, idx2label\n",
        "\n",
        "\n",
        "def get_dataloaders(img_size: int):\n",
        "    \"\"\"\n",
        "    Public API is the same as in your original notebook:\n",
        "        train_loader, val_loader, test_loader = get_dataloaders(img_size)\n",
        "\n",
        "    Internally, we now:\n",
        "      - profile the audio\n",
        "      - auto-configure preprocessing\n",
        "      - learn global log-Mel normalization\n",
        "    \"\"\"\n",
        "    global NUM_CLASSES, TARGET_SR, CLIP_SECONDS, TARGET_SAMPLES\n",
        "\n",
        "    # Built-in SpeechCommands splits (unchanged)\n",
        "    train_base = torchaudio.datasets.SPEECHCOMMANDS(\n",
        "        root=DATA_ROOT, download=True, subset=\"training\"\n",
        "    )\n",
        "    val_base = torchaudio.datasets.SPEECHCOMMANDS(\n",
        "        root=DATA_ROOT, download=True, subset=\"validation\"\n",
        "    )\n",
        "    test_base = torchaudio.datasets.SPEECHCOMMANDS(\n",
        "        root=DATA_ROOT, download=True, subset=\"testing\"\n",
        "    )\n",
        "\n",
        "    # Label mapping based on training subset\n",
        "    label2idx, idx2label = _build_label_mapping(train_base)\n",
        "    NUM_CLASSES = len(label2idx)\n",
        "\n",
        "    # Automated audio preprocessing\n",
        "    auto_preproc = AutoAudioPreprocessor()\n",
        "    auto_preproc.fit(train_base, img_size=img_size)\n",
        "    # ðŸ‘‰ print report here\n",
        "    print_preprocessing_report(auto_preproc.config, auto_preproc.profile, img_size)\n",
        "\n",
        "    train_tf, test_tf = auto_preproc.make_transforms(img_size=img_size)\n",
        "\n",
        "    # Expose inferred low-level parameters if you still need them anywhere\n",
        "    TARGET_SR = auto_preproc.config.target_sr\n",
        "    CLIP_SECONDS = auto_preproc.config.clip_seconds\n",
        "    TARGET_SAMPLES = int(TARGET_SR * CLIP_SECONDS)\n",
        "\n",
        "    # Wrap datasets\n",
        "    train_set = SpeechCommandsWrapped(train_base, label2idx, train_tf)\n",
        "    val_set   = SpeechCommandsWrapped(val_base,   label2idx, test_tf)\n",
        "    test_set  = SpeechCommandsWrapped(test_base,  label2idx, test_tf)\n",
        "\n",
        "    # Dataloaders (same structure as before)\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_loader = DataLoader(\n",
        "        train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True,\n",
        "        worker_init_fn=_seed_worker, generator=g\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True,\n",
        "        worker_init_fn=_seed_worker, generator=g\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True,\n",
        "        worker_init_fn=_seed_worker, generator=g\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ywXJwo0SiOL",
        "outputId": "a0b5a2f3-6542-4c83-85b0-8ab1fa4bbea8"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchcodec\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Sweep configs\n",
        "SWEEP = [\n",
        "    {\"name\": \"dscnn_plus_full_res64\",         \"backbone\": \"dscnn_plus\",         \"unfreeze_blocks\": 999, \"img_size\": 64, \"lr\": 1.2e-3},\n",
        "    {\"name\": \"dscnn_plus_last3_res64\",        \"backbone\": \"dscnn_plus\",         \"unfreeze_blocks\": 3,   \"img_size\": 64, \"lr\": 1.5e-3},\n",
        "    {\"name\": \"mbv2_tiny_full_res64\",          \"backbone\": \"mobilenetv2_tiny\",   \"unfreeze_blocks\": 999, \"img_size\": 64, \"lr\": 1.0e-3},\n",
        "    {\"name\": \"mbv2_small_full_res64\",         \"backbone\": \"mobilenetv2_small\",  \"unfreeze_blocks\": 999, \"img_size\": 64, \"lr\": 9.0e-4},\n",
        "]\n",
        "\n",
        "EPOCHS_PER_TRIAL = 3\n",
        "FINAL_EPOCHS = 10\n",
        "\n",
        "LABEL_SMOOTHING = 0.08\n",
        "MIXUP_ALPHA = 0.35\n",
        "USE_MIXUP = True\n",
        "\n",
        "\n",
        "# Blocks / Models\n",
        "class DSConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, kernel_size=3, stride=stride, padding=1, groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.drop = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return self.drop(x)\n",
        "\n",
        "\n",
        "class DSCNNPlus(nn.Module):\n",
        "    def __init__(self, num_classes: int, drop=0.15):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 48, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DSConvBlock(48, 96,  stride=1, dropout=drop),\n",
        "            DSConvBlock(96, 192, stride=2, dropout=drop),\n",
        "            DSConvBlock(192, 192, stride=1, dropout=drop),\n",
        "            DSConvBlock(192, 256, stride=2, dropout=drop),\n",
        "            DSConvBlock(256, 256, stride=1, dropout=drop),\n",
        "        ])\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.head_drop = nn.Dropout(drop)\n",
        "        self.head = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.head_drop(x)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor=8):\n",
        "    return int(math.ceil(v / divisor) * divisor)\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride, expand_ratio, drop=0.0):\n",
        "        super().__init__()\n",
        "        assert stride in [1, 2]\n",
        "        hidden = int(round(in_ch * expand_ratio))\n",
        "        self.use_res = (stride == 1 and in_ch == out_ch)\n",
        "\n",
        "        layers = []\n",
        "        if expand_ratio != 1:\n",
        "            layers += [\n",
        "                nn.Conv2d(in_ch, hidden, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden),\n",
        "                nn.ReLU6(inplace=True),\n",
        "            ]\n",
        "        layers += [\n",
        "            nn.Conv2d(hidden, hidden, 3, stride=stride, padding=1, groups=hidden, bias=False),\n",
        "            nn.BatchNorm2d(hidden),\n",
        "            nn.ReLU6(inplace=True),\n",
        "            nn.Conv2d(hidden, out_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "        ]\n",
        "        self.block = nn.Sequential(*layers)\n",
        "        self.drop = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        out = self.drop(out)\n",
        "        if self.use_res:\n",
        "            return x + out\n",
        "        return out\n",
        "\n",
        "\n",
        "class MobileNetV2Spec(nn.Module):\n",
        "    def __init__(self, num_classes: int, width_mult=0.75, drop=0.10):\n",
        "        super().__init__()\n",
        "        cfg = [\n",
        "            (1,  16, 1, 1),\n",
        "            (6,  24, 2, 2),\n",
        "            (6,  32, 3, 2),\n",
        "            (6,  64, 3, 2),\n",
        "            (6,  96, 2, 1),\n",
        "            (6, 160, 1, 2),\n",
        "        ]\n",
        "\n",
        "        in_ch = _make_divisible(32 * width_mult)\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, in_ch, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_ch),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        blocks = []\n",
        "        for t, c, n, s in cfg:\n",
        "            out_ch = _make_divisible(c * width_mult)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                blocks.append(InvertedResidual(in_ch, out_ch, stride=stride, expand_ratio=t, drop=drop))\n",
        "                in_ch = out_ch\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        last_ch = _make_divisible(192 * width_mult)\n",
        "        self.last = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, last_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(last_ch),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.head_drop = nn.Dropout(drop)\n",
        "        self.head = nn.Linear(last_ch, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "        x = self.last(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.head_drop(x)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "def build_backbone(backbone: str, num_classes: int):\n",
        "    if backbone == \"dscnn_plus\":\n",
        "        return DSCNNPlus(num_classes=num_classes, drop=0.15)\n",
        "    elif backbone == \"mobilenetv2_tiny\":\n",
        "        # Very close to DSCNN speed; slightly different capacity/bias\n",
        "        return MobileNetV2Spec(num_classes=num_classes, width_mult=0.50, drop=0.10)\n",
        "    elif backbone == \"mobilenetv2_small\":\n",
        "        # Still fast, generally stronger than tiny\n",
        "        return MobileNetV2Spec(num_classes=num_classes, width_mult=0.75, drop=0.10)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
        "\n",
        "def unfreeze_module(m: nn.Module):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def freeze_module(m: nn.Module):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def get_head_module(model: nn.Module) -> nn.Module:\n",
        "    if hasattr(model, \"head\"):\n",
        "        return model.head\n",
        "    raise ValueError(\"Could not find classifier head (expected .head)\")\n",
        "\n",
        "def get_block_list(backbone: str, model: nn.Module):\n",
        "    if hasattr(model, \"blocks\"):\n",
        "        return list(model.blocks)\n",
        "    raise ValueError(f\"Unsupported backbone for block unfreezing: {backbone}\")\n",
        "\n",
        "def freeze_all_but_head(model: nn.Module):\n",
        "    freeze_module(model)\n",
        "    unfreeze_module(get_head_module(model))\n",
        "\n",
        "def unfreeze_last_n_blocks(model: nn.Module, backbone: str, n_blocks: int):\n",
        "    blocks = get_block_list(backbone, model)\n",
        "    if n_blocks >= 999 or n_blocks >= len(blocks):\n",
        "        unfreeze_module(model)\n",
        "        return\n",
        "    freeze_all_but_head(model)\n",
        "    for b in blocks[-n_blocks:]:\n",
        "        unfreeze_module(b)\n",
        "\n",
        "def smooth_one_hot(y: torch.Tensor, num_classes: int, smoothing: float):\n",
        "    with torch.no_grad():\n",
        "        y_oh = torch.zeros((y.size(0), num_classes), device=y.device, dtype=torch.float32)\n",
        "        y_oh.scatter_(1, y.unsqueeze(1), 1.0)\n",
        "        if smoothing > 0:\n",
        "            y_oh = y_oh * (1.0 - smoothing) + smoothing / num_classes\n",
        "    return y_oh\n",
        "\n",
        "def mixup_batch(x, y, alpha: float, num_classes: int, smoothing: float):\n",
        "    if alpha <= 0:\n",
        "        return x, smooth_one_hot(y, num_classes, smoothing)\n",
        "\n",
        "    lam = torch.distributions.Beta(alpha, alpha).sample((x.size(0),)).to(x.device)\n",
        "    lam = torch.maximum(lam, 1.0 - lam)\n",
        "    lam_x = lam.view(-1, 1, 1, 1)\n",
        "\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    x2 = x[idx]\n",
        "    y2 = y[idx]\n",
        "\n",
        "    y1_sm = smooth_one_hot(y,  num_classes, smoothing)\n",
        "    y2_sm = smooth_one_hot(y2, num_classes, smoothing)\n",
        "\n",
        "    x_mix = x * lam_x + x2 * (1.0 - lam_x)\n",
        "    lam_y = lam.view(-1, 1)\n",
        "    y_mix = y1_sm * lam_y + y2_sm * (1.0 - lam_y)\n",
        "    return x_mix, y_mix\n",
        "\n",
        "def soft_target_ce(logits: torch.Tensor, soft_targets: torch.Tensor):\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "    return -(soft_targets * log_probs).sum(dim=1).mean()\n",
        "\n",
        "\n",
        "# Optimizer / scheduler\n",
        "def make_optimizer(model, lr: float):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    return optim.AdamW(params, lr=lr, weight_decay=1e-4)\n",
        "\n",
        "def make_scheduler(opt, total_steps: int, warmup_steps: int):\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "def count_params(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Train / eval utils\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            y_sm = smooth_one_hot(y, NUM_CLASSES, LABEL_SMOOTHING)\n",
        "            loss = soft_target_ce(logits, y_sm)\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    return total_loss / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scheduler, scaler=None):\n",
        "    model.train()\n",
        "    correct, total = 0, 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if USE_MIXUP:\n",
        "            x, y_soft = mixup_batch(x, y, MIXUP_ALPHA, NUM_CLASSES, LABEL_SMOOTHING)\n",
        "        else:\n",
        "            y_soft = smooth_one_hot(y, NUM_CLASSES, LABEL_SMOOTHING)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        use_amp = (device.type == \"cuda\")\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
        "            logits = model(x)\n",
        "            loss = soft_target_ce(logits, y_soft)\n",
        "\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    return total_loss / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "best_cfg = None\n",
        "best_val_acc = -1.0\n",
        "\n",
        "for cfg in SWEEP:\n",
        "    t0 = time.time()\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(cfg[\"img_size\"])\n",
        "\n",
        "    model = build_backbone(cfg[\"backbone\"], num_classes=NUM_CLASSES).to(device)\n",
        "    unfreeze_last_n_blocks(model, backbone=cfg[\"backbone\"], n_blocks=cfg[\"unfreeze_blocks\"])\n",
        "\n",
        "    opt = make_optimizer(model, lr=cfg[\"lr\"])\n",
        "\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    total_steps = steps_per_epoch * EPOCHS_PER_TRIAL\n",
        "    warmup_steps = max(20, int(0.05 * total_steps))\n",
        "    sched = make_scheduler(opt, total_steps=total_steps, warmup_steps=warmup_steps)\n",
        "\n",
        "    scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "    for epoch in range(1, EPOCHS_PER_TRIAL + 1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, opt, sched, scaler=scaler)\n",
        "        va_loss, va_acc = evaluate(model, val_loader)\n",
        "        print(f\"[{cfg['name']}] epoch {epoch}/{EPOCHS_PER_TRIAL} | tr_acc={tr_acc:.4f} | va_acc={va_acc:.4f}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    total_p, train_p = count_params(model)\n",
        "    print(f\"[{cfg['name']}] done in {elapsed:.1f}s | params={total_p/1e6:.2f}M (trainable {train_p/1e6:.2f}M) | val_acc={va_acc:.4f}\\n\")\n",
        "\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        best_cfg = cfg\n",
        "\n",
        "print(\"Best cfg:\", best_cfg)\n",
        "print(\"Best val acc:\", best_val_acc)\n",
        "\n",
        "# Final train with best cfg\n",
        "train_loader, val_loader, test_loader = get_dataloaders(best_cfg[\"img_size\"])\n",
        "best_model = build_backbone(best_cfg[\"backbone\"], num_classes=NUM_CLASSES).to(device)\n",
        "unfreeze_last_n_blocks(best_model, backbone=best_cfg[\"backbone\"], n_blocks=best_cfg[\"unfreeze_blocks\"])\n",
        "\n",
        "optimizer = make_optimizer(best_model, lr=best_cfg[\"lr\"])\n",
        "\n",
        "steps_per_epoch = len(train_loader)\n",
        "total_steps = steps_per_epoch * FINAL_EPOCHS\n",
        "warmup_steps = max(50, int(0.08 * total_steps))\n",
        "scheduler = make_scheduler(optimizer, total_steps=total_steps, warmup_steps=warmup_steps)\n",
        "\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "best_val = 0.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, FINAL_EPOCHS + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(best_model, train_loader, optimizer, scheduler, scaler=scaler)\n",
        "    va_loss, va_acc = evaluate(best_model, val_loader)\n",
        "\n",
        "    print(f\"[FINAL] epoch {epoch}/{FINAL_EPOCHS} | tr_acc={tr_acc:.4f} | va_acc={va_acc:.4f}\")\n",
        "\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in best_model.state_dict().items()}\n",
        "\n",
        "if best_state is not None:\n",
        "    best_model.load_state_dict(best_state)\n",
        "\n",
        "test_loss, final_test_acc = evaluate(best_model, test_loader)\n",
        "print(f\"Final test accuracy: {final_test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def _evaluate_acc(model, loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    want_half = False\n",
        "    try:\n",
        "        p0 = next(model.parameters())\n",
        "        want_half = (p0.dtype == torch.float16)\n",
        "    except StopIteration:\n",
        "        pass\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        if want_half:\n",
        "            x = x.half()\n",
        "        logits = model(x)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return float(correct / max(1, total))\n",
        "\n",
        "def _state_dict_size_mb(model, tmp_path=\"__tmp_size.pth\"):\n",
        "    try:\n",
        "        torch.save(model.state_dict(), tmp_path)\n",
        "        return os.path.getsize(tmp_path) / (1024 * 1024)\n",
        "    finally:\n",
        "        try:\n",
        "            if os.path.exists(tmp_path):\n",
        "                os.remove(tmp_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "@torch.no_grad()\n",
        "def _probe_forward(model, loader, device):\n",
        "    model.eval()\n",
        "    try:\n",
        "        x, _ = next(iter(loader))\n",
        "        x = x[:1].to(device)\n",
        "\n",
        "        # Match dtype if fp16 model\n",
        "        try:\n",
        "            p0 = next(model.parameters())\n",
        "            if p0.dtype == torch.float16:\n",
        "                x = x.half()\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "        _ = model(x)\n",
        "        return True, \"\"\n",
        "    except Exception as e:\n",
        "        return False, f\"{type(e).__name__}: {e}\"\n",
        "\n",
        "def quantize_model(best_model, device, test_loader, max_abs_drop=0.01, min_size_gain=0.05):\n",
        "    report = {\"fp32\": {}, \"fp16\": {}, \"selected\": None}\n",
        "\n",
        "    # ---- baseline fp32 ----\n",
        "    fp32 = best_model.to(device).eval()\n",
        "    ok, reason = _probe_forward(fp32, test_loader, device)\n",
        "    if not ok:\n",
        "        raise RuntimeError(f\"FP32 probe failed (unexpected): {reason}\")\n",
        "\n",
        "    fp32_acc  = _evaluate_acc(fp32, test_loader, device)\n",
        "    fp32_size = _state_dict_size_mb(fp32)\n",
        "    report[\"fp32\"] = {\"supported\": True, \"acc\": fp32_acc, \"size_mb\": fp32_size}\n",
        "\n",
        "    # ---- candidate fp16 ----\n",
        "    fp16_supported = False\n",
        "    fp16_acc = None\n",
        "    fp16_size = None\n",
        "    fp16_model = None\n",
        "    fp16_reason = \"\"\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        fp16_model = copy.deepcopy(fp32).to(device).half().eval()\n",
        "        ok, reason = _probe_forward(fp16_model, test_loader, device)\n",
        "        fp16_supported = ok\n",
        "        fp16_reason = reason\n",
        "\n",
        "        if ok:\n",
        "            fp16_acc  = _evaluate_acc(fp16_model, test_loader, device)\n",
        "            fp16_size = _state_dict_size_mb(fp16_model)\n",
        "\n",
        "    report[\"fp16\"] = {\n",
        "        \"supported\": bool(fp16_supported),\n",
        "        \"reason\": \"\" if fp16_supported else fp16_reason,\n",
        "        \"acc\": fp16_acc,\n",
        "        \"size_mb\": fp16_size,\n",
        "    }\n",
        "\n",
        "    selected_name = \"fp32\"\n",
        "    selected_model = fp32\n",
        "\n",
        "    if fp16_supported:\n",
        "        abs_drop = fp32_acc - fp16_acc\n",
        "        size_gain = (fp32_size - fp16_size) / max(1e-9, fp32_size)\n",
        "        if (abs_drop <= max_abs_drop) and (size_gain >= min_size_gain):\n",
        "            selected_name = \"fp16\"\n",
        "            selected_model = fp16_model\n",
        "\n",
        "    report[\"selected\"] = {\n",
        "        \"name\": selected_name,\n",
        "        \"baseline_acc\": fp32_acc,\n",
        "        \"baseline_size_mb\": fp32_size,\n",
        "        \"chosen_acc\": _evaluate_acc(selected_model, test_loader, device),\n",
        "        \"chosen_size_mb\": _state_dict_size_mb(selected_model),\n",
        "        \"max_abs_drop\": float(max_abs_drop),\n",
        "        \"min_size_gain\": float(min_size_gain),\n",
        "    }\n",
        "\n",
        "    return selected_model, report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimized_best_model, opt_report = quantize_model(\n",
        "    best_model=best_model,\n",
        "    device=device,\n",
        "    test_loader=test_loader,\n",
        "    max_abs_drop=0.01,\n",
        "    min_size_gain=0.05\n",
        ")\n",
        "\n",
        "print(\"Optimization report:\", opt_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "save_path = \"best_model.pth\"\n",
        "torch.save(best_model.state_dict(), save_path)\n",
        "\n",
        "size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
        "\n",
        "print(\"Saved:\", save_path)\n",
        "print(f\"Best model test accuracy: {final_test_acc:.4f}\")\n",
        "print(f\"Saved .pth size: {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "optimized_test_acc = _evaluate_acc(optimized_best_model, test_loader, device)\n",
        "\n",
        "save_path = \"optimized_best_model.pth\"\n",
        "torch.save(optimized_best_model.state_dict(), save_path)\n",
        "size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
        "\n",
        "print(\"Saved:\", save_path)\n",
        "print(f\"Optimized selected={opt_report['selected']['name']} test accuracy: {optimized_test_acc:.4f}\")\n",
        "print(f\"Saved .pth size: {size_mb:.2f} MB\")\n",
        "\n",
        "if \"final_test_acc\" in globals():\n",
        "    delta = optimized_test_acc - final_test_acc\n",
        "    print(f\"Accuracy vs FP32 best: {delta:+.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
