{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmnzCuhi6_Pl"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# IMAGE PREPROCESSING PDF REPORT (Strategy B) — Improved Charts + Simple Aug Modes + Expanded Steps\n",
        "# =========================\n",
        "\n",
        "import os, tempfile, datetime\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "from reportlab.platypus import (\n",
        "    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak,\n",
        "    Image as RLImage\n",
        ")\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.units import cm\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Styles (aligned with your sensor report look)\n",
        "# -------------------------\n",
        "styles = getSampleStyleSheet()\n",
        "\n",
        "H1 = ParagraphStyle(\"H1\", parent=styles[\"Heading1\"], alignment=1, spaceAfter=10)\n",
        "H2 = ParagraphStyle(\"H2\", parent=styles[\"Heading2\"], spaceBefore=6, spaceAfter=8)\n",
        "body = ParagraphStyle(\"body\", parent=styles[\"BodyText\"], spaceAfter=6, leading=13)\n",
        "small = ParagraphStyle(\"small\", parent=styles[\"BodyText\"], fontSize=9, leading=11, spaceAfter=6)\n",
        "caption = ParagraphStyle(\"cap\", parent=styles[\"BodyText\"], fontSize=9, leading=11, alignment=1, spaceAfter=10)\n",
        "\n",
        "cover_title = ParagraphStyle(\"cover_title\", parent=styles[\"Title\"], alignment=1, fontSize=24, spaceAfter=18, leading=30)\n",
        "cover_subtitle = ParagraphStyle(\"cover_subtitle\", parent=styles[\"Heading2\"], alignment=1, fontSize=16, spaceAfter=12, leading=20)\n",
        "cover_meta = ParagraphStyle(\"cover_meta\", parent=styles[\"Normal\"], alignment=1, fontSize=10, textColor=colors.gray, spaceAfter=24)\n",
        "cover_desc = ParagraphStyle(\"cover_desc\", parent=styles[\"Normal\"], alignment=1, fontSize=12, leading=16, spaceAfter=0)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Plot helpers (publication-friendly)\n",
        "# -------------------------\n",
        "def _set_pub_rcparams():\n",
        "    plt.rcParams.update({\n",
        "        \"font.size\": 9,\n",
        "        \"axes.titlesize\": 11,\n",
        "        \"axes.labelsize\": 9,\n",
        "        \"xtick.labelsize\": 8,\n",
        "        \"ytick.labelsize\": 8,\n",
        "        \"legend.fontsize\": 8,\n",
        "        \"figure.dpi\": 150,\n",
        "        \"savefig.dpi\": 300,\n",
        "        \"axes.linewidth\": 0.8,\n",
        "    })\n",
        "_set_pub_rcparams()\n",
        "\n",
        "def _style_axes(ax, grid_axis=\"y\"):\n",
        "    ax.set_axisbelow(True)\n",
        "    if grid_axis in (\"y\", \"both\"):\n",
        "        ax.grid(True, axis=\"y\", linestyle=\"--\", linewidth=0.6, alpha=0.35)\n",
        "    if grid_axis in (\"x\", \"both\"):\n",
        "        ax.grid(True, axis=\"x\", linestyle=\"--\", linewidth=0.6, alpha=0.25)\n",
        "    for s in (\"top\", \"right\"):\n",
        "        ax.spines[s].set_visible(False)\n",
        "    for s in (\"left\", \"bottom\"):\n",
        "        ax.spines[s].set_linewidth(0.8)\n",
        "        ax.spines[s].set_alpha(0.7)\n",
        "    ax.tick_params(axis=\"both\", which=\"both\", length=3, width=0.8)\n",
        "    return ax\n",
        "\n",
        "def _save_fig_to_png(fig) -> str:\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "    tmp.close()\n",
        "    fig.savefig(tmp.name, dpi=300, bbox_inches=\"tight\", pad_inches=0.06, facecolor=\"white\")\n",
        "    plt.close(fig)\n",
        "    return tmp.name\n",
        "\n",
        "def _fit_rl_image(img_path: str, max_w: float, max_h: float) -> RLImage:\n",
        "    im = PILImage.open(img_path)\n",
        "    w, h = im.size\n",
        "    im.close()\n",
        "    scale = min(max_w / float(w), max_h / float(h))\n",
        "    return RLImage(img_path, width=w * scale, height=h * scale)\n",
        "\n",
        "def _convert_logo_to_png(logo_path: str) -> str | None:\n",
        "    try:\n",
        "        img = PILImage.open(logo_path).convert(\"RGBA\")\n",
        "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "        tmp.close()\n",
        "        img.save(tmp.name, format=\"PNG\")\n",
        "        return tmp.name\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Chart: improved class distribution (clean labels + stable ordering)\n",
        "# -------------------------\n",
        "def _chart_class_counts(class_counts: dict) -> str | None:\n",
        "    if not class_counts:\n",
        "        return None\n",
        "\n",
        "    # Stable ordering:\n",
        "    # - If keys look numeric (\"0\",\"1\",...), sort numerically; else alphabetically\n",
        "    keys = list(class_counts.keys())\n",
        "    def _is_intlike(s):\n",
        "        try:\n",
        "            int(str(s))\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    if all(_is_intlike(k) for k in keys):\n",
        "        items = sorted(class_counts.items(), key=lambda x: int(str(x[0])))\n",
        "    else:\n",
        "        items = sorted(class_counts.items(), key=lambda x: str(x[0]).lower())\n",
        "\n",
        "    labels = [str(k) for k, _ in items]\n",
        "    values = [int(v) for _, v in items]\n",
        "    total = sum(values)\n",
        "    if total <= 0:\n",
        "        return None\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7.2, 3.8))\n",
        "    x = np.arange(len(labels))\n",
        "    bars = ax.bar(x, values, width=0.65)\n",
        "\n",
        "    ax.set_title(\"Class distribution\")\n",
        "    ax.set_ylabel(\"Images\")\n",
        "    ax.set_xticks(x)\n",
        "\n",
        "    # rotate only if labels are long\n",
        "    max_len = max(len(l) for l in labels) if labels else 0\n",
        "    rot = 0 if max_len <= 10 and len(labels) <= 12 else 30\n",
        "    ax.set_xticklabels(labels, rotation=rot, ha=\"right\" if rot else \"center\")\n",
        "\n",
        "    _style_axes(ax, grid_axis=\"y\")\n",
        "\n",
        "    # Clean bar labels: count + percent\n",
        "    try:\n",
        "        ax.bar_label(\n",
        "            bars,\n",
        "            labels=[f\"{v} ({(v/total)*100:.1f}%)\" for v in values],\n",
        "            padding=3,\n",
        "            fontsize=8\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return _save_fig_to_png(fig)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Resolution sample + smarter resolution chart\n",
        "# - If constant resolution -> skip chart (and we'll print a sentence instead)\n",
        "# - Else -> scatter of (W,H)\n",
        "# -------------------------\n",
        "def _sample_resolution_pairs_for_report(train_loader, max_images=256):\n",
        "    \"\"\"\n",
        "    Extract approximate original sizes is hard after transforms.\n",
        "    Instead, we visualize post-transform tensor shape, and report\n",
        "    precomputed dataset stats (if available). Here we only add a small\n",
        "    'post-transform' confirmation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        xb, _ = next(iter(train_loader))\n",
        "        # xb shape: [B,C,H,W]\n",
        "        h = int(xb.shape[2])\n",
        "        w = int(xb.shape[3])\n",
        "        return {\"post_transform_shape\": f\"{w}×{h}\"}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "\n",
        "def _chart_resolution_scatter(res_stats: dict) -> str | None:\n",
        "    \"\"\"\n",
        "    Expects res_stats to include either:\n",
        "      - sample_pairs: list[(w,h)]  (best)\n",
        "    Otherwise, will skip.\n",
        "    \"\"\"\n",
        "    pairs = res_stats.get(\"sample_pairs\", None)\n",
        "    if not pairs:\n",
        "        return None\n",
        "\n",
        "    ws = np.array([p[0] for p in pairs], dtype=float)\n",
        "    hs = np.array([p[1] for p in pairs], dtype=float)\n",
        "\n",
        "    # If constant resolution, skip\n",
        "    if ws.min() == ws.max() and hs.min() == hs.max():\n",
        "        return None\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6.9, 4.0))\n",
        "    ax.scatter(ws, hs, s=14, alpha=0.6)\n",
        "    ax.set_title(\"Resolution scatter (sampled)\")\n",
        "    ax.set_xlabel(\"Width (px)\")\n",
        "    ax.set_ylabel(\"Height (px)\")\n",
        "    _style_axes(ax, grid_axis=\"both\")\n",
        "\n",
        "    # Put mean marker\n",
        "    try:\n",
        "        ax.scatter([ws.mean()], [hs.mean()], s=60, marker=\"x\")\n",
        "        ax.text(ws.mean(), hs.mean(), f\"  mean≈{ws.mean():.0f}×{hs.mean():.0f}\", va=\"center\", fontsize=8)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return _save_fig_to_png(fig)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Visual grid of augmented samples (train_loader already includes Strategy-B aug)\n",
        "# -------------------------\n",
        "def _make_aug_grid_png(train_loader, max_images=12) -> str | None:\n",
        "    try:\n",
        "        xb, yb = next(iter(train_loader))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    # Unnormalize for display (ImageNet normalization used in your transforms)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
        "    std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
        "\n",
        "    x = xb[:max_images].detach().cpu()\n",
        "    if x.shape[1] == 3:\n",
        "        x = x * std + mean\n",
        "    x = x.clamp(0, 1)\n",
        "\n",
        "    n = x.size(0)\n",
        "    cols = 6\n",
        "    rows = int(np.ceil(n / cols))\n",
        "\n",
        "    fig = plt.figure(figsize=(cols*2.05, rows*2.05))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(rows, cols, i+1)\n",
        "        img = x[i].permute(1,2,0).numpy()\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(str(int(yb[i])), fontsize=9)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    fig.suptitle(\"Augmented training samples (Strategy B)\", fontsize=12)\n",
        "    fig.tight_layout()\n",
        "    return _save_fig_to_png(fig)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Augmentation explanation (simple, report-friendly)\n",
        "# -------------------------\n",
        "def _augmentation_explanation_paragraphs(strategy_b: dict) -> list[str]:\n",
        "    \"\"\"\n",
        "    Returns short, simple explanation paragraphs for all modes + highlight chosen mode.\n",
        "    \"\"\"\n",
        "    chosen_family = str(strategy_b.get(\"family\", \"\")).lower()\n",
        "    chosen_level = strategy_b.get(\"level\", \"\")\n",
        "\n",
        "    # Simple mode explanations (always included, like a mini glossary)\n",
        "    mode_text = []\n",
        "\n",
        "    mode_text.append(\n",
        "        \"<b>Augmentation modes (simple explanation):</b> \"\n",
        "        \"Augmentations apply random, label-preserving changes to training images to reduce overfitting. \"\n",
        "        \"They are <b>never</b> applied to validation/test to keep evaluation fair.\"\n",
        "    )\n",
        "    mode_text.append(\n",
        "        \"<b>• None:</b> No random changes. Useful when data is abundant or when you want maximum determinism, \"\n",
        "        \"but it can overfit on small datasets.\"\n",
        "    )\n",
        "    mode_text.append(\n",
        "        \"<b>• Basic:</b> A light set of common transforms (typically horizontal flips and small random crops). \"\n",
        "        \"Think “minor camera framing changes”.\"\n",
        "    )\n",
        "    mode_text.append(\n",
        "        \"<b>• TrivialAugmentWide:</b> Applies <b>one</b> randomly chosen transform (e.g., rotate, brightness, contrast) \"\n",
        "        \"with random strength. Strong regularization without any tuning.\"\n",
        "    )\n",
        "    mode_text.append(\n",
        "        \"<b>• RandAugment:</b> Applies <b>N</b> random transforms sequentially, all with a shared strength <b>M</b>. \"\n",
        "        \"This is stronger than Basic, but still cheap because it does not perform any search.\"\n",
        "    )\n",
        "\n",
        "    # Highlight chosen mode\n",
        "    if chosen_family == \"randaugment\":\n",
        "        mode_text.append(\n",
        "            f\"<b>Chosen in this run:</b> <b>RandAugment</b> (level=<b>{chosen_level}</b>, \"\n",
        "            f\"N=<b>{strategy_b.get('randaugment_N')}</b>, M=<b>{strategy_b.get('randaugment_M')}</b>).\"\n",
        "        )\n",
        "    elif chosen_family == \"trivialaugment\":\n",
        "        mode_text.append(\n",
        "            f\"<b>Chosen in this run:</b> <b>TrivialAugmentWide</b> (level=<b>{chosen_level}</b>).\"\n",
        "        )\n",
        "    elif chosen_family == \"basic\":\n",
        "        mode_text.append(\n",
        "            f\"<b>Chosen in this run:</b> <b>Basic</b> (level=<b>{chosen_level}</b>).\"\n",
        "        )\n",
        "    else:\n",
        "        mode_text.append(\n",
        "            f\"<b>Chosen in this run:</b> <b>{strategy_b.get('family','')}</b> (level=<b>{chosen_level}</b>).\"\n",
        "        )\n",
        "\n",
        "    return mode_text\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main: generate report\n",
        "# -------------------------\n",
        "def generate_image_preprocessing_report(\n",
        "    report: dict,\n",
        "    train_loader=None,\n",
        "    path: str = \"image_prep_report.pdf\",\n",
        "    project_name: str = \"Automata AI - Image Preprocessing Report\",\n",
        "    logo_path: str | None = None\n",
        "):\n",
        "    doc = SimpleDocTemplate(\n",
        "        path,\n",
        "        pagesize=A4,\n",
        "        rightMargin=2*cm,\n",
        "        leftMargin=2*cm,\n",
        "        topMargin=3.1*cm,\n",
        "        bottomMargin=2.0*cm\n",
        "    )\n",
        "\n",
        "    story = []\n",
        "    tmp_files = []\n",
        "\n",
        "    # Logo handling\n",
        "    logo_png = None\n",
        "    if logo_path and os.path.exists(logo_path):\n",
        "        logo_png = _convert_logo_to_png(logo_path)\n",
        "        if logo_png:\n",
        "            tmp_files.append(logo_png)\n",
        "\n",
        "    def draw_header_footer(canvas, doc_):\n",
        "        canvas.saveState()\n",
        "        header_top = doc_.pagesize[1] - 1.0*cm\n",
        "        header_bottom = doc_.pagesize[1] - doc_.topMargin + 0.25*cm\n",
        "\n",
        "        if logo_png:\n",
        "            lw, lh = (1.25*cm, 1.25*cm)\n",
        "            x = doc_.leftMargin\n",
        "            y = header_top - lh\n",
        "            canvas.drawImage(logo_png, x, y, width=lw, height=lh, preserveAspectRatio=True, mask=\"auto\")\n",
        "\n",
        "        canvas.setFont(\"Helvetica-Bold\", 10)\n",
        "        canvas.drawRightString(\n",
        "            doc_.pagesize[0] - doc_.rightMargin,\n",
        "            doc_.pagesize[1] - 1.35*cm,\n",
        "            project_name\n",
        "        )\n",
        "\n",
        "        canvas.setLineWidth(0.4)\n",
        "        canvas.setStrokeColor(colors.grey)\n",
        "        canvas.line(doc_.leftMargin, header_bottom, doc_.pagesize[0] - doc_.rightMargin, header_bottom)\n",
        "\n",
        "        canvas.setFont(\"Helvetica\", 8)\n",
        "        canvas.setFillColor(colors.black)\n",
        "        canvas.drawString(doc_.leftMargin, 1.15*cm, f\"Page {doc_.page}\")\n",
        "        canvas.drawRightString(\n",
        "            doc_.pagesize[0] - doc_.rightMargin,\n",
        "            1.15*cm,\n",
        "            f\"© {datetime.datetime.now().year} Automata AI — All rights reserved\"\n",
        "        )\n",
        "        canvas.restoreState()\n",
        "\n",
        "    # -------------------------\n",
        "    # Cover page\n",
        "    # -------------------------\n",
        "    story.append(Spacer(1, 2.6 * cm))\n",
        "\n",
        "    if logo_png:\n",
        "        cover_logo = _fit_rl_image(logo_png, max_w=6.5*cm, max_h=6.5*cm)\n",
        "        cover_logo.hAlign = \"CENTER\"\n",
        "        story.append(cover_logo)\n",
        "\n",
        "    story.append(Spacer(1, 1.2 * cm))\n",
        "    story.append(Paragraph(project_name, cover_title))\n",
        "    story.append(Paragraph(\"Automated Preprocessing Report (Image Modality)\", cover_subtitle))\n",
        "    story.append(Paragraph(f\"Generated on {report.get('timestamp','')}\", cover_meta))\n",
        "    story.append(Spacer(1, 1.2 * cm))\n",
        "    story.append(Paragraph(\n",
        "        \"This report summarizes dataset characteristics, preprocessing decisions, and the resulting training-ready loaders for image classification.\",\n",
        "        cover_desc\n",
        "    ))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # -------------------------\n",
        "    # 1) Dataset Overview\n",
        "    # -------------------------\n",
        "    story.append(Paragraph(\"1. Dataset Overview\", H2))\n",
        "\n",
        "    class_counts = report.get(\"class_counts\", {}) or {}\n",
        "    counts_only = list(class_counts.values()) if class_counts else []\n",
        "    imbalance_ratio = report.get(\"imbalance_ratio\", None)\n",
        "    if imbalance_ratio is None and counts_only:\n",
        "        imbalance_ratio = (max(counts_only) / max(1, min(counts_only)))\n",
        "\n",
        "    res = report.get(\"resolution_stats\", {}) or {}\n",
        "    sb = report.get(\"strategy_b\", {}) or {}\n",
        "    splits = report.get(\"splits\", {}) or {}\n",
        "    loader_cfg = report.get(\"loader\", {}) or {}\n",
        "\n",
        "    # For readability\n",
        "    dataset_mode = report.get(\"dataset_mode\", \"\")\n",
        "    data_root = report.get(\"data_root\", \"\")\n",
        "    n_images = report.get(\"num_images\", \"\")\n",
        "    n_classes = report.get(\"num_classes\", \"\")\n",
        "\n",
        "    overview_rows = [\n",
        "        [\"Dataset mode\", str(dataset_mode)],\n",
        "        [\"Data root\", str(data_root)],\n",
        "        [\"# Images\", str(n_images)],\n",
        "        [\"# Classes\", str(n_classes)],\n",
        "        [\"Imbalance ratio\", f\"{float(imbalance_ratio):.3f}\" if imbalance_ratio is not None else \"\"],\n",
        "        [\"Chosen img_size\", str(report.get(\"img_size\", \"\"))],\n",
        "        [\"Normalization\", str(report.get(\"normalization\", \"\"))],\n",
        "        [\"Batch size\", str(loader_cfg.get(\"batch_size\", \"\"))],\n",
        "        [\"Num workers\", str(loader_cfg.get(\"num_workers\", \"\"))],\n",
        "        [\"Seed\", str(splits.get(\"seed\", report.get(\"seed\", \"\")))],\n",
        "    ]\n",
        "\n",
        "    # Add resolution stats if available\n",
        "    if \"width_mean\" in res:\n",
        "        overview_rows += [\n",
        "            [\"Mean resolution (sampled)\", f\"{res.get('width_mean',0):.1f} × {res.get('height_mean',0):.1f}\"],\n",
        "            [\"Median resolution (sampled)\", f\"{res.get('width_median',0):.1f} × {res.get('height_median',0):.1f}\"],\n",
        "            [\"Resolution min/max (sampled)\", f\"{res.get('width_min','?')}–{res.get('width_max','?')} × {res.get('height_min','?')}–{res.get('height_max','?')}\"],\n",
        "            [\"Unreadable in scan\", f\"{res.get('resolution_bad_count',0)} / {res.get('resolution_scan_limit',0)}\"],\n",
        "        ]\n",
        "\n",
        "    # Post-transform confirmation (optional)\n",
        "    if train_loader is not None:\n",
        "        post_shape = _sample_resolution_pairs_for_report(train_loader)\n",
        "        if post_shape.get(\"post_transform_shape\"):\n",
        "            overview_rows.append([\"Post-transform tensor shape\", post_shape[\"post_transform_shape\"]])\n",
        "\n",
        "    t = Table(overview_rows, colWidths=[7.5*cm, 8.5*cm])\n",
        "    t.setStyle(TableStyle([\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.3, colors.grey),\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.whitesmoke),\n",
        "        (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
        "        (\"ROWBACKGROUNDS\", (0,1), (-1,-1), [colors.white, colors.Color(0.97,0.97,0.97)]),\n",
        "        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
        "        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
        "        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n",
        "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n",
        "    ]))\n",
        "    story.append(t)\n",
        "    story.append(Spacer(1, 0.4*cm))\n",
        "\n",
        "    # Charts (better)\n",
        "    chart_paths = []\n",
        "\n",
        "    p1 = _chart_class_counts(class_counts)\n",
        "    if p1:\n",
        "        chart_paths.append((\"Target class distribution\", p1))\n",
        "\n",
        "    # Optional resolution scatter if sample_pairs exists\n",
        "    p2 = _chart_resolution_scatter(res)\n",
        "    if p2:\n",
        "        chart_paths.append((\"Resolution scatter (sampled)\", p2))\n",
        "\n",
        "    for title, p in chart_paths:\n",
        "        tmp_files.append(p)\n",
        "        story.append(_fit_rl_image(p, max_w=doc.width, max_h=8.2*cm))\n",
        "        story.append(Paragraph(title, caption))\n",
        "\n",
        "    # If resolution chart skipped because constant, add a smart note\n",
        "    if \"width_min\" in res and \"width_max\" in res and \"height_min\" in res and \"height_max\" in res:\n",
        "        if res[\"width_min\"] == res[\"width_max\"] and res[\"height_min\"] == res[\"height_max\"]:\n",
        "            story.append(Paragraph(\n",
        "                f\"All sampled images share the same resolution: <b>{res['width_min']}×{res['height_min']}</b>. \"\n",
        "                \"A scatter plot is omitted because it would be uninformative.\",\n",
        "                small\n",
        "            ))\n",
        "\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # -------------------------\n",
        "    # Configuration Snapshot (like sensor report)\n",
        "    # -------------------------\n",
        "    story.append(Paragraph(\"Configuration Snapshot\", H2))\n",
        "\n",
        "    cfg_rows = [\n",
        "        [\"STRONG_AUG_THRESHOLD\", str(sb.get(\"strong_aug_threshold\", \"\"))],\n",
        "        [\"MODERATE_AUG_THRESHOLD\", str(sb.get(\"moderate_aug_threshold\", \"\"))],\n",
        "        [\"RandAugment N\", str(sb.get(\"randaugment_N\", \"\"))],\n",
        "        [\"RandAugment M\", str(sb.get(\"randaugment_M\", \"\"))],\n",
        "        [\"Augmentation family\", str(sb.get(\"family\", \"\"))],\n",
        "        [\"Augmentation level\", str(sb.get(\"level\", \"\"))],\n",
        "        [\"Validation split\", str(splits.get(\"val_split\", \"\"))],\n",
        "        [\"Train / Val / Test samples\", f\"{splits.get('train_samples','')} / {splits.get('val_samples','')} / {splits.get('test_samples','')}\"],\n",
        "        [\"Batch size\", str(loader_cfg.get(\"batch_size\", \"\"))],\n",
        "        [\"Num workers\", str(loader_cfg.get(\"num_workers\", \"\"))],\n",
        "        [\"Pin memory\", str(loader_cfg.get(\"pin_memory\", \"\"))],\n",
        "    ]\n",
        "\n",
        "    tc = Table(cfg_rows, colWidths=[7.5*cm, 8.5*cm])\n",
        "    tc.setStyle(TableStyle([\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.whitesmoke),\n",
        "        (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n",
        "        (\"ROWBACKGROUNDS\", (0,1), (-1,-1), [colors.white, colors.Color(0.97,0.97,0.97)]),\n",
        "        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
        "        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
        "        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n",
        "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n",
        "    ]))\n",
        "    story.append(tc)\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # -------------------------\n",
        "    # 2) Preprocessing Steps Applied (expanded)\n",
        "    # -------------------------\n",
        "    story.append(Paragraph(\"2. Preprocessing Steps Applied\", H2))\n",
        "\n",
        "    story.append(Paragraph(\n",
        "        \"<b>• Input decoding & integrity:</b> Images are loaded and decoded into a consistent in-memory representation. \"\n",
        "        \"For file-based datasets, unreadable images can silently break training; therefore, the pipeline optionally performs \"\n",
        "        \"a sampled scan to detect decode failures early.\",\n",
        "        body\n",
        "    ))\n",
        "\n",
        "    story.append(Paragraph(\n",
        "        f\"<b>• Standardization (shape):</b> All images are resized to <b>{report.get('img_size','')}</b> × \"\n",
        "        f\"<b>{report.get('img_size','')}</b>. This produces fixed tensor shapes, stable batching, and predictable compute cost \"\n",
        "        \"across datasets—important when comparing architectures fairly in NAS.\",\n",
        "        body\n",
        "    ))\n",
        "\n",
        "    story.append(Paragraph(\n",
        "        f\"<b>• Tensor conversion & normalization:</b> Images are converted to floating-point tensors and normalized using \"\n",
        "        f\"<b>{report.get('normalization','')}</b> statistics. Normalization stabilizes optimization and makes training behavior \"\n",
        "        \"more consistent across datasets and architectures (especially when using pretrained backbones).\",\n",
        "        body\n",
        "    ))\n",
        "\n",
        "    # Strategy B explanation (simple + chosen mode highlight)\n",
        "    for p in _augmentation_explanation_paragraphs(sb):\n",
        "        story.append(Paragraph(p, body if p.startswith(\"<b>•\") else small))\n",
        "\n",
        "    # Split explanation\n",
        "    story.append(Paragraph(\n",
        "        f\"<b>• Splitting & evaluation fairness:</b> The dataset is split into train/validation/test \"\n",
        "        f\"(<b>{splits.get('train_samples','')}</b> / <b>{splits.get('val_samples','')}</b> / <b>{splits.get('test_samples','')}</b>). \"\n",
        "        \"Augmentations are applied to training batches only. Validation and test pipelines remain deterministic to ensure \"\n",
        "        \"fair comparison between candidate architectures.\",\n",
        "        body\n",
        "    ))\n",
        "\n",
        "    # Loader settings explanation\n",
        "    story.append(Paragraph(\n",
        "        f\"<b>• DataLoader settings:</b> batch_size=<b>{loader_cfg.get('batch_size','')}</b>, \"\n",
        "        f\"num_workers=<b>{loader_cfg.get('num_workers','')}</b>, pin_memory=<b>{loader_cfg.get('pin_memory','')}</b>. \"\n",
        "        \"These settings control input throughput and help keep the GPU utilized during training.\",\n",
        "        body\n",
        "    ))\n",
        "\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # -------------------------\n",
        "    # 3) Visual Sanity Check\n",
        "    # -------------------------\n",
        "    story.append(Paragraph(\"3. Visual Sanity Check\", H2))\n",
        "    story.append(Paragraph(\n",
        "        \"The grid below shows a sample of training images after the selected Strategy B augmentation policy. \"\n",
        "        \"This is a quick sanity check that augmentations are label-preserving and not overly destructive.\",\n",
        "        body\n",
        "    ))\n",
        "\n",
        "    if train_loader is not None:\n",
        "        grid = _make_aug_grid_png(train_loader, max_images=12)\n",
        "        if grid:\n",
        "            tmp_files.append(grid)\n",
        "            story.append(_fit_rl_image(grid, max_w=doc.width, max_h=16*cm))\n",
        "            story.append(Paragraph(\"Augmented samples (train loader)\", caption))\n",
        "        else:\n",
        "            story.append(Paragraph(\"Could not render augmented sample grid (train_loader unavailable or empty).\", small))\n",
        "    else:\n",
        "        story.append(Paragraph(\"No train_loader provided; skipping sample grid.\", small))\n",
        "\n",
        "    story.append(Paragraph(\"End of report.\", ParagraphStyle(\"end\", fontSize=9, alignment=1)))\n",
        "\n",
        "    # -------------------------\n",
        "    # Build + cleanup\n",
        "    # -------------------------\n",
        "    doc.build(story, onFirstPage=draw_header_footer, onLaterPages=draw_header_footer)\n",
        "\n",
        "    for f in tmp_files:\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(f\"[INFO] Image preprocessing report saved to {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWrtK1b-QJn9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random, numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image  # needed for ImageFolder resolution scan\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility\n",
        "# ----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset / Loader config\n",
        "# ----------------------------\n",
        "DATA_ROOT = \"./data\"     # CIFAR-10 will download here by default.\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "VAL_SPLIT = 0.1          # (train -> train/val) split\n",
        "PIN_MEMORY = True\n",
        "\n",
        "# If you want to use your own dataset (ImageFolder), set:\n",
        "# DATASET_MODE = \"imagefolder\"\n",
        "# and set DATA_ROOT to folder:\n",
        "# DATA_ROOT/\n",
        "#   class_a/xxx.jpg\n",
        "#   class_b/yyy.jpg\n",
        "DATASET_MODE = \"cifar10\"   # \"cifar10\" or \"imagefolder\"\n",
        "\n",
        "# ----------------------------\n",
        "# Strategy B thresholds & params\n",
        "# ----------------------------\n",
        "STRONG_AUG_THRESHOLD = 5_000\n",
        "MODERATE_AUG_THRESHOLD = 50_000\n",
        "\n",
        "# RandAugment knobs (torchvision scale)\n",
        "RAND_N = 2\n",
        "RAND_M_STRONG = 18\n",
        "RAND_M_MODERATE = 10\n",
        "RAND_M_LIGHT = 6\n",
        "\n",
        "# Default (will be overwritten correctly once dataset is loaded)\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Will be populated after calling get_dataloaders(...)\n",
        "LAST_IMAGE_PREP_REPORT = None\n",
        "\n",
        "\n",
        "def _decide_strategy_b(num_images: int):\n",
        "    \"\"\"\n",
        "    Strategy B decision: returns (level, family, randN, randM)\n",
        "    family ∈ {\"trivialaugment\", \"randaugment\", \"basic\", \"none\"}\n",
        "    \"\"\"\n",
        "    if num_images < STRONG_AUG_THRESHOLD:\n",
        "        level = \"strong\"\n",
        "        family = \"trivialaugment\"     # best default for small datasets\n",
        "        randM = RAND_M_STRONG\n",
        "    elif num_images <= MODERATE_AUG_THRESHOLD:\n",
        "        level = \"moderate\"\n",
        "        family = \"randaugment\"\n",
        "        randM = RAND_M_MODERATE\n",
        "    else:\n",
        "        level = \"light\"\n",
        "        family = \"basic\"              # could set to \"none\" if you prefer\n",
        "        randM = RAND_M_LIGHT\n",
        "\n",
        "    return level, family, RAND_N, randM\n",
        "\n",
        "\n",
        "def make_transforms(img_size: int, family: str, rand_n: int, rand_m: int):\n",
        "    \"\"\"\n",
        "    Build train/eval transforms.\n",
        "    IMPORTANT:\n",
        "      - Train gets Strategy B augmentation.\n",
        "      - Val/Test are deterministic (no random aug).\n",
        "    \"\"\"\n",
        "    # ImageNet normalization (works well for pretrained backbones used later)\n",
        "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    train_ops = [T.Resize((img_size, img_size))]\n",
        "\n",
        "    if family == \"trivialaugment\":\n",
        "        train_ops.append(T.TrivialAugmentWide())\n",
        "    elif family == \"randaugment\":\n",
        "        train_ops.append(T.RandAugment(num_ops=rand_n, magnitude=rand_m))\n",
        "    elif family == \"basic\":\n",
        "        train_ops.extend([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.RandomResizedCrop(img_size, scale=(0.85, 1.0)),\n",
        "        ])\n",
        "    elif family == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown Strategy B family: {family}\")\n",
        "\n",
        "    train_ops.extend([T.ToTensor(), normalize])\n",
        "\n",
        "    eval_tf = T.Compose([\n",
        "        T.Resize((img_size, img_size)),\n",
        "        T.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    return T.Compose(train_ops), eval_tf\n",
        "\n",
        "\n",
        "def _class_counts_from_dataset(ds, num_classes: int):\n",
        "    \"\"\"\n",
        "    Returns class counts as dict[str,int], using class names if available.\n",
        "    \"\"\"\n",
        "    if hasattr(ds, \"targets\"):  # CIFAR10\n",
        "        y = np.array(ds.targets, dtype=int)\n",
        "        counts = np.bincount(y, minlength=num_classes)\n",
        "        return {str(i): int(counts[i]) for i in range(len(counts))}\n",
        "    if hasattr(ds, \"samples\"):  # ImageFolder\n",
        "        y = np.array([lbl for _, lbl in ds.samples], dtype=int)\n",
        "        counts = np.bincount(y, minlength=num_classes)\n",
        "        if hasattr(ds, \"classes\"):\n",
        "            return {str(ds.classes[i]): int(counts[i]) for i in range(len(counts))}\n",
        "        return {str(i): int(counts[i]) for i in range(len(counts))}\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _sample_resolution_stats(ds, limit: int = 600):\n",
        "    \"\"\"\n",
        "    Quick resolution stats using a sample (for CIFAR10 reads from ds.data, for ImageFolder opens files).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    n = len(ds)\n",
        "    k = min(limit, n)\n",
        "    idxs = rng.choice(n, size=k, replace=False)\n",
        "\n",
        "    widths, heights, bad = [], [], 0\n",
        "\n",
        "    if isinstance(ds, torchvision.datasets.CIFAR10):\n",
        "        # ds.data: uint8 [N,32,32,3]\n",
        "        for i in idxs:\n",
        "            img = ds.data[int(i)]\n",
        "            h, w = int(img.shape[0]), int(img.shape[1])\n",
        "            widths.append(w)\n",
        "            heights.append(h)\n",
        "    else:\n",
        "        # ImageFolder: open paths\n",
        "        for i in idxs:\n",
        "            path, _ = ds.samples[int(i)]\n",
        "            try:\n",
        "                with Image.open(path) as im:\n",
        "                    w, h = im.size\n",
        "                widths.append(int(w))\n",
        "                heights.append(int(h))\n",
        "            except Exception:\n",
        "                bad += 1\n",
        "\n",
        "    out = {\n",
        "        \"resolution_scan_limit\": int(k),\n",
        "        \"resolution_bad_count\": int(bad),\n",
        "    }\n",
        "    if widths:\n",
        "        out.update({\n",
        "            \"width_mean\": float(np.mean(widths)),\n",
        "            \"height_mean\": float(np.mean(heights)),\n",
        "            \"width_median\": float(np.median(widths)),\n",
        "            \"height_median\": float(np.median(heights)),\n",
        "            \"width_min\": int(np.min(widths)),\n",
        "            \"height_min\": int(np.min(heights)),\n",
        "            \"width_max\": int(np.max(widths)),\n",
        "            \"height_max\": int(np.max(heights)),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_dataloaders(img_size: int):\n",
        "    \"\"\"\n",
        "    Returns: train_loader, val_loader, test_loader\n",
        "    Also populates LAST_IMAGE_PREP_REPORT for PDF generation.\n",
        "    \"\"\"\n",
        "    global NUM_CLASSES, LAST_IMAGE_PREP_REPORT\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load dataset (NO transforms yet)\n",
        "    # ----------------------------\n",
        "    if DATASET_MODE.lower() == \"cifar10\":\n",
        "        full_train = torchvision.datasets.CIFAR10(\n",
        "            root=DATA_ROOT, train=True, download=True, transform=None\n",
        "        )\n",
        "        test_set = torchvision.datasets.CIFAR10(\n",
        "            root=DATA_ROOT, train=False, download=True, transform=None\n",
        "        )\n",
        "        NUM_CLASSES = 10\n",
        "\n",
        "    elif DATASET_MODE.lower() == \"imagefolder\":\n",
        "        if not os.path.isdir(DATA_ROOT):\n",
        "            raise FileNotFoundError(f\"DATA_ROOT not found: {DATA_ROOT}\")\n",
        "\n",
        "        full_train = torchvision.datasets.ImageFolder(DATA_ROOT, transform=None)\n",
        "        test_set = None  # will be created below\n",
        "        NUM_CLASSES = len(full_train.classes)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown DATASET_MODE: {DATASET_MODE} (use 'cifar10' or 'imagefolder')\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Compute dataset stats for report\n",
        "    # ----------------------------\n",
        "    n_images = int(len(full_train))\n",
        "    class_counts = _class_counts_from_dataset(full_train, NUM_CLASSES)\n",
        "    counts_only = list(class_counts.values()) if class_counts else []\n",
        "    imbalance_ratio = (max(counts_only) / max(1, min(counts_only))) if counts_only else 1.0\n",
        "    res_stats = _sample_resolution_stats(full_train, limit=600)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Strategy B decision based on dataset size\n",
        "    # ----------------------------\n",
        "    level, family, rand_n, rand_m = _decide_strategy_b(n_images)\n",
        "\n",
        "    # Build transforms (train has Strategy B aug; eval is deterministic)\n",
        "    train_tf, eval_tf = make_transforms(img_size, family, rand_n, rand_m)\n",
        "\n",
        "    # Assign transforms AFTER decision\n",
        "    full_train.transform = train_tf\n",
        "\n",
        "    # ----------------------------\n",
        "    # Splits\n",
        "    # ----------------------------\n",
        "    if DATASET_MODE.lower() == \"cifar10\":\n",
        "        # CIFAR-10 already has a separate test split\n",
        "        test_set.transform = eval_tf\n",
        "\n",
        "        val_len = int(len(full_train) * VAL_SPLIT)\n",
        "        train_len = len(full_train) - val_len\n",
        "        train_set, val_set = random_split(\n",
        "            full_train, [train_len, val_len],\n",
        "            generator=torch.Generator().manual_seed(SEED)\n",
        "        )\n",
        "\n",
        "        # Val must be deterministic: create a second CIFAR10 dataset with eval transform\n",
        "        val_base = torchvision.datasets.CIFAR10(\n",
        "            root=DATA_ROOT, train=True, download=True, transform=eval_tf\n",
        "        )\n",
        "        val_set = torch.utils.data.Subset(val_base, val_set.indices)\n",
        "\n",
        "    else:\n",
        "        # ImageFolder: create train/val/test splits from the single dataset.\n",
        "        # test split is fixed to 10% here; change if you want.\n",
        "        TEST_SPLIT = 0.1\n",
        "        test_len = max(1, int(TEST_SPLIT * len(full_train)))\n",
        "        remain_len = len(full_train) - test_len\n",
        "\n",
        "        remain_set, test_set = random_split(\n",
        "            full_train, [remain_len, test_len],\n",
        "            generator=torch.Generator().manual_seed(SEED)\n",
        "        )\n",
        "\n",
        "        val_len = max(1, int(remain_len * VAL_SPLIT))\n",
        "        train_len = remain_len - val_len\n",
        "\n",
        "        train_set, val_set = random_split(\n",
        "            remain_set, [train_len, val_len],\n",
        "            generator=torch.Generator().manual_seed(SEED)\n",
        "        )\n",
        "\n",
        "        # Make deterministic eval datasets for val/test with the same indices\n",
        "        eval_base = torchvision.datasets.ImageFolder(DATA_ROOT, transform=eval_tf)\n",
        "        val_set = torch.utils.data.Subset(eval_base, val_set.indices)\n",
        "        test_set = torch.utils.data.Subset(eval_base, test_set.indices)\n",
        "\n",
        "    # ----------------------------\n",
        "    # DataLoaders\n",
        "    # ----------------------------\n",
        "    train_loader = DataLoader(\n",
        "        train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    # ----------------------------\n",
        "    # Store report snapshot for PDF generation (sensor-like)\n",
        "    # ----------------------------\n",
        "    LAST_IMAGE_PREP_REPORT = {\n",
        "        \"timestamp\": __import__(\"datetime\").datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"dataset_mode\": DATASET_MODE,\n",
        "        \"data_root\": DATA_ROOT,\n",
        "        \"num_images\": int(n_images),\n",
        "        \"num_classes\": int(NUM_CLASSES),\n",
        "        \"class_counts\": class_counts,\n",
        "        \"imbalance_ratio\": float(imbalance_ratio),\n",
        "        \"img_size\": int(img_size),\n",
        "        \"normalization\": \"imagenet\",\n",
        "        \"strategy_b\": {\n",
        "            \"level\": level,\n",
        "            \"family\": family,\n",
        "            \"randaugment_N\": int(rand_n),\n",
        "            \"randaugment_M\": int(rand_m),\n",
        "            \"strong_aug_threshold\": int(STRONG_AUG_THRESHOLD),\n",
        "            \"moderate_aug_threshold\": int(MODERATE_AUG_THRESHOLD),\n",
        "        },\n",
        "        \"resolution_stats\": res_stats,\n",
        "        \"splits\": {\n",
        "            \"val_split\": float(VAL_SPLIT),\n",
        "            \"train_samples\": int(len(train_set)),\n",
        "            \"val_samples\": int(len(val_set)),\n",
        "            \"test_samples\": int(len(test_set)),\n",
        "            \"train_batches\": int(len(train_loader)),\n",
        "            \"val_batches\": int(len(val_loader)),\n",
        "            \"test_batches\": int(len(test_loader)),\n",
        "            \"seed\": int(SEED),\n",
        "        },\n",
        "        \"loader\": {\n",
        "            \"batch_size\": int(BATCH_SIZE),\n",
        "            \"num_workers\": int(NUM_WORKERS),\n",
        "            \"pin_memory\": bool(PIN_MEMORY),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Quick, helpful log (won't break anything)\n",
        "    print(f\"[Strategy B] n={n_images} → level={level}, family={family}\"\n",
        "          + (f\", RandAug(N={rand_n}, M={rand_m})\" if family == \"randaugment\" else \"\")\n",
        "          + f\" | img_size={img_size} | NUM_CLASSES={NUM_CLASSES}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPefVUQv7oBu",
        "outputId": "34bc314c-f67e-4283-8ff9-4ac736a66c55"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = get_dataloaders(64)\n",
        "\n",
        "generate_image_preprocessing_report(\n",
        "    LAST_IMAGE_PREP_REPORT,\n",
        "    train_loader=train_loader,                  # enables augmented image grid\n",
        "    path=\"image_prep_report.pdf\",\n",
        "    project_name=\"Automata AI - Preprocessing Report\",\n",
        "    logo_path=None  # put your logo path here if you want\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "6_4nirI7IUaT",
        "outputId": "850dc8ac-3c79-4d4e-a625-244f90bc205b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Sweep configs\n",
        "SWEEP = [\n",
        "    {\"name\": \"mnetv3_small_unfreeze0_res160\",         \"backbone\": \"mnetv3_small\",         \"unfreeze_blocks\": 0, \"img_size\": 160, \"lr\": 3e-3},\n",
        "    {\"name\": \"mnetv3_small_unfreeze1_res160\",         \"backbone\": \"mnetv3_small\",         \"unfreeze_blocks\": 1, \"img_size\": 160, \"lr\": 2e-3},\n",
        "    {\"name\": \"shufflenetv2_x0_5_unfreeze1_res160\",    \"backbone\": \"shufflenetv2_x0_5\",    \"unfreeze_blocks\": 1, \"img_size\": 160, \"lr\": 2e-3},\n",
        "    {\"name\": \"squeezenet1_1_unfreeze1_res160\",        \"backbone\": \"squeezenet1_1\",        \"unfreeze_blocks\": 1, \"img_size\": 160, \"lr\": 2e-3},\n",
        "]\n",
        "\n",
        "EPOCHS_PER_TRIAL = 3\n",
        "FINAL_EPOCHS = 5\n",
        "\n",
        "# Model builders\n",
        "def build_backbone(backbone: str, num_classes: int):\n",
        "    if backbone == \"mnetv3_small\":\n",
        "        weights = torchvision.models.MobileNet_V3_Small_Weights.DEFAULT\n",
        "        model = torchvision.models.mobilenet_v3_small(weights=weights)\n",
        "        in_features = model.classifier[-1].in_features\n",
        "        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
        "        return model\n",
        "\n",
        "    elif backbone == \"shufflenetv2_x0_5\":\n",
        "        weights = torchvision.models.ShuffleNet_V2_X0_5_Weights.DEFAULT\n",
        "        model = torchvision.models.shufflenet_v2_x0_5(weights=weights)\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(in_features, num_classes)\n",
        "        return model\n",
        "\n",
        "    elif backbone == \"squeezenet1_1\":\n",
        "        weights = torchvision.models.SqueezeNet1_1_Weights.DEFAULT\n",
        "        model = torchvision.models.squeezenet1_1(weights=weights)\n",
        "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "        return model\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
        "\n",
        "# Freeze / unfreeze\n",
        "def unfreeze_module(m: nn.Module):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def get_head_module(model: nn.Module) -> nn.Module:\n",
        "    if hasattr(model, \"classifier\"):\n",
        "        return model.classifier\n",
        "    if hasattr(model, \"fc\"):\n",
        "        return model.fc\n",
        "    raise ValueError(\"Could not find classifier head (expected .classifier or .fc)\")\n",
        "\n",
        "def get_block_list(backbone: str, model: nn.Module):\n",
        "    if backbone == \"mnetv3_small\":\n",
        "        return list(model.features)\n",
        "    if backbone == \"shufflenetv2_x0_5\":\n",
        "        return [model.conv1, model.maxpool, model.stage2, model.stage3, model.stage4, model.conv5]\n",
        "    if backbone == \"squeezenet1_1\":\n",
        "        return list(model.features)\n",
        "    raise ValueError(f\"Unsupported backbone for block unfreezing: {backbone}\")\n",
        "\n",
        "def freeze_all_but_head(model: nn.Module):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    head = get_head_module(model)\n",
        "    unfreeze_module(head)\n",
        "\n",
        "def unfreeze_last_n_blocks(model: nn.Module, backbone: str, n_blocks: int):\n",
        "    freeze_all_but_head(model)\n",
        "    if n_blocks <= 0:\n",
        "        return\n",
        "    blocks = get_block_list(backbone, model)\n",
        "    start = max(0, len(blocks) - n_blocks)\n",
        "    for i in range(start, len(blocks)):\n",
        "        unfreeze_module(blocks[i])\n",
        "\n",
        "def count_trainable_params(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Train / eval\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def make_optimizer(model, lr: float):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    return optim.AdamW(params, lr=lr, weight_decay=1e-4)\n",
        "\n",
        "# Sweep + final fine-tune\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "results = []\n",
        "best_cfg = None\n",
        "\n",
        "for trial in SWEEP:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Trial:\", trial[\"name\"])\n",
        "    print(trial)\n",
        "    t0 = time.time()\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(trial[\"img_size\"])\n",
        "\n",
        "    model = build_backbone(trial[\"backbone\"], num_classes=NUM_CLASSES).to(device)\n",
        "    unfreeze_last_n_blocks(model, backbone=trial[\"backbone\"], n_blocks=trial[\"unfreeze_blocks\"])\n",
        "\n",
        "    trainable = count_trainable_params(model)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable params: {trainable:,} / {total_params:,} ({100*trainable/total_params:.2f}%)\")\n",
        "\n",
        "    optimizer = make_optimizer(model, lr=trial[\"lr\"])\n",
        "\n",
        "    best_val = 0.0\n",
        "    for epoch in range(1, EPOCHS_PER_TRIAL + 1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        va_loss, va_acc = evaluate(model, val_loader, criterion)\n",
        "        best_val = max(best_val, va_acc)\n",
        "        print(f\"Epoch {epoch:02d}/{EPOCHS_PER_TRIAL} | \"\n",
        "              f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
        "              f\"val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "\n",
        "    te_loss, te_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    out = {\n",
        "        \"name\": trial[\"name\"],\n",
        "        \"backbone\": trial[\"backbone\"],\n",
        "        \"unfreeze_blocks\": trial[\"unfreeze_blocks\"],\n",
        "        \"img_size\": trial[\"img_size\"],\n",
        "        \"lr\": trial[\"lr\"],\n",
        "        \"best_val_acc\": float(best_val),\n",
        "        \"test_acc\": float(te_acc),\n",
        "        \"seconds\": float(elapsed),\n",
        "    }\n",
        "    results.append(out)\n",
        "\n",
        "    print(f\"Trial done in {elapsed/60:.2f} min | best val acc {best_val:.4f} | test acc {te_acc:.4f}\")\n",
        "\n",
        "    if (best_cfg is None) or (out[\"best_val_acc\"] > best_cfg[\"best_val_acc\"]):\n",
        "        best_cfg = out\n",
        "\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(\"Sweep results (sorted by best_val_acc):\")\n",
        "results_sorted = sorted(results, key=lambda x: x[\"best_val_acc\"], reverse=True)\n",
        "for r in results_sorted:\n",
        "    print(f\"{r['name']:40s} | val {r['best_val_acc']:.4f} | test {r['test_acc']:.4f} | {r['seconds']:.0f}s\")\n",
        "\n",
        "print(\"\\nBest config:\", best_cfg)\n",
        "\n",
        "# Final fine-tune on best config\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Final fine-tune best config for a few more epochs...\")\n",
        "\n",
        "train_loader, val_loader, test_loader = get_dataloaders(best_cfg[\"img_size\"])\n",
        "best_model = build_backbone(best_cfg[\"backbone\"], num_classes=NUM_CLASSES).to(device)\n",
        "unfreeze_last_n_blocks(best_model, backbone=best_cfg[\"backbone\"], n_blocks=best_cfg[\"unfreeze_blocks\"])\n",
        "\n",
        "optimizer = make_optimizer(best_model, lr=best_cfg[\"lr\"])\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FINAL_EPOCHS)\n",
        "\n",
        "best_val = 0.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, FINAL_EPOCHS + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(best_model, train_loader, optimizer, criterion)\n",
        "    va_loss, va_acc = evaluate(best_model, val_loader, criterion)\n",
        "    scheduler.step()\n",
        "\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in best_model.state_dict().items()}\n",
        "\n",
        "    print(f\"[Final] Epoch {epoch:02d}/{FINAL_EPOCHS} | \"\n",
        "          f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
        "          f\"val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "\n",
        "if best_state is not None:\n",
        "    best_model.load_state_dict(best_state)\n",
        "\n",
        "final_test_loss, final_test_acc = evaluate(best_model, test_loader, criterion)\n",
        "\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(f\"FINAL Best Val Acc: {best_val:.4f}\")\n",
        "print(f\"FINAL Test Acc:     {final_test_acc:.4f}\")\n",
        "print(\"#\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se-DkuSjIfVy",
        "outputId": "9b4507d9-e2cd-4d44-8c32-027db52d9518"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "save_path = \"best_model.pth\"\n",
        "torch.save(best_model.state_dict(), save_path)\n",
        "\n",
        "size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
        "\n",
        "print(\"Saved:\", save_path)\n",
        "print(f\"Best model test accuracy: {final_test_acc:.4f}\")\n",
        "print(f\"Saved .pth size: {size_mb:.2f} MB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
