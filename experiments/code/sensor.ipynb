{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2C3BK1qt7wJ"
   },
   "outputs": [],
   "source": [
    "pip install pillow reportlab matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 27511,
     "status": "error",
     "timestamp": 1767607412214,
     "user": {
      "displayName": "Ahmed Samer",
      "userId": "17843102519676096162"
     },
     "user_tz": -120
    },
    "id": "_4cPU91TtvIx",
    "outputId": "a3350085-0137-4614-eb93-24c3a90240b5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys, os\n",
    "LIB_DIR = \"/content/drive/MyDrive/Automata AI/Sensor Pipeline\"\n",
    "print(\"Exists?\", os.path.exists(LIB_DIR))\n",
    "print(\"Files:\", os.listdir(LIB_DIR)[:20])\n",
    "\n",
    "if LIB_DIR not in sys.path:\n",
    "    sys.path.append(LIB_DIR)\n",
    "\n",
    "print(\"sys.path contains LIB_DIR?\", LIB_DIR in sys.path)\n",
    "\n",
    "# show python files only\n",
    "print(\"PY files:\", [f for f in os.listdir(LIB_DIR) if f.endswith(\".py\")])\n",
    "\n",
    "from automata_preprocessing import run_preprocessing, PreprocessConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74ikCMrJvelC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#test data\n",
    "df = pd.DataFrame({\n",
    "    \"reading_text\": [\n",
    "        \"temperature 22C humidity 40 comfortable\",\n",
    "        \"temperature 31C humidity 75 uncomfortable hot humid\",\n",
    "        \"temperature 18C humidity 35 comfortable cool dry\",\n",
    "        \"temperature 29C humidity 60 uncomfortable warm\",\n",
    "        \"temperature 24C humidity 45 comfortable normal\",\n",
    "        \"temperature 34C humidity 80 uncomfortable very hot humid\",\n",
    "        \"temperature 20C humidity 30 uncomfortable cold dry\",\n",
    "        \"temperature 26C humidity 50 comfortable warm\",\n",
    "    ],\n",
    "    \"label\": [\"Comfortable\",\"Uncomfortable\",\"Comfortable\",\"Uncomfortable\",\"Comfortable\",\"Uncomfortable\",\"Uncomfortable\",\"Comfortable\"]\n",
    "})\n",
    "\n",
    "X = df[[\"reading_text\"]]\n",
    "y = df[\"label\"]\n",
    "\n",
    "logo_path = \"/content/drive/MyDrive/Automata AI/Automata_AI_Logo.png\"\n",
    "\n",
    "cfg = PreprocessConfig(\n",
    "    report=True,\n",
    "    report_path=\"/content/drive/MyDrive/Automata AI/Sensor Pipeline/sensor_preprocessing_report.pdf\",\n",
    "    project_name=\"Sensor Pipeline – Preprocessing\",\n",
    "    logo_path=logo_path,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X, y = run_preprocessing(X, y, cfg)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "# Make aliases for meta-feature code\n",
    "data, target = X, y\n",
    "\n",
    "print(\"\\n✅ Data cleaning complete!\")\n",
    "print(f\"Final Shape: {df.shape}\")\n",
    "print(f\"Train/Test Split: {len(X_train)} train / {len(X_test)} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nc_plKB4icXu"
   },
   "outputs": [],
   "source": [
    "def build_meta_entries_for_all_models_from_preprocessed(X,y,task_id: int = 0,dataset_id: int = 0,dataset_name: str = \"unknown\") -> dict:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Helper: dataset-level meta-features\n",
    "    # -------------------------------------------------------------------------\n",
    "    def compute_dataset_features(X_df: pd.DataFrame, y_arr) -> dict:\n",
    "        meta = {}\n",
    "\n",
    "        # Basic counts\n",
    "        try:\n",
    "            n_samples, n_features = X_df.shape\n",
    "        except Exception:\n",
    "            n_samples, n_features = None, None\n",
    "        meta[\"n_samples\"] = int(n_samples) if n_samples is not None else None\n",
    "        meta[\"n_features\"] = int(n_features) if n_features is not None else None\n",
    "\n",
    "        # Numeric / categorical / binary features\n",
    "        try:\n",
    "            dtypes = X_df.dtypes\n",
    "            numeric_mask = [is_numeric_dtype(dt) for dt in dtypes]\n",
    "            n_numeric = int(np.sum(numeric_mask))\n",
    "            n_categorical = int(len(dtypes) - n_numeric)\n",
    "\n",
    "            n_binary = 0\n",
    "            for col in X_df.columns:\n",
    "                vals = pd.Series(X_df[col]).dropna().unique()\n",
    "                if len(vals) == 2:\n",
    "                    n_binary += 1\n",
    "\n",
    "            meta[\"n_numeric_features\"] = n_numeric\n",
    "            meta[\"n_categorical_features\"] = n_categorical\n",
    "            meta[\"n_binary_features\"] = int(n_binary)\n",
    "        except Exception:\n",
    "            numeric_mask = None\n",
    "            meta[\"n_numeric_features\"] = None\n",
    "            meta[\"n_categorical_features\"] = None\n",
    "            meta[\"n_binary_features\"] = None\n",
    "\n",
    "        # Class stats\n",
    "        try:\n",
    "            y_np = np.asarray(y_arr)\n",
    "            le = LabelEncoder()\n",
    "            y_enc = le.fit_transform(y_np)\n",
    "            classes, counts = np.unique(y_enc, return_counts=True)\n",
    "            n_classes = len(classes)\n",
    "            probs = counts / counts.sum()\n",
    "            class_balance_std = float(probs.std()) if n_classes > 0 else None\n",
    "            class_entropy = float(\n",
    "                -(probs * np.log2(probs + 1e-12)).sum()\n",
    "            ) if n_classes > 0 else None\n",
    "            meta[\"n_classes\"] = int(n_classes)\n",
    "            meta[\"class_balance_std\"] = class_balance_std\n",
    "            meta[\"class_entropy\"] = class_entropy\n",
    "        except Exception:\n",
    "            meta[\"n_classes\"] = None\n",
    "            meta[\"class_balance_std\"] = None\n",
    "            meta[\"class_entropy\"] = None\n",
    "\n",
    "        # Numeric feature variance and correlations\n",
    "        try:\n",
    "            num_cols = X_df.select_dtypes(include=[np.number])\n",
    "\n",
    "            mean_var = 0.0\n",
    "            med_var = 0.0\n",
    "            mean_corr = 0.0\n",
    "            max_corr = 0.0\n",
    "\n",
    "            if num_cols.shape[1] > 0 and num_cols.shape[0] > 1:\n",
    "                vars_ = num_cols.var(axis=0, ddof=1).values\n",
    "                if np.isfinite(vars_).sum() > 0:\n",
    "                    mean_var = float(np.nanmean(vars_))\n",
    "                    med_var = float(np.nanmedian(vars_))\n",
    "\n",
    "                max_corr_features = min(num_cols.shape[1], 50)\n",
    "                corr = num_cols.iloc[:, :max_corr_features].corr().abs().values\n",
    "                upper = corr[np.triu_indices_from(corr, k=1)]\n",
    "                finite_upper = upper[np.isfinite(upper)]\n",
    "                if finite_upper.size > 0:\n",
    "                    mean_corr = float(finite_upper.mean())\n",
    "                    max_corr = float(finite_upper.max())\n",
    "\n",
    "            meta[\"mean_feature_variance\"] = mean_var\n",
    "            meta[\"median_feature_variance\"] = med_var\n",
    "            meta[\"mean_corr_abs\"] = mean_corr\n",
    "            meta[\"max_corr_abs\"] = max_corr\n",
    "        except Exception:\n",
    "            meta[\"mean_feature_variance\"] = 0.0\n",
    "            meta[\"median_feature_variance\"] = 0.0\n",
    "            meta[\"mean_corr_abs\"] = 0.0\n",
    "            meta[\"max_corr_abs\"] = 0.0\n",
    "\n",
    "        # Extra dataset-level features\n",
    "        try:\n",
    "            num_cols = X_df.select_dtypes(include=[np.number])\n",
    "\n",
    "            # 1) feature_skewness_mean\n",
    "            if num_cols.shape[1] > 0:\n",
    "                skews = num_cols.skew(axis=0, skipna=True)\n",
    "                skews = skews.replace([np.inf, -np.inf], np.nan)\n",
    "                feature_skewness_mean = float(\n",
    "                    skews.mean(skipna=True)\n",
    "                ) if not skews.isna().all() else 0.0\n",
    "            else:\n",
    "                feature_skewness_mean = 0.0\n",
    "            meta[\"feature_skewness_mean\"] = feature_skewness_mean\n",
    "\n",
    "            # 2) feature_kurtosis_mean\n",
    "            if num_cols.shape[1] > 0:\n",
    "                kurts = num_cols.kurt(axis=0, skipna=True)\n",
    "                kurts = kurts.replace([np.inf, -np.inf], np.nan)\n",
    "                feature_kurtosis_mean = float(\n",
    "                    kurts.mean(skipna=True)\n",
    "                ) if not kurts.isna().all() else 0.0\n",
    "            else:\n",
    "                feature_kurtosis_mean = 0.0\n",
    "            meta[\"feature_kurtosis_mean\"] = feature_kurtosis_mean\n",
    "\n",
    "            # 3) missing_percentage\n",
    "            if (\n",
    "                n_samples is not None\n",
    "                and n_features is not None\n",
    "                and n_samples > 0\n",
    "                and n_features > 0\n",
    "            ):\n",
    "                total_cells = float(n_samples * n_features)\n",
    "                missing_count = float(X_df.isna().sum().sum())\n",
    "                missing_percentage = missing_count / total_cells\n",
    "            else:\n",
    "                missing_percentage = 0.0\n",
    "            meta[\"missing_percentage\"] = float(missing_percentage)\n",
    "\n",
    "            # 4) avg_cardinality_categorical\n",
    "            avg_card = 0.0\n",
    "            if \"numeric_mask\" in locals() and numeric_mask is not None:\n",
    "                cat_cols = [\n",
    "                    col for col, isnum in zip(X_df.columns, numeric_mask) if not isnum\n",
    "                ]\n",
    "                if len(cat_cols) > 0:\n",
    "                    cards = []\n",
    "                    for col in cat_cols:\n",
    "                        try:\n",
    "                            cards.append(X_df[col].nunique(dropna=True))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    if len(cards) > 0:\n",
    "                        avg_card = float(np.mean(cards))\n",
    "            meta[\"avg_cardinality_categorical\"] = avg_card\n",
    "\n",
    "            # 5) complexity_ratio\n",
    "            if n_samples is not None and n_features is not None and n_samples > 0:\n",
    "                complexity_ratio = float(n_features) / float(n_samples)\n",
    "            else:\n",
    "                complexity_ratio = 0.0\n",
    "            meta[\"complexity_ratio\"] = complexity_ratio\n",
    "\n",
    "            # 6) intrinsic_dim_estimate (PCA-based)\n",
    "            intrinsic_dim = 0.0\n",
    "            try:\n",
    "                if num_cols.shape[1] >= 2 and num_cols.shape[0] >= 5:\n",
    "                    X_pca = num_cols.to_numpy(dtype=np.float32)\n",
    "                    col_means = np.nanmean(X_pca, axis=0)\n",
    "                    inds = np.where(np.isnan(X_pca))\n",
    "                    if inds[0].size > 0:\n",
    "                        X_pca[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "                    n_components = min(X_pca.shape[0], X_pca.shape[1])\n",
    "                    if n_components >= 1:\n",
    "                        pca = PCA(n_components=n_components)\n",
    "                        pca.fit(X_pca)\n",
    "                        cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "                        k = int(np.searchsorted(cumsum, 0.95) + 1)\n",
    "                        intrinsic_dim = float(max(1, min(k, n_components)))\n",
    "            except Exception:\n",
    "                intrinsic_dim = 0.0\n",
    "\n",
    "            meta[\"intrinsic_dim_estimate\"] = intrinsic_dim\n",
    "        except Exception:\n",
    "            meta.setdefault(\"feature_skewness_mean\", 0.0)\n",
    "            meta.setdefault(\"feature_kurtosis_mean\", 0.0)\n",
    "            meta.setdefault(\"missing_percentage\", 0.0)\n",
    "            meta.setdefault(\"avg_cardinality_categorical\", 0.0)\n",
    "            meta.setdefault(\"complexity_ratio\", 0.0)\n",
    "            meta.setdefault(\"intrinsic_dim_estimate\", 0.0)\n",
    "\n",
    "        return meta\n",
    "\n",
    "    def _safe_stratify_or_none(y_enc: np.ndarray):\n",
    "        values, counts = np.unique(y_enc, return_counts=True)\n",
    "        if counts.min() < 2:\n",
    "            return None\n",
    "        return y_enc\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Helper: compute landmarks\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def compute_landmarks(X_train: pd.DataFrame, y_train) -> dict:\n",
    "        landmarks = {\n",
    "            \"landmark_lr_accuracy\": 0.0,\n",
    "            \"landmark_dt_depth3_accuracy\": 0.0,\n",
    "            \"landmark_knn3_accuracy\": 0.0,\n",
    "            \"landmark_random_noise_accuracy\": 0.0,\n",
    "            \"fisher_discriminant_ratio\": 0.0,\n",
    "        }\n",
    "\n",
    "        y_arr = np.asarray(y_train)\n",
    "        if y_arr.ndim > 1:\n",
    "            y_arr = y_arr.ravel()\n",
    "        try:\n",
    "            y_arr = y_arr.astype(int)\n",
    "        except Exception:\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le_fallback = LabelEncoder()\n",
    "            y_arr = le_fallback.fit_transform(y_arr)\n",
    "\n",
    "        if X_train.shape[0] < 5 or len(np.unique(y_arr)) < 2:\n",
    "            return landmarks\n",
    "\n",
    "        LANDMARK_SUBSAMPLE_FRACTION = 0.15\n",
    "        LANDMARK_MAX_ROWS           = 1000\n",
    "        LANDMARK_MIN_ROWS           = 20\n",
    "\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_num = X_train.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "            if X_num.shape[1] == 0:\n",
    "                X_num = pd.DataFrame(index=X_train.index)\n",
    "                for col in X_train.columns:\n",
    "                    s = X_train[col]\n",
    "                    if is_numeric_dtype(s):\n",
    "                        X_num[col] = pd.to_numeric(s, errors=\"coerce\").fillna(0)\n",
    "                    else:\n",
    "                        le_col = LabelEncoder()\n",
    "                        X_num[col] = le_col.fit_transform(\n",
    "                            s.astype(str).fillna(\"__NA__\")\n",
    "                        )\n",
    "        else:\n",
    "            X_num = pd.DataFrame(X_train)\n",
    "\n",
    "        if X_num.shape[1] == 0:\n",
    "            return landmarks\n",
    "\n",
    "        X_num = X_num.replace([np.inf, -np.inf], np.nan)\n",
    "        vals = X_num.to_numpy(dtype=np.float32)\n",
    "\n",
    "        if np.isnan(vals).any() or not np.isfinite(vals).all():\n",
    "            vals[~np.isfinite(vals)] = np.nan\n",
    "            col_means = np.nanmean(vals, axis=0)\n",
    "            col_means = np.where(np.isnan(col_means), 0.0, col_means)\n",
    "            inds = np.where(np.isnan(vals))\n",
    "            if inds[0].size > 0:\n",
    "                vals[inds] = np.take(col_means, inds[1])\n",
    "            X_num = pd.DataFrame(vals, columns=X_num.columns)\n",
    "\n",
    "        # --- SUBSAMPLE rows\n",
    "        n_rows = X_num.shape[0]\n",
    "        RNG = np.random.RandomState(42)\n",
    "\n",
    "        n_sub = min(LANDMARK_MAX_ROWS, int(LANDMARK_SUBSAMPLE_FRACTION * n_rows))\n",
    "        if n_sub < LANDMARK_MIN_ROWS:\n",
    "            n_sub = LANDMARK_MIN_ROWS\n",
    "        n_sub = min(n_sub, n_rows)\n",
    "\n",
    "        idx = RNG.choice(n_rows, size=n_sub, replace=False)\n",
    "        X_num_sub = X_num.iloc[idx].reset_index(drop=True).astype(np.float32)\n",
    "        y_sub = y_arr[idx]\n",
    "\n",
    "        if len(np.unique(y_sub)) < 2:\n",
    "            return landmarks\n",
    "\n",
    "        strat_labels = _safe_stratify_or_none(y_sub)\n",
    "\n",
    "        # 1) Logistic Regression accuracy\n",
    "        try:\n",
    "            Xtr, Xte, ytr, yte = train_test_split(\n",
    "                X_num_sub,\n",
    "                y_sub,\n",
    "                test_size=0.2,\n",
    "                random_state=42,\n",
    "                stratify=strat_labels,\n",
    "            )\n",
    "            clf = LogisticRegression(max_iter=50, C=0.1, solver=\"lbfgs\")\n",
    "            clf.fit(Xtr, ytr)\n",
    "            acc = accuracy_score(yte, clf.predict(Xte))\n",
    "            landmarks[\"landmark_lr_accuracy\"] = float(acc)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 2) Decision Tree depth=3 accuracy\n",
    "        try:\n",
    "            Xtr, Xte, ytr, yte = train_test_split(\n",
    "                X_num_sub,\n",
    "                y_sub,\n",
    "                test_size=0.2,\n",
    "                random_state=42,\n",
    "                stratify=strat_labels,\n",
    "            )\n",
    "            clf = DecisionTreeClassifier(\n",
    "                max_depth=3, min_samples_leaf=5, random_state=42\n",
    "            )\n",
    "            clf.fit(Xtr, ytr)\n",
    "            acc = accuracy_score(yte, clf.predict(Xte))\n",
    "            landmarks[\"landmark_dt_depth3_accuracy\"] = float(acc)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 3) KNN-3 accuracy (optionally dim-reduce to 30 features)\n",
    "        try:\n",
    "            X_knn = X_num_sub\n",
    "            if X_knn.shape[1] > 30:\n",
    "                cols = RNG.choice(X_knn.shape[1], size=30, replace=False)\n",
    "                X_knn = X_knn.iloc[:, cols]\n",
    "\n",
    "            Xtr, Xte, ytr, yte = train_test_split(\n",
    "                X_knn,\n",
    "                y_sub,\n",
    "                test_size=0.2,\n",
    "                random_state=42,\n",
    "                stratify=strat_labels,\n",
    "            )\n",
    "            clf = KNeighborsClassifier(n_neighbors=3)\n",
    "            clf.fit(Xtr, ytr)\n",
    "            acc = accuracy_score(yte, clf.predict(Xte))\n",
    "            landmarks[\"landmark_knn3_accuracy\"] = float(acc)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 4) Random noise baseline accuracy\n",
    "        try:\n",
    "            counts = np.bincount(y_sub)\n",
    "            probs = counts / counts.sum()\n",
    "            preds = RNG.choice(np.arange(len(probs)), size=len(y_sub), p=probs)\n",
    "            acc = accuracy_score(y_sub, preds)\n",
    "            landmarks[\"landmark_random_noise_accuracy\"] = float(acc)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 5) Fisher Discriminant Ratio (on subsample)\n",
    "        try:\n",
    "            fdr_values = []\n",
    "            for j in range(X_num_sub.shape[1]):\n",
    "                xj = X_num_sub.iloc[:, j].values.astype(float)\n",
    "                mu = xj.mean()\n",
    "                num = 0.0\n",
    "                den = 0.0\n",
    "                for c in np.unique(y_sub):\n",
    "                    mask_c = (y_sub == c)\n",
    "                    xc = xj[mask_c]\n",
    "                    if xc.size == 0:\n",
    "                        continue\n",
    "                    nc = xc.size\n",
    "                    mu_c = xc.mean()\n",
    "                    var_c = xc.var(ddof=1) if nc > 1 else 0.0\n",
    "                    num += nc * (mu_c - mu) ** 2\n",
    "                    den += nc * var_c\n",
    "                if den > 0:\n",
    "                    fdr_values.append(num / (den + 1e-12))\n",
    "            if fdr_values:\n",
    "                landmarks[\"fisher_discriminant_ratio\"] = float(np.mean(fdr_values))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return landmarks\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # schema\n",
    "    # -------------------------------------------------------------------------\n",
    "    KEY_ORDER = [\n",
    "        \"Task_id\",\n",
    "        \"dataset_id\",\n",
    "        \"dataset_name\",\n",
    "        \"n_samples\",\n",
    "        \"n_features\",\n",
    "        \"n_numeric_features\",\n",
    "        \"n_categorical_features\",\n",
    "        \"n_binary_features\",\n",
    "        \"n_classes\",\n",
    "        \"class_balance_std\",\n",
    "        \"class_entropy\",\n",
    "        \"mean_feature_variance\",\n",
    "        \"median_feature_variance\",\n",
    "        \"mean_corr_abs\",\n",
    "        \"max_corr_abs\",\n",
    "        \"feature_skewness_mean\",\n",
    "        \"feature_kurtosis_mean\",\n",
    "        \"missing_percentage\",\n",
    "        \"avg_cardinality_categorical\",\n",
    "        \"complexity_ratio\",\n",
    "        \"intrinsic_dim_estimate\",\n",
    "        \"landmark_lr_accuracy\",\n",
    "        \"landmark_dt_depth3_accuracy\",\n",
    "        \"landmark_knn3_accuracy\",\n",
    "        \"landmark_random_noise_accuracy\",\n",
    "        \"fisher_discriminant_ratio\",\n",
    "        \"model_id\",\n",
    "        \"model_name\",\n",
    "        \"model_family\",\n",
    "        \"is_deep_learning\",\n",
    "        \"is_tree_based\",\n",
    "        \"is_linear\",\n",
    "        \"parameterization_type\",\n",
    "        \"complexity_training_big_o\",\n",
    "        \"complexity_inference_big_o\",\n",
    "        \"is_probabilistic\",\n",
    "        \"is_ensemble_model\",\n",
    "        \"regularization_supported\",\n",
    "        \"supports_multiclass_natively\",\n",
    "        \"supports_online_learning\",\n",
    "        \"supports_multiple_trees\",\n",
    "        \"tree_growth_strategy\",\n",
    "        \"default_max_depth\",\n",
    "        \"supports_pruning\",\n",
    "        \"splitting_criterion\",\n",
    "        \"architecture_type\",\n",
    "        \"supports_dropout\",\n",
    "        \"supports_batchnorm\",\n",
    "        \"default_activation\",\n",
    "        \"supports_cuda_acceleration\",\n",
    "        \"supports_non_linearity\",\n",
    "        \"supports_categorical_directly\",\n",
    "        \"supports_missing_values\",\n",
    "        \"supports_gpu\",\n",
    "        \"n_estimators\",\n",
    "        \"avg_tree_depth\",\n",
    "        \"max_tree_depth\",\n",
    "        \"n_leaves_mean\",\n",
    "        \"n_layers\",\n",
    "        \"hidden_units_mean\",\n",
    "        \"dropout_rate_mean\",\n",
    "        \"activation_type\",\n",
    "        \"batch_size\",\n",
    "        \"epochs\",\n",
    "        \"accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"precision_macro\",\n",
    "        \"trained_model_size_kb\",\n",
    "        \"inference_speed_ms\",\n",
    "        \"static_usage_ram_kb\",\n",
    "        \"dynamic_usage_ram_kb\",\n",
    "        \"full_ram_usage_kb\",\n",
    "        \"model_n_parameters\",\n",
    "    ]\n",
    "\n",
    "    STRING_KEYS = {\n",
    "        \"dataset_name\",\n",
    "        \"model_name\",\n",
    "        \"model_family\",\n",
    "        \"parameterization_type\",\n",
    "        \"complexity_training_big_o\",\n",
    "        \"complexity_inference_big_o\",\n",
    "        \"regularization_supported\",\n",
    "        \"tree_growth_strategy\",\n",
    "        \"splitting_criterion\",\n",
    "        \"architecture_type\",\n",
    "        \"default_activation\",\n",
    "        \"activation_type\",\n",
    "    }\n",
    "\n",
    "    LANDMARK_KEYS = [\n",
    "        \"landmark_lr_accuracy\",\n",
    "        \"landmark_dt_depth3_accuracy\",\n",
    "        \"landmark_knn3_accuracy\",\n",
    "        \"landmark_random_noise_accuracy\",\n",
    "        \"fisher_discriminant_ratio\",\n",
    "    ]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute dataset meta + landmarks\n",
    "    # -------------------------------------------------------------------------\n",
    "    try:\n",
    "        ds_meta = compute_dataset_features(X, y)\n",
    "    except Exception:\n",
    "        ds_meta = {}\n",
    "\n",
    "    try:\n",
    "        lm = compute_landmarks(X, y)\n",
    "    except Exception:\n",
    "        lm = {k: 0.0 for k in LANDMARK_KEYS}\n",
    "\n",
    "    # Ensure all landmark keys exist\n",
    "    for k in LANDMARK_KEYS:\n",
    "        if k not in lm or lm[k] is None:\n",
    "            lm[k] = 0.0\n",
    "\n",
    "    meta_common = {}\n",
    "    meta_common.update(ds_meta)\n",
    "    meta_common.update(lm)\n",
    "\n",
    "    # IDs from arguments\n",
    "    base_ids = {\n",
    "        \"Task_id\": int(task_id),\n",
    "        \"dataset_id\": int(dataset_id),\n",
    "        \"dataset_name\": str(dataset_name),\n",
    "    }\n",
    "\n",
    "    # Performance/resource placeholders\n",
    "    perf_placeholders = {\n",
    "        \"accuracy\": 0.0,\n",
    "        \"f1_macro\": 0.0,\n",
    "        \"precision_macro\": 0.0,\n",
    "        \"trained_model_size_kb\": 0.0,\n",
    "        \"inference_speed_ms\": 0.0,\n",
    "        \"static_usage_ram_kb\": 0.0,\n",
    "        \"dynamic_usage_ram_kb\": 0.0,\n",
    "        \"full_ram_usage_kb\": 0.0,\n",
    "        \"model_n_parameters\": 0.0,\n",
    "    }\n",
    "\n",
    "    entries_by_model = {}\n",
    "\n",
    "    MODEL_IDS = {\n",
    "        \"logreg\": 1,\n",
    "        \"rf\": 2,\n",
    "        \"xgboost\": 3,\n",
    "        \"cnn1d\": 4,\n",
    "        \"tiny_rnn\": 5,\n",
    "        \"mlp\": 6,\n",
    "        \"tinyconv\": 7,\n",
    "    }\n",
    "\n",
    "    DL_MAX_EPOCHS = 20\n",
    "    DL_BATCH_SIZE = 128\n",
    "\n",
    "    MODEL_CAPABILITIES = {\n",
    "        \"logreg\": {\n",
    "            \"model_id\": MODEL_IDS[\"logreg\"],\n",
    "            \"model_name\": \"logreg\",\n",
    "            \"is_deep_learning\": False,\n",
    "            \"is_tree_based\": False,\n",
    "            \"is_linear\": True,\n",
    "            \"model_family\": \"Linear\",\n",
    "            \"parameterization_type\": \"linear-in-features\",\n",
    "            \"complexity_training_big_o\": \"O(n · d)\",\n",
    "            \"complexity_inference_big_o\": \"O(d)\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": False,\n",
    "            \"regularization_supported\": \"L2\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": False,\n",
    "            \"tree_growth_strategy\": \"none\",\n",
    "            \"default_max_depth\": 0,\n",
    "            \"supports_pruning\": False,\n",
    "            \"splitting_criterion\": \"none\",\n",
    "            \"architecture_type\": \"none\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"none\",\n",
    "            \"supports_cuda_acceleration\": False,\n",
    "            \"supports_non_linearity\": False,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": False,\n",
    "            \"supports_gpu\": False,\n",
    "            \"n_estimators\": 0,\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 0,\n",
    "            \"hidden_units_mean\": 0.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"none\",\n",
    "            \"batch_size\": 0,\n",
    "            \"epochs\": 0,\n",
    "        },\n",
    "        \"rf\": {\n",
    "            \"model_id\": MODEL_IDS[\"rf\"],\n",
    "            \"model_name\": \"rf\",\n",
    "            \"is_deep_learning\": False,\n",
    "            \"is_tree_based\": True,\n",
    "            \"is_linear\": False,\n",
    "            \"model_family\": \"TreeEnsemble\",\n",
    "            \"parameterization_type\": \"fixed-per-estimator\",\n",
    "            \"complexity_training_big_o\": \"O(n · log n · trees)\",\n",
    "            \"complexity_inference_big_o\": \"O(trees · depth)\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": True,\n",
    "            \"regularization_supported\": \"None\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": True,\n",
    "            \"tree_growth_strategy\": \"depth-based\",\n",
    "            \"default_max_depth\": 0,\n",
    "            \"supports_pruning\": False,\n",
    "            \"splitting_criterion\": \"gini\",\n",
    "            \"architecture_type\": \"none\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"none\",\n",
    "            \"supports_cuda_acceleration\": False,\n",
    "            \"supports_non_linearity\": True,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": False,\n",
    "            \"supports_gpu\": False,\n",
    "            \"n_estimators\": 200,\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 0,\n",
    "            \"hidden_units_mean\": 0.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"none\",\n",
    "            \"batch_size\": 0,\n",
    "            \"epochs\": 0,\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"model_id\": MODEL_IDS[\"xgboost\"],\n",
    "            \"model_name\": \"xgboost\",\n",
    "            \"is_deep_learning\": False,\n",
    "            \"is_tree_based\": True,\n",
    "            \"is_linear\": False,\n",
    "            \"model_family\": \"BoostedTrees\",\n",
    "            \"parameterization_type\": \"fixed-per-estimator\",\n",
    "            \"complexity_training_big_o\": \"O(n · log n · trees)\",\n",
    "            \"complexity_inference_big_o\": \"O(trees · depth)\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": True,\n",
    "            \"regularization_supported\": \"L1/L2\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": True,\n",
    "            \"tree_growth_strategy\": \"leaf-based\",\n",
    "            \"default_max_depth\": 6,\n",
    "            \"supports_pruning\": True,\n",
    "            \"splitting_criterion\": \"gain\",\n",
    "            \"architecture_type\": \"none\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"none\",\n",
    "            \"supports_cuda_acceleration\": True,\n",
    "            \"supports_non_linearity\": True,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": True,\n",
    "            \"supports_gpu\": True,\n",
    "            \"n_estimators\": 200,\n",
    "            \"avg_tree_depth\": 6.0,\n",
    "            \"max_tree_depth\": 6,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 0,\n",
    "            \"hidden_units_mean\": 0.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"none\",\n",
    "            \"batch_size\": 0,\n",
    "            \"epochs\": 0,\n",
    "        },\n",
    "        \"cnn1d\": {\n",
    "            \"model_id\": MODEL_IDS[\"cnn1d\"],\n",
    "            \"model_name\": \"cnn1d\",\n",
    "            \"is_deep_learning\": True,\n",
    "            \"is_tree_based\": False,\n",
    "            \"is_linear\": False,\n",
    "            \"model_family\": \"CNN\",\n",
    "            \"parameterization_type\": \"linear-in-features\",\n",
    "            \"complexity_training_big_o\": \"O(n · d · epochs)\",\n",
    "            \"complexity_inference_big_o\": \"O(d · filters)\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": False,\n",
    "            \"regularization_supported\": \"L2\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": False,\n",
    "            \"tree_growth_strategy\": \"none\",\n",
    "            \"default_max_depth\": 0,\n",
    "            \"supports_pruning\": False,\n",
    "            \"splitting_criterion\": \"none\",\n",
    "            \"architecture_type\": \"CNN1D\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"relu\",\n",
    "            \"supports_cuda_acceleration\": True,\n",
    "            \"supports_non_linearity\": True,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": False,\n",
    "            \"supports_gpu\": True,\n",
    "            \"n_estimators\": 0,\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 2,\n",
    "            \"hidden_units_mean\": 8.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"relu\",\n",
    "            \"batch_size\": DL_BATCH_SIZE,\n",
    "            \"epochs\": DL_MAX_EPOCHS,\n",
    "        },\n",
    "        \"tiny_rnn\": {\n",
    "            \"model_id\": MODEL_IDS[\"tiny_rnn\"],\n",
    "            \"model_name\": \"tiny_rnn\",\n",
    "            \"is_deep_learning\": True,\n",
    "            \"is_tree_based\": False,\n",
    "            \"is_linear\": False,\n",
    "            \"model_family\": \"RNN\",\n",
    "            \"parameterization_type\": \"linear-in-features\",\n",
    "            \"complexity_training_big_o\": \"O(n · d · hidden_dim · epochs)\",\n",
    "            \"complexity_inference_big_o\": \"O(d · hidden_dim)\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": False,\n",
    "            \"regularization_supported\": \"L2\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": False,\n",
    "            \"tree_growth_strategy\": \"none\",\n",
    "            \"default_max_depth\": 0,\n",
    "            \"supports_pruning\": False,\n",
    "            \"splitting_criterion\": \"none\",\n",
    "            \"architecture_type\": \"RNN-GRU\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"tanh\",\n",
    "            \"supports_cuda_acceleration\": True,\n",
    "            \"supports_non_linearity\": True,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": False,\n",
    "            \"supports_gpu\": True,\n",
    "            \"n_estimators\": 0,\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 2,\n",
    "            \"hidden_units_mean\": 32.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"tanh\",\n",
    "            \"batch_size\": DL_BATCH_SIZE,\n",
    "            \"epochs\": DL_MAX_EPOCHS,\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"model_id\": MODEL_IDS[\"mlp\"],\n",
    "            \"model_name\": \"mlp\",\n",
    "            \"is_deep_learning\": True,\n",
    "            \"is_tree_based\": False,\n",
    "            \"is_linear\": False,\n",
    "            \"model_family\": \"MLP\",\n",
    "            \"parameterization_type\": \"linear-in-features\",\n",
    "            \"complexity_training_big_o\": \"O(n · Σ(layer_dims) · epochs)\",\n",
    "            \"complexity_inference_big_o\": \"O(Σ(layer_dims))\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": False,\n",
    "            \"regularization_supported\": \"L2\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": False,\n",
    "            \"tree_growth_strategy\": \"none\",\n",
    "            \"default_max_depth\": 0,\n",
    "            \"supports_pruning\": False,\n",
    "            \"splitting_criterion\": \"none\",\n",
    "            \"architecture_type\": \"MLP\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"relu\",\n",
    "            \"supports_cuda_acceleration\": True,\n",
    "            \"supports_non_linearity\": True,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": False,\n",
    "            \"supports_gpu\": True,\n",
    "            \"n_estimators\": 0,\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 3,\n",
    "            \"hidden_units_mean\": (128.0 + 64.0) / 2.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"relu\",\n",
    "            \"batch_size\": DL_BATCH_SIZE,\n",
    "            \"epochs\": DL_MAX_EPOCHS,\n",
    "        },\n",
    "        \"tinyconv\": {\n",
    "            \"model_id\": MODEL_IDS[\"tinyconv\"],\n",
    "            \"model_name\": \"tinyconv\",\n",
    "            \"is_deep_learning\": True,\n",
    "            \"is_tree_based\": False,\n",
    "            \"is_linear\": False,\n",
    "            \"model_family\": \"CNN\",\n",
    "            \"parameterization_type\": \"linear-in-features\",\n",
    "            \"complexity_training_big_o\": \"O(n · d · epochs)\",\n",
    "            \"complexity_inference_big_o\": \"O(d · filters)\",\n",
    "            \"is_probabilistic\": True,\n",
    "            \"is_ensemble_model\": False,\n",
    "            \"regularization_supported\": \"L2\",\n",
    "            \"supports_multiclass_natively\": True,\n",
    "            \"supports_online_learning\": False,\n",
    "            \"supports_multiple_trees\": False,\n",
    "            \"tree_growth_strategy\": \"none\",\n",
    "            \"default_max_depth\": 0,\n",
    "            \"supports_pruning\": False,\n",
    "            \"splitting_criterion\": \"none\",\n",
    "            \"architecture_type\": \"CNN1D\",\n",
    "            \"supports_dropout\": False,\n",
    "            \"supports_batchnorm\": False,\n",
    "            \"default_activation\": \"relu\",\n",
    "            \"supports_cuda_acceleration\": True,\n",
    "            \"supports_non_linearity\": True,\n",
    "            \"supports_categorical_directly\": False,\n",
    "            \"supports_missing_values\": False,\n",
    "            \"supports_gpu\": True,\n",
    "            \"n_estimators\": 0,\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0,\n",
    "            \"n_leaves_mean\": 0.0,\n",
    "            \"n_layers\": 2,\n",
    "            \"hidden_units_mean\": 4.0,\n",
    "            \"dropout_rate_mean\": 0.0,\n",
    "            \"activation_type\": \"relu\",\n",
    "            \"batch_size\": DL_BATCH_SIZE,\n",
    "            \"epochs\": DL_MAX_EPOCHS,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for model_name, caps in MODEL_CAPABILITIES.items():\n",
    "        combined = {}\n",
    "        combined.update(base_ids)\n",
    "        combined.update(meta_common)\n",
    "        combined.update(caps)\n",
    "\n",
    "        for k, v in perf_placeholders.items():\n",
    "            combined.setdefault(k, v)\n",
    "\n",
    "        ordered_entry = {}\n",
    "        for key in KEY_ORDER:\n",
    "            val = combined.get(key, None)\n",
    "            if val is None:\n",
    "                if key in STRING_KEYS:\n",
    "                    val = \"unknown\"\n",
    "                else:\n",
    "                    val = 0.0\n",
    "            ordered_entry[key] = val\n",
    "\n",
    "        entries_by_model[model_name] = ordered_entry\n",
    "\n",
    "    return entries_by_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFdEzbYAicXw"
   },
   "outputs": [],
   "source": [
    "candidates = build_meta_entries_for_all_models_from_preprocessed(data, target)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU2-CuNUicXx"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# You must define these somewhere in your code:\n",
    "# MAX_THREADS = ...\n",
    "# RANDOM_SEED = ...\n",
    "\n",
    "\n",
    "def make_logreg():\n",
    "    return LogisticRegression(\n",
    "        max_iter=800,\n",
    "        solver=\"lbfgs\"\n",
    "    )\n",
    "\n",
    "def make_rf():\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def make_xgboost():\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "    # IMPORTANT: do NOT hardcode objective/num_class here.\n",
    "    # We'll set objective/num_class later in train_and_eval_model based on n_classes.\n",
    "    return xgb.XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"mlogloss\"\n",
    "    )\n",
    "\n",
    "# ---------- Simple PyTorch models ----------\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TinyConv1DNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Main 1D-CNN model (for 'cnn1d').\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        # treat features as length=input_dim, channels=1\n",
    "        self.conv = nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(16)\n",
    "        self.fc = nn.Linear(8 * 16, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, D]\n",
    "        x = x.unsqueeze(1)          # [B,1,D]\n",
    "        x = self.conv(x)            # [B,8,D]\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)            # [B,8,16]\n",
    "        x = x.view(x.size(0), -1)   # [B,8*16]\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TinyConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Separate smaller conv model for 'tinyconv' (not just an alias of cnn1d).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        # even smaller: fewer channels, smaller pooled length\n",
    "        self.conv = nn.Conv1d(1, 4, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(8)\n",
    "        self.fc = nn.Linear(4 * 8, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, D]\n",
    "        x = x.unsqueeze(1)          # [B,1,D]\n",
    "        x = self.conv(x)            # [B,4,D]\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)            # [B,4,8]\n",
    "        x = x.view(x.size(0), -1)   # [B,4*8]\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TinyRNNNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 32\n",
    "        # Represent each feature as one timestep with dim=1\n",
    "        self.rnn = nn.GRU(input_size=1, hidden_size=self.hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,D] -> [B,D,1]\n",
    "        x = x.unsqueeze(-1)\n",
    "        out, h = self.rnn(x)\n",
    "        # use last hidden state\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "\n",
    "def make_cnn1d(input_dim, n_classes):\n",
    "    model = TinyConv1DNet(input_dim, n_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_tinyconv(input_dim, n_classes):\n",
    "    \"\"\"\n",
    "    Actual tiny conv model, separate from cnn1d.\n",
    "    \"\"\"\n",
    "    model = TinyConvNet(input_dim, n_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_tiny_rnn(input_dim, n_classes):\n",
    "    model = TinyRNNNet(input_dim, n_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_mlp(input_dim, n_classes):\n",
    "    model = MLPNet(input_dim, n_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    \"logreg\":   (\"classic\", make_logreg),\n",
    "    \"rf\":       (\"classic\", make_rf),\n",
    "    \"xgboost\":  (\"classic\", make_xgboost),\n",
    "    \"cnn1d\":    (\"deep\",    make_cnn1d),\n",
    "    \"tiny_rnn\": (\"deep\",    make_tiny_rnn),\n",
    "    \"mlp\":      (\"deep\",    make_mlp),\n",
    "    \"tinyconv\": (\"deep\",    make_tinyconv),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmHPY4DMnAJx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Hyperparameter search spaces\n",
    "\n",
    "CLASSIC_PARAM_GRIDS = {\n",
    "    \"logreg\": {\n",
    "        \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"max_iter\": [200, 500]\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2]\n",
    "    },\n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0]\n",
    "    },\n",
    "}\n",
    "\n",
    "# Search space for deep models\n",
    "DEEP_PARAM_CONFIGS = {\n",
    "    \"cnn1d\": [\n",
    "        {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 15},\n",
    "        {\"lr\": 1e-4, \"batch_size\": 64, \"epochs\": 20},\n",
    "    ],\n",
    "    \"tiny_rnn\": [\n",
    "        {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 20},\n",
    "        {\"lr\": 5e-4, \"batch_size\": 64, \"epochs\": 25},\n",
    "    ],\n",
    "    \"mlp\": [\n",
    "        {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 20},\n",
    "        {\"lr\": 1e-4, \"batch_size\": 64, \"epochs\": 30},\n",
    "    ],\n",
    "    \"tinyconv\": [\n",
    "        {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 20},\n",
    "        {\"lr\": 5e-4, \"batch_size\": 64, \"epochs\": 25},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Utility: model size\n",
    "\n",
    "def get_model_size_mb(model, is_deep=False):\n",
    "    if not is_deep:\n",
    "        # sklearn / classic\n",
    "        data = pickle.dumps(model)\n",
    "        size_bytes = len(data)\n",
    "    else:\n",
    "        # Keras / deep\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".h5\", delete=False) as tmp:\n",
    "            tmp_path = tmp.name\n",
    "        try:\n",
    "            model.save(tmp_path, include_optimizer=True)\n",
    "            size_bytes = os.path.getsize(tmp_path)\n",
    "        finally:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "\n",
    "    return size_bytes / (1024 ** 2)  # MB\n",
    "\n",
    "# Classic model training + tuning\n",
    "\n",
    "def train_and_tune_classic(model_name, make_fn, X_train, y_train, X_val, y_val):\n",
    "    if model_name not in CLASSIC_PARAM_GRIDS:\n",
    "        raise ValueError(f\"No param grid defined for classic model: {model_name}\")\n",
    "\n",
    "    base_model = make_fn()\n",
    "    param_grid = CLASSIC_PARAM_GRIDS[model_name]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=3\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    model_size_mb = get_model_size_mb(best_model, is_deep=False)\n",
    "\n",
    "    print(f\"[{model_name}] best params: {grid.best_params_}\")\n",
    "    print(f\"[{model_name}] validation accuracy: {val_acc:.4f}\")\n",
    "    print(f\"[{model_name}] serialized size: {model_size_mb:.4f} MB\")\n",
    "\n",
    "    return best_model, val_acc, model_size_mb\n",
    "\n",
    "# Deep model training + tuning\n",
    "\n",
    "def train_and_tune_deep(model_name, make_fn, X_train, y_train, X_val, y_val):\n",
    "    if model_name not in DEEP_PARAM_CONFIGS:\n",
    "        raise ValueError(f\"No deep param configs defined for: {model_name}\")\n",
    "\n",
    "    best_acc = -np.inf\n",
    "    best_model = None\n",
    "    best_cfg = None\n",
    "\n",
    "    for cfg in DEEP_PARAM_CONFIGS[model_name]:\n",
    "        print(f\"\\n[{model_name}] Trying config: {cfg}\")\n",
    "\n",
    "        model = make_fn(lr=cfg[\"lr\"])\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=cfg[\"epochs\"],\n",
    "            batch_size=cfg[\"batch_size\"],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Use best val accuracy across epochs\n",
    "        val_acc = max(history.history.get(\"val_accuracy\", [0.0]))\n",
    "\n",
    "        print(f\"[{model_name}] val_accuracy (best over epochs): {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = model\n",
    "            best_cfg = cfg\n",
    "\n",
    "    model_size_mb = get_model_size_mb(best_model, is_deep=True)\n",
    "\n",
    "    print(f\"\\n[{model_name}] best config: {best_cfg}\")\n",
    "    print(f\"[{model_name}] best validation accuracy: {best_acc:.4f}\")\n",
    "    print(f\"[{model_name}] serialized size: {model_size_mb:.4f} MB\")\n",
    "\n",
    "    return best_model, best_acc, model_size_mb\n",
    "\n",
    "# Main: use best candidate name\n",
    "\n",
    "def run_best_candidate(best_model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    best_model_name is something like 'logreg', 'rf', 'xgboost', 'cnn1d', etc.\n",
    "    Looks it up in MODELS, then trains + tunes and prints metrics.\n",
    "    \"\"\"\n",
    "    if best_model_name not in MODELS:\n",
    "        raise ValueError(f\"Unknown model name: {best_model_name}\")\n",
    "\n",
    "    model_type, make_fn = MODELS[best_model_name]\n",
    "\n",
    "    if model_type == \"classic\":\n",
    "        return train_and_tune_classic(best_model_name, make_fn, X_train, y_train, X_val, y_val)\n",
    "    elif model_type == \"deep\":\n",
    "        return train_and_tune_deep(best_model_name, make_fn, X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type for {best_model_name}: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1T6jGjHmmyh"
   },
   "outputs": [],
   "source": [
    "def preprocessing_logic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols_to_drop = [\n",
    "        \"accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"precision_macro\",\n",
    "        \"trained_model_size_kb\",\n",
    "        \"inference_speed_ms\",\n",
    "        \"model_name\",\n",
    "        \"Task_id\",\n",
    "        \"dataset_id\",\n",
    "        \"model_id\",\n",
    "        \"dataset_name\",\n",
    "        \"static_usage_ram_kb\",\n",
    "        \"dynamic_usage_ram_kb\",\n",
    "        \"full_ram_usage_kb\",\n",
    "        \"mean_feature_variance\",\n",
    "        \"median_feature_variance\"\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    zero_variance_cols = [\n",
    "        \"n_leaves_mean\",\n",
    "        \"dropout_rate_mean\"\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in zero_variance_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    if \"regularization_supported\" in df.columns:\n",
    "        df = df.drop(\"regularization_supported\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDiHSOV_kxre"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load(\"xgb_pipeline_model.pkl\")\n",
    "\n",
    "best_name = None\n",
    "best_score = float(\"-inf\")\n",
    "\n",
    "for name, features in candidates.items():\n",
    "    df_entry = pd.DataFrame([features])\n",
    "\n",
    "    score = loaded_model.predict(df_entry)[0]\n",
    "\n",
    "    print(f\"{name} -> predicted score: {score}\")\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_name = name\n",
    "\n",
    "print(\"Best candidate name:\", best_name)\n",
    "print(\"Best predicted score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAL0-IdzoW3f"
   },
   "outputs": [],
   "source": [
    "best_model, best_acc, best_size = run_best_candidate(\n",
    "    best_name,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
