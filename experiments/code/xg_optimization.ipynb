{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea517a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data.astype(np.float32, copy=False)\n",
    "y = data.target.astype(np.int32, copy=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"Classes:\", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PKL_PATH = r\"xgb_model_before_optimization.pkl\"  \n",
    "\n",
    "if os.path.exists(MODEL_PKL_PATH):\n",
    "    with open(MODEL_PKL_PATH, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    if hasattr(obj, \"get_booster\"):\n",
    "        booster = obj.get_booster()\n",
    "        print(\"Loaded sklearn XGB model -> got Booster.\")\n",
    "    elif isinstance(obj, xgb.Booster):\n",
    "        booster = obj\n",
    "        print(\"Loaded Booster directly.\")\n",
    "    else:\n",
    "        raise TypeError(f\"Loaded object is not XGBoost Booster/sklearn model. Type={type(obj)}\")\n",
    "\n",
    "else:\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"max_depth\": 4,\n",
    "        \"eta\": 0.1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    booster = xgb.train(params=params, dtrain=dtrain, num_boost_round=200)\n",
    "    print(\"Trained baseline Booster (because pickle not found).\")\n",
    "\n",
    "print(\"Booster ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a188fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ubj(bst: xgb.Booster, path: str) -> float:\n",
    "    bst.save_model(path)\n",
    "    return os.path.getsize(path) / (1024 * 1024)\n",
    "\n",
    "dm_test = xgb.DMatrix(X_test)\n",
    "p = booster.predict(dm_test)  \n",
    "\n",
    "y_hat = (p >= 0.5).astype(np.int32)\n",
    "acc = float(accuracy_score(y_test, y_hat))\n",
    "\n",
    "base_path = \"xgb_baseline.ubj\"\n",
    "base_mb = export_ubj(booster, base_path)\n",
    "\n",
    "print(\"Baseline accuracy:\", acc)\n",
    "print(\"Baseline exported:\", base_path, f\"({base_mb:.4f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e66d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUNE_GAMMA = 0.01      \n",
    "PRUNE_MAX_DEPTH = 6    \n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "bst_refresh = xgb.train(\n",
    "    params={\"process_type\": \"update\", \"updater\": \"refresh\", \"refresh_leaf\": True},\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1,\n",
    "    xgb_model=booster,\n",
    ")\n",
    "\n",
    "bst_pruned = xgb.train(\n",
    "    params={\n",
    "        \"process_type\": \"update\",\n",
    "        \"updater\": \"prune\",\n",
    "        \"gamma\": PRUNE_GAMMA,\n",
    "        \"max_depth\": PRUNE_MAX_DEPTH,\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1,\n",
    "    xgb_model=bst_refresh,\n",
    ")\n",
    "\n",
    "print(\"Optimization done (refresh + prune).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d776d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_test = xgb.DMatrix(X_test)\n",
    "\n",
    "p_opt = bst_pruned.predict(dm_test)\n",
    "y_hat_opt = (p_opt >= 0.5).astype(np.int32)\n",
    "acc_opt = float(accuracy_score(y_test, y_hat_opt))\n",
    "\n",
    "opt_path = \"xgb_optimized.ubj\"\n",
    "opt_mb = export_ubj(bst_pruned, opt_path)\n",
    "\n",
    "bst_loaded = xgb.Booster()\n",
    "bst_loaded.load_model(opt_path)\n",
    "p_loaded = bst_loaded.predict(dm_test)\n",
    "\n",
    "max_diff = float(np.max(np.abs(p_loaded - p_opt)))\n",
    "\n",
    "print(\"Optimized accuracy:\", acc_opt)\n",
    "print(\"Optimized exported:\", opt_path, f\"({opt_mb:.4f} MB)\")\n",
    "print(\"Reload max |diff|:\", max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e48be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_ms(\n",
    "    bst: xgb.Booster,\n",
    "    X: np.ndarray,\n",
    "    batch_size: int = 1,\n",
    "    n_warmup: int = 20,\n",
    "    n_runs: int = 200,\n",
    "    nthread: int | None = None,\n",
    ") -> dict:\n",
    "\n",
    "    if nthread is not None:\n",
    "        bst.set_param({\"nthread\": int(nthread)})\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    n = X.shape[0]\n",
    "    if n == 0:\n",
    "        raise ValueError(\"X is empty.\")\n",
    "    if batch_size <= 0:\n",
    "        raise ValueError(\"batch_size must be >= 1.\")\n",
    "\n",
    "    idxs = [((i * batch_size) % n) for i in range(max(n_warmup, n_runs))]\n",
    "    batches = [X[i:i+batch_size] for i in idxs]\n",
    "\n",
    "    for xb in batches[:n_warmup]:\n",
    "        dm = xgb.DMatrix(xb)\n",
    "        _ = bst.predict(dm)\n",
    "\n",
    "    times = []\n",
    "    for xb in batches[:n_runs]:\n",
    "        dm = xgb.DMatrix(xb)\n",
    "        t0 = time.perf_counter()\n",
    "        _ = bst.predict(dm)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append((t1 - t0) * 1000.0)\n",
    "\n",
    "    times = np.array(times, dtype=np.float64)\n",
    "    ms_mean = float(times.mean())\n",
    "    ms_p50 = float(np.percentile(times, 50))\n",
    "    ms_p95 = float(np.percentile(times, 95))\n",
    "\n",
    "    return {\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"nthread\": None if nthread is None else int(nthread),\n",
    "        \"mean_ms_per_batch\": ms_mean,\n",
    "        \"p50_ms_per_batch\": ms_p50,\n",
    "        \"p95_ms_per_batch\": ms_p95,\n",
    "        \"mean_ms_per_sample\": ms_mean / batch_size,\n",
    "        \"p50_ms_per_sample\": ms_p50 / batch_size,\n",
    "        \"p95_ms_per_sample\": ms_p95 / batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356cb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 8, 32, 128]\n",
    "nthread = None  \n",
    "\n",
    "results = []\n",
    "for bs in batch_sizes:\n",
    "    r_base = measure_inference_ms(booster, X_test, batch_size=bs, nthread=nthread)\n",
    "    r_base[\"model\"] = \"baseline\"\n",
    "    results.append(r_base)\n",
    "\n",
    "    r_opt = measure_inference_ms(bst_pruned, X_test, batch_size=bs, nthread=nthread)\n",
    "    r_opt[\"model\"] = \"optimized\"\n",
    "    results.append(r_opt)\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    b = next(r for r in results if r[\"model\"] == \"baseline\" and r[\"batch_size\"] == bs)\n",
    "    o = next(r for r in results if r[\"model\"] == \"optimized\" and r[\"batch_size\"] == bs)\n",
    "    print(f\"\\nBatch size = {bs}\")\n",
    "    print(f\"  Baseline : mean {b['mean_ms_per_batch']:.4f} ms/batch  | {b['mean_ms_per_sample']:.4f} ms/sample\")\n",
    "    print(f\"  Optimized: mean {o['mean_ms_per_batch']:.4f} ms/batch  | {o['mean_ms_per_sample']:.4f} ms/sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b170104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def measure_inference_time(model, dmatrix, num_runs=100, warm_up=10):\n",
    "    for _ in range(warm_up):\n",
    "        _ = model.predict(dmatrix)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = model.predict(dmatrix)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time_sec = end_time - start_time\n",
    "    avg_time_per_run_sec = total_time_sec / num_runs\n",
    "    num_samples = dmatrix.num_row()\n",
    "    \n",
    "    avg_time_per_sample_us = (avg_time_per_run_sec / num_samples) * 1e6\n",
    "    total_avg_time_ms = avg_time_per_run_sec * 1000\n",
    "\n",
    "    print(f\"--- Inference Speed ({num_runs} runs) ---\")\n",
    "    print(f\"Total time per batch: {total_avg_time_ms:.4f} ms\")\n",
    "    print(f\"Time per sample:      {avg_time_per_sample_us:.4f} Âµs\")\n",
    "    \n",
    "    return {\n",
    "        \"time_per_sample_us\": avg_time_per_sample_us,\n",
    "        \"total_time_ms\": total_avg_time_ms\n",
    "    }\n",
    "\n",
    "print(\"Baseline Model:\")\n",
    "_ = measure_inference_time(booster, dm_test)\n",
    "\n",
    "print(\"\\nOptimized Model:\")\n",
    "_ = measure_inference_time(bst_pruned, dm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_nodes(bst: xgb.Booster) -> int:\n",
    "    trees = bst.get_dump(with_stats=False, dump_format=\"json\")\n",
    "    total = 0\n",
    "    for t in trees:\n",
    "        obj = json.loads(t)\n",
    "        stack = [obj]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            total += 1\n",
    "            if \"children\" in node:\n",
    "                stack.extend(node[\"children\"])\n",
    "    return total\n",
    "\n",
    "before_nodes = count_total_nodes(booster)\n",
    "after_nodes  = count_total_nodes(bst_pruned)\n",
    "\n",
    "print(\"Nodes before:\", before_nodes)\n",
    "print(\"Nodes after :\", after_nodes)\n",
    "print(\"Reduced by  :\", before_nodes - after_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_overfitting_xgb(bst: xgb.Booster, X_train, y_train, X_test, y_test, name=\"model\"):\n",
    "    if not isinstance(bst, xgb.Booster):\n",
    "        raise TypeError(f\"bst must be xgboost.Booster, got {type(bst)}\")\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test  = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train).astype(int, copy=False)\n",
    "    y_test  = np.asarray(y_test).astype(int, copy=False)\n",
    "\n",
    "    if X_train.ndim != 2 or X_test.ndim != 2:\n",
    "        raise ValueError(\"X_train and X_test must be 2D arrays.\")\n",
    "    if X_train.shape[0] != y_train.shape[0] or X_test.shape[0] != y_test.shape[0]:\n",
    "        raise ValueError(\"Mismatch between X and y lengths.\")\n",
    "\n",
    "    cfg = json.loads(bst.save_config())\n",
    "    objective = cfg[\"learner\"][\"objective\"][\"name\"]\n",
    "    num_class = int(cfg[\"learner\"][\"learner_model_param\"].get(\"num_class\", 0))\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train)\n",
    "    dtest  = xgb.DMatrix(X_test)\n",
    "\n",
    "    pred_train = bst.predict(dtrain)\n",
    "    pred_test  = bst.predict(dtest)\n",
    "\n",
    "    if objective == \"binary:logistic\":\n",
    "        if pred_train.ndim != 1 or pred_test.ndim != 1:\n",
    "            raise ValueError(\"binary:logistic should produce 1D predictions.\")\n",
    "        yhat_train = (pred_train >= 0.5).astype(int)\n",
    "        yhat_test  = (pred_test  >= 0.5).astype(int)\n",
    "\n",
    "    elif objective == \"binary:logitraw\":\n",
    "        if pred_train.ndim != 1 or pred_test.ndim != 1:\n",
    "            raise ValueError(\"binary:logitraw should produce 1D margins.\")\n",
    "        prob_train = 1.0 / (1.0 + np.exp(-pred_train))\n",
    "        prob_test  = 1.0 / (1.0 + np.exp(-pred_test))\n",
    "        yhat_train = (prob_train >= 0.5).astype(int)\n",
    "        yhat_test  = (prob_test  >= 0.5).astype(int)\n",
    "\n",
    "    elif objective == \"multi:softprob\":\n",
    "        if num_class <= 1:\n",
    "            raise ValueError(\"multi:softprob requires num_class > 1.\")\n",
    "        if pred_train.ndim == 1:\n",
    "            if pred_train.size != X_train.shape[0] * num_class:\n",
    "                raise ValueError(\"Unexpected softprob size for train.\")\n",
    "            pred_train = pred_train.reshape(X_train.shape[0], num_class)\n",
    "        if pred_test.ndim == 1:\n",
    "            if pred_test.size != X_test.shape[0] * num_class:\n",
    "                raise ValueError(\"Unexpected softprob size for test.\")\n",
    "            pred_test = pred_test.reshape(X_test.shape[0], num_class)\n",
    "\n",
    "        yhat_train = np.argmax(pred_train, axis=1).astype(int)\n",
    "        yhat_test  = np.argmax(pred_test, axis=1).astype(int)\n",
    "\n",
    "    elif objective == \"multi:softmax\":\n",
    "        if pred_train.ndim != 1 or pred_test.ndim != 1:\n",
    "            raise ValueError(\"multi:softmax should produce 1D class indices.\")\n",
    "        yhat_train = pred_train.astype(int)\n",
    "        yhat_test  = pred_test.astype(int)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Objective not supported in this evaluator: {objective}\\n\"\n",
    "            \"Add handling here if you use a different objective.\"\n",
    "        )\n",
    "\n",
    "    acc_train = float(accuracy_score(y_train, yhat_train))\n",
    "    acc_test  = float(accuracy_score(y_test, yhat_test))\n",
    "    gap = acc_train - acc_test\n",
    "\n",
    "    print(f\"[{name}] objective : {objective}\")\n",
    "    print(f\"[{name}] Train acc: {acc_train:.4f}\")\n",
    "    print(f\"[{name}] Test  acc: {acc_test:.4f}\")\n",
    "    print(f\"[{name}] Gap       : {gap:.4f}\")\n",
    "\n",
    "    return {\"train_acc\": acc_train, \"test_acc\": acc_test, \"gap\": gap, \"objective\": objective}\n",
    "\n",
    "print(\"Baseline:\")\n",
    "_ = evaluate_overfitting_xgb(booster, X_train, y_train, X_test, y_test, name=\"baseline\")\n",
    "\n",
    "print(\"\\nOptimized:\")\n",
    "_ = evaluate_overfitting_xgb(bst_pruned, X_train, y_train, X_test, y_test, name=\"optimized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
