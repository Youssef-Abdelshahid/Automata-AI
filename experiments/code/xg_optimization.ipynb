{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea517a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data.astype(np.float32, copy=False)\n",
    "y = data.target.astype(np.int32, copy=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"Classes:\", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PKL_PATH = r\"xgb_model_before_optimization.pkl\"  \n",
    "\n",
    "if os.path.exists(MODEL_PKL_PATH):\n",
    "    with open(MODEL_PKL_PATH, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    if hasattr(obj, \"get_booster\"):\n",
    "        booster = obj.get_booster()\n",
    "        print(\"Loaded sklearn XGB model -> got Booster.\")\n",
    "    elif isinstance(obj, xgb.Booster):\n",
    "        booster = obj\n",
    "        print(\"Loaded Booster directly.\")\n",
    "    else:\n",
    "        raise TypeError(f\"Loaded object is not XGBoost Booster/sklearn model. Type={type(obj)}\")\n",
    "\n",
    "else:\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"max_depth\": 4,\n",
    "        \"eta\": 0.1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    booster = xgb.train(params=params, dtrain=dtrain, num_boost_round=200)\n",
    "    print(\"Trained baseline Booster (because pickle not found).\")\n",
    "\n",
    "print(\"Booster ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e66d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dm_test = xgb.DMatrix(X_test)\n",
    "\n",
    "print(\"--- Auto-Tuning Optimization Levels (Hybrid: Retrain + Prune) ---\")\n",
    "\n",
    "def _total_nodes(bst):\n",
    "    trees = bst.get_dump(with_stats=False, dump_format=\"json\")\n",
    "    total = 0\n",
    "    for t in trees:\n",
    "        obj = json.loads(t)\n",
    "        stack = [obj]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            total += 1\n",
    "            if \"children\" in node:\n",
    "                stack.extend(node[\"children\"])\n",
    "    return total\n",
    "\n",
    "def _size_kb(bst):\n",
    "    tmp_path = \"temp_model.ubj\"\n",
    "    bst.save_model(tmp_path)\n",
    "    size = os.path.getsize(tmp_path) / 1024.0\n",
    "    try: os.remove(tmp_path)\n",
    "    except: pass\n",
    "    return size\n",
    "\n",
    "base_params = {\n",
    "    \"objective\": \"binary:logistic\", \"eval_metric\": \"logloss\", \"eta\": 0.1,\n",
    "    \"subsample\": 0.9, \"colsample_bytree\": 0.9, \"seed\": 42, \"max_depth\": 4, \"gamma\": 0\n",
    "}\n",
    "bst_base = xgb.train(params=base_params, dtrain=dtrain, num_boost_round=200)\n",
    "acc_base = accuracy_score(y_test, (bst_base.predict(dm_test) >= 0.5).astype(int))\n",
    "base_nodes = _total_nodes(bst_base)\n",
    "print(f\"Baseline: Acc={acc_base:.4f}, Nodes={base_nodes}, Size={_size_kb(bst_base):.2f} KB\")\n",
    "\n",
    "gammas = [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100]\n",
    "print(\"\\n[Retraining Sweep]\")\n",
    "sweep_results = []\n",
    "\n",
    "def _train_sweep_candidate(g_val, d_val=6):\n",
    "    p = base_params.copy()\n",
    "    p.update({\"gamma\": g_val, \"max_depth\": d_val})\n",
    "    return xgb.train(params=p, dtrain=dtrain, num_boost_round=200)\n",
    "\n",
    "for g in gammas:\n",
    "    bst = _train_sweep_candidate(g)\n",
    "    acc = accuracy_score(y_test, (bst.predict(dm_test) >= 0.5).astype(int))\n",
    "    nodes = _total_nodes(bst)\n",
    "    drop = (acc_base - acc) / acc_base * 100\n",
    "    comp = (base_nodes - nodes) / base_nodes * 100\n",
    "    print(f\"Gamma={g:<5} | Nodes={nodes:<5} ({comp:.1f}%) | Acc={acc:.4f} (-{drop:.2f}%)\")\n",
    "    sweep_results.append({\"bst\": bst, \"gamma\": g, \"acc\": acc, \"nodes\": nodes, \"drop\": drop})\n",
    "\n",
    "levels = {}\n",
    "trained_models = {\"Baseline\": bst_base}\n",
    "\n",
    "valid_cands = sorted(sweep_results, key=lambda x: x[\"nodes\"])\n",
    "\n",
    "def get_best_under(limit):\n",
    "    found = [x for x in valid_cands if x[\"drop\"] <= limit]\n",
    "    return found[0] if found else None\n",
    "\n",
    "l_cand = get_best_under(3.0)\n",
    "if l_cand:\n",
    "    levels[\"Light\"] = l_cand[\"bst\"]\n",
    "    trained_models[\"Light\"] = l_cand[\"bst\"]\n",
    "    print(f\"Selected Light: Gamma={l_cand['gamma']}\")\n",
    "\n",
    "m_cand = get_best_under(6.0)\n",
    "if m_cand:\n",
    "    light_nodes = _total_nodes(levels[\"Light\"]) if \"Light\" in levels else float('inf')\n",
    "    if \"Light\" not in levels or m_cand[\"nodes\"] < light_nodes:\n",
    "         levels[\"Medium\"] = m_cand[\"bst\"]\n",
    "         trained_models[\"Medium\"] = m_cand[\"bst\"]\n",
    "         print(f\"Selected Medium: Gamma={m_cand['gamma']}\")\n",
    "    else:\n",
    "        print(f\"Skipped Medium: Best candidate (Gamma={m_cand['gamma']}, Nodes={m_cand['nodes']}) is not smaller than Light.\")\n",
    "else:\n",
    "    print(\"Skipped Medium: No candidate found < 6.0% drop.\")\n",
    "\n",
    "base_for_prune = levels.get(\"Medium\", levels.get(\"Light\", bst_base))\n",
    "print(f\"\\n[Aggressive Pruning] Base Nodes={_total_nodes(base_for_prune)}\")\n",
    "\n",
    "def _prune_candidate(base_model, prune_gamma, prune_depth=6):\n",
    "    bst_refresh = xgb.train(\n",
    "        params={\"process_type\": \"update\", \"updater\": \"refresh\", \"refresh_leaf\": True},\n",
    "        dtrain=dtrain, num_boost_round=1, xgb_model=base_model\n",
    "    )\n",
    "    bst_pruned = xgb.train(\n",
    "        params={\"process_type\": \"update\", \"updater\": \"prune\", \"gamma\": prune_gamma, \"max_depth\": prune_depth},\n",
    "        dtrain=dtrain, num_boost_round=1, xgb_model=bst_refresh\n",
    "    )\n",
    "    return bst_pruned\n",
    "\n",
    "prune_gammas = [0.1, 1, 5, 10, 50]\n",
    "agg_limit = 50.0\n",
    "best_agg = None\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=r\".*manually specified.*\");\n",
    "    for pg in prune_gammas:\n",
    "        bst_p = _prune_candidate(base_for_prune, pg)\n",
    "        acc = accuracy_score(y_test, (bst_p.predict(dm_test) >= 0.5).astype(int))\n",
    "        nodes = _total_nodes(bst_p)\n",
    "        drop = (acc_base - acc) / acc_base * 100\n",
    "        print(f\"Prune Gamma={pg:<5} | Nodes={nodes:<5} | Acc={acc:.4f} (-{drop:.2f}%)\")\n",
    "        \n",
    "        if drop > agg_limit:\n",
    "            print(f\"Stopping: Drop {drop:.2f}% > {agg_limit}%\")\n",
    "            break\n",
    "        \n",
    "        best_agg = {\"bst\": bst_p, \"gamma\": pg, \"acc\": acc, \"nodes\": nodes}\n",
    "\n",
    "if best_agg:\n",
    "    med_nodes = _total_nodes(levels[\"Medium\"]) if \"Medium\" in levels else base_nodes\n",
    "    if best_agg[\"nodes\"] < med_nodes:\n",
    "        levels[\"Aggressive\"] = best_agg[\"bst\"]\n",
    "        trained_models[\"Aggressive\"] = best_agg[\"bst\"]\n",
    "        print(f\"Selected Aggressive: Gamma={best_agg['gamma']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d776d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Sizes (UBJ format) ---\")\n",
    "for name, bst in trained_models.items():\n",
    "    path = f\"xgb_{name.lower().replace(' ', '_')}.ubj\"\n",
    "    bst.save_model(path)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"{name:<12} : {size_mb:.4f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e48be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inference Speed Benchmark (100 runs per level) ---\")\n",
    "for name, bst in trained_models.items():\n",
    "    for _ in range(10): bst.predict(dm_test)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(100): bst.predict(dm_test)\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    total_ms = (t1 - t0) * 1000\n",
    "    per_sample_us = (total_ms / 100 / X_test.shape[0]) * 1000\n",
    "    print(f\"{name:<12} : {total_ms:.2f} ms/batch  | {per_sample_us:.2f} us/sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_overfitting_xgb(bst: xgb.Booster, X_train, y_train, X_test, y_test, name=\"model\"):\n",
    "    if not isinstance(bst, xgb.Booster):\n",
    "        raise TypeError(f\"bst must be xgboost.Booster, got {type(bst)}\")\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test  = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train).astype(int, copy=False)\n",
    "    y_test  = np.asarray(y_test).astype(int, copy=False)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train)\n",
    "    dtest  = xgb.DMatrix(X_test)\n",
    "\n",
    "    pred_train = bst.predict(dtrain)\n",
    "    pred_test  = bst.predict(dtest)\n",
    "\n",
    "    yhat_train = (pred_train >= 0.5).astype(int)\n",
    "    yhat_test  = (pred_test  >= 0.5).astype(int)\n",
    "\n",
    "    acc_train = float(accuracy_score(y_train, yhat_train))\n",
    "    acc_test  = float(accuracy_score(y_test, yhat_test))\n",
    "    gap = acc_train - acc_test\n",
    "\n",
    "    print(f\"[{name}] Train acc: {acc_train:.4f} | Test acc: {acc_test:.4f} | Gap: {gap:.4f}\")\n",
    "\n",
    "print(\"--- Overfitting Analysis ---\")\n",
    "for name, bst in trained_models.items():\n",
    "    evaluate_overfitting_xgb(bst, X_train, y_train, X_test, y_test, name=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
